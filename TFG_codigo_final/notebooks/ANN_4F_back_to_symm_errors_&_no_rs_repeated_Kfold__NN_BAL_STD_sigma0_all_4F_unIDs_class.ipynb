{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "XY_bal_log_Rel= np.genfromtxt('../data/XY_bal_log_Rel.txt',dtype='str') \n",
    "XY_bal_log_Rel_data = np.asarray(XY_bal_log_Rel[1::,:],dtype=float)\n",
    "print(XY_bal_log_Rel [0,:])\n",
    "\n",
    "print(XY_bal_log_Rel_data.shape)\n",
    "\n",
    "XY_bal_log_Rel_data_sigma0=np.zeros([0,XY_bal_log_Rel_data.shape[1]])\n",
    "\n",
    "sigmaastro=0\n",
    "\n",
    "for i in range (0,len(XY_bal_log_Rel_data)):\n",
    "    if XY_bal_log_Rel_data[i,2]>=sigmaastro: #remeber column are 0=beta, 1=beta_err, 2=E_peak, 3=sigma, 4=curv_sign\n",
    "        XY_bal_log_Rel_data_sigma0=np.concatenate( (XY_bal_log_Rel_data_sigma0, [XY_bal_log_Rel_data[i,:]] ) , axis=0)\n",
    "\n",
    "\n",
    "XY_bal_log_Rel_data=XY_bal_log_Rel_data_sigma0\n",
    "\n",
    "X_bal_log_Rel_data= XY_bal_log_Rel_data[:,[0,1,2,3]]\n",
    "Y=XY_bal_log_Rel_data[:,4]\n",
    "print(X_bal_log_Rel_data.shape)\n",
    "print(Y.shape)\n",
    "print(10**XY_bal_log_Rel_data[:,2].min())\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "ax0, ax1, ax2, ax3 = axes.flatten()\n",
    "\n",
    "#colors = ['red', 'tan', 'lime']\n",
    "ax0.hist(X_bal_log_Rel_data[:,[0]], 100, color='orange')\n",
    "#ax0.hist(DM_log_bal_Rel[:,[0]], 100, color='m')\n",
    "#ax0.hist(unids_log[:,[0]], 100, color='red')\n",
    "#ax0.legend(prop={'size': 10})\n",
    "#ax0.set_title('Epeak distribution')\n",
    "ax0.set_xlabel(r' $Log(E_{peak})$')\n",
    "ax0.set_ylabel('count')\n",
    "\n",
    "ax1.hist(X_bal_log_Rel_data[:,[1]], 100, color='orange')\n",
    "#ax1.hist(DM_log_bal_Rel[:,[1]], 100, color='m')\n",
    "#ax1.hist(unids_log[:,[1]], 100,color='red')\n",
    "ax1.legend(('TOT: Astro+DM'))\n",
    "#ax1.set_title('Beta distribution')\n",
    "ax1.set_xlabel(r' $Log(\\beta)$')\n",
    "ax1.set_ylabel('count')\n",
    "\n",
    "ax2.hist(X_bal_log_Rel_data[:,[2]], 100, color='orange')\n",
    "#ax2.hist(DM_log_bal_Rel[:,[2]], 100, color='m')\n",
    "#ax2.hist(unids_log[:,[2]], 100, color='red')\n",
    "#ax2.set_title('sigma distribution')\n",
    "ax2.set_xlabel(r' $Log(\\sigma_{TS})$')\n",
    "ax2.set_ylabel('count')\n",
    "\n",
    "\n",
    "ax3.hist(X_bal_log_Rel_data[:,[3]], 100, color='orange')\n",
    "#ax3.hist(DM_log_bal_Rel[:,[3]], 100, color='m')\n",
    "#ax3.hist(unids_log[:,[3]], 100, color='red')\n",
    "#ax3.set_title('beta_err distribution')\n",
    "ax3.set_xlabel(r' $Log(\\beta_{rel})$')\n",
    "ax3.set_ylabel('count')\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig(\"histo_tot_data_bal.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sigma max',10**X_bal_log_Rel_data[:,2].max())\n",
    "print('sigma min',10**X_bal_log_Rel_data[:,2].min())\n",
    "print('log sigma max',X_bal_log_Rel_data[:,2].max())\n",
    "print('log sigma min',X_bal_log_Rel_data[:,2].min())\n",
    "np.log10(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDM_sample=0\n",
    "\n",
    "for i in range(0,len(Y)):\n",
    "       if Y[i]==1: \n",
    "        NDM_sample=NDM_sample+1\n",
    "\n",
    "print (NDM_sample)\n",
    "\n",
    "Nastro_sample=0\n",
    "\n",
    "for i in range(0,len(Y)):\n",
    "       if Y[i]==0: \n",
    "        Nastro_sample=Nastro_sample+1\n",
    "        \n",
    "print (Nastro_sample)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_log=X_bal_log_Rel_data\n",
    "\n",
    "#normalizer = sklearn.preprocessing.StandardScaler()\n",
    "#normalizer.fit(X_log)\n",
    "#print('StandardSaler mean', normalizer.mean_)\n",
    "#X_log = normalizer.transform(X_log)\n",
    "\n",
    "#X_log = np.log10(X_log)\n",
    "\n",
    "print(X_log.shape)\n",
    "print(X_log[1:5,0])\n",
    "print(X_log[1:5,1])\n",
    "\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unids_3F = np.genfromtxt('../data/unids_3F_beta_err_names.txt',dtype='str') \n",
    "unids_3F_data = np.asarray(unids_3F[1::,:],dtype=float)\n",
    "\n",
    "print(unids_3F[0,:])\n",
    "\n",
    "unids_log=np.log10(unids_3F_data[:,[0,1,2,3]])\n",
    "print(unids_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure()\n",
    "plt.scatter(unids_log[:,0],unids_log[:,1], color='red',label='unids',s=5)\n",
    "plt.scatter(np.log10(unids_3F_data[:,0]),np.log10(unids_3F_data[:,1]), \n",
    "          color='green',label='DM-cand',s=1)\n",
    "#plt.errorbar(selected_unIDs_80[:,0], selected_unIDs_80[:,1], yerr=selected_unIDs_80[:,3], fmt=\"o\")\n",
    "\n",
    "plt.ylabel(r' $Log(\\beta)$')\n",
    "plt.xlabel(r' $Log(E_{peak})$')\n",
    "plt.xlim(-5,6)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure()\n",
    "plt.scatter(unids_log[:,0],unids_log[:,2], color='red',label='unids',s=5)\n",
    "#plt.scatter(np.log10(unids_3F_data[:,0]),np.log10(unids_3F_data[:,1]), \n",
    "         # color='green',label='DM-cand',s=1)\n",
    "#plt.errorbar(selected_unIDs_80[:,0], selected_unIDs_80[:,1], yerr=selected_unIDs_80[:,3], fmt=\"o\")\n",
    "\n",
    "plt.ylabel(r' $Log(sigma)$')\n",
    "plt.xlabel(r' $Log(E_{peak})$')\n",
    "#plt.xlim(-5,6)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### n_activation_cells = np.arange(1, 100, 100//20)\n",
    "#hidden_layers = (n_activation_cells, n_activation_cells)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#classifier = MLPClassifier(solver='adam', alpha=0.0, batch_size=120, \n",
    "#hidden_layer_sizes=(11, 1), learning_rate_init=0.015, max_iter=1000, random_state=0, activation='relu')\n",
    "\n",
    "N_cells = np.arange(1, 110, 10)\n",
    "hidden_layers_1 = [ (n_cells,)  for n_cells in N_cells  ]\n",
    "hidden_layers_2 = [ (n_cells, n_cells)  for n_cells in N_cells  ]\n",
    "#hidden_layers_3 = [ (n_cells, n_cells, n_cells)  for n_cells in N_cells  ]\n",
    "#hidden_layers_4 = [ (n_cells, n_cells, n_cells, n_cells)  for n_cells in N_cells  ]\n",
    "\n",
    "params = [{'model__hidden_layer_sizes': hidden_layers_1},\n",
    "         {'model__hidden_layer_sizes': hidden_layers_2},\n",
    "         #{'model__hidden_layer_sizes': hidden_layers_3},\n",
    "         #{'model__hidden_layer_sizes': hidden_layers_4},\n",
    "         ]\n",
    "\n",
    "steps = [ ('scaler', StandardScaler()),\n",
    "         ('model', MLPClassifier(solver='adam', alpha=0.0, batch_size=120, hidden_layer_sizes=params, learning_rate_init=0.015, max_iter=1000, random_state=None, activation='relu') ) ] #1e-4 es la default as√≠ que no cambia nada\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "normalizer = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "N_splits=5 #25% of testing set with N_splits=4, but we lost the easy 5 statitics in each bin\n",
    "N_Repeats=1\n",
    "#N_sample=N_splits*N_Repeats\n",
    "\n",
    "OA=[]\n",
    "TN=[]\n",
    "TP=[]\n",
    "#unIDs_std_proba_check_repeated_rs_stats_all=np.array((1,))\n",
    "\n",
    "#OA_rs_stats=open(\"OA_rs_stats.txt\", \"w\")\n",
    "#OA_rs_stats.write('seed_value OA \\n') \n",
    "#TN_rs_stats=open(\"TN_rs_stats.txt\", \"w\")\n",
    "#TN_rs_stats.write('seed_value TN \\n') \n",
    "#TP_rs_stats=open(\"TP_rs_stats.txt\", \"w\")\n",
    "#TP_rs_stats.write('seed_value TP \\n') \n",
    "\n",
    "unids_DM_std_proba_check_repeated_kfold_rskf_4F_21=open(\"../data/results/ann/4F/unids_DM_std_proba_check_repeated_kfold_rskf_4F_21.txt\", \"w\")\n",
    "unids_DM_std_proba_check_repeated_kfold_rskf_4F_21.write('Numb unids_DM_proba_check_repeated_kfold \\n') \n",
    "\n",
    "\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=N_splits, n_repeats=N_Repeats)\n",
    "rskf.get_n_splits(X_log, Y)\n",
    "print('rskf',rskf)\n",
    "ANN = MLPClassifier(solver='adam', alpha=0.0, batch_size=120, hidden_layer_sizes=(21,), \n",
    "                 learning_rate_init=0.015, max_iter=1000, random_state=None, activation='relu')\n",
    "    \n",
    "for train_index, test_index in rskf.split(X_log, Y):\n",
    "        print('train_index',train_index.shape, \"test_index\", test_index.shape)\n",
    "        X_train_split, X_test_split = X_log[train_index], X_log[test_index]\n",
    "        Y_train_split, Y_test_split = Y[train_index], Y[test_index]\n",
    "        normalizer.fit(X_train_split)\n",
    "        X_train_split_std=normalizer.transform(X_train_split)\n",
    "        X_test_split_std=normalizer.transform(X_test_split)\n",
    "    \n",
    "        ANN_fit=ANN.fit(X_train_split_std, Y_train_split)\n",
    "        Y_test_split_01_std_check=ANN_fit.predict(X_test_split_std)\n",
    "        Y_test_split_proba_std_check=ANN_fit.predict_proba(X_test_split_std)\n",
    "        \n",
    "        OA.extend([accuracy_score(Y_test_split, Y_test_split_01_std_check)])\n",
    "        conf_matrix=sklearn.metrics.confusion_matrix(Y_test_split, Y_test_split_01_std_check, normalize='true')\n",
    "        TN.extend([conf_matrix[0,0]])\n",
    "        TP.extend([conf_matrix[1,1]])\n",
    "           \n",
    "        unids_std_check=normalizer.transform(unids_log)\n",
    "        unIDs_std_proba_check_repeated_kfold=ANN_fit.predict_proba(unids_std_check)\n",
    "        \n",
    "        for i in range(0,len(unids_std_check)):\n",
    "                #unIDs_std_proba_check_repeated_rs_stats_all[i]=np.append(ANN_fit.predict_proba(unids_std_check)[i,1])\n",
    "                unids_DM_std_proba_check_repeated_kfold_rskf_4F_21.write('{} {} \\n'.format(i, \n",
    "                                                                    unIDs_std_proba_check_repeated_kfold[i,1])) \n",
    "\n",
    "       # OA_rs_stats.write('{} {} \\n'.format(seed_value[j], OA[j]))\n",
    "        #TN_rs_stats.write('{} {} \\n'.format(seed_value[j], TN[j]))\n",
    "        #TP_rs_stats.write('{} {} \\n'.format(seed_value[j], TP[j]))  \n",
    "    \n",
    "unids_DM_std_proba_check_repeated_kfold_rskf_4F_21.close()\n",
    "\n",
    "#OA_rs_stats.close()\n",
    "#TN_rs_stats.close()\n",
    "#TP_rs_stats.close()\n",
    "\n",
    "print('X_train_split.shape', X_train_split.shape)\n",
    "print('X_test_split.shape', X_test_split.shape)\n",
    "\n",
    "#unIDs_std_proba_check_repeated_rs_stats=np.array(unIDs_std_proba_check_repeated_rs_stats)\n",
    "#print('unIDs_std_proba_check_repeated_rs_stats.shape',unIDs_std_proba_check_repeated_rs_stats.shape)\n",
    "#print(unIDs_std_proba_check_repeated_rs_stats[0:3,:])\n",
    "\n",
    "OA=np.array(OA)\n",
    "#OA=np.reshape(OA,(len(unids_std_check),(N_sample)))\n",
    "TN=np.array(TN)\n",
    "#TN=np.reshape(TN,(len(unids_std_check),(N_sample)))\n",
    "TP=np.array(TP)\n",
    "#TP=np.reshape(TP,(len(unids_std_check),(N_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 7), sharex=True)\n",
    "#fig.suptitle('ANN Best Hyper-parameters (2Features & 2Classes)', fontsize=16)\n",
    "\n",
    "best_score = 0\n",
    "for i, param in enumerate(params):\n",
    "    #params = {'model__hidden_layer_sizes': (N_cells,)}\n",
    "    search = GridSearchCV(pipe, param, scoring='accuracy', n_jobs=-1, \n",
    "                          cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0)).fit(X_train_split, Y_train_split)\n",
    "    \n",
    "    grid_results = pd.DataFrame( search.cv_results_ )\n",
    "    \n",
    "    \n",
    "    # MAKE THE PLOT\n",
    "    axs[0].errorbar(N_cells, grid_results['mean_test_score'], yerr=grid_results['std_test_score'], \n",
    "                  fmt='-o', ms=5, mec='k', capsize=4, label= str(i+1)+' layers' )\n",
    "    axs[0].set_xlabel('Number of cells on each layer', fontsize=15)\n",
    "    axs[0].set_ylabel('accuracy', fontsize=15)\n",
    "    axs[0].grid(linestyle='-.', color='k')\n",
    "    \n",
    "    axs[1].errorbar(N_cells, grid_results['mean_fit_time'], yerr=grid_results['std_fit_time'],\n",
    "                   fmt='-o', ms=5, mec='k', capsize=4 )\n",
    "    \n",
    "    axs[0].set_xticks(np.arange(1, 110, step=10))\n",
    "    axs[0].set_yticks(np.arange(0.8, 1.01, step=0.1))\n",
    "    axs[1].set_xlabel('Number of cells on each layer', fontsize=15)\n",
    "    axs[1].set_ylabel('Fit Time (s)', fontsize=15)\n",
    "    axs[1].grid(linestyle='-.', color='k')\n",
    "    axs[0].legend(fontsize=15)\n",
    "    \n",
    "    #print(search.cv_results_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "x_min, x_max = -6.5, 6.5 #Log[Epeak]\n",
    "y_min, y_max = -7.5, 0.5 #Log[Beta] (including truncated sample values, i.e. bmin-bmin_err>0.0)\n",
    "z_min, z_max = 0.0, 2.89 #Log[sigma_det] (including unids_min y sigma_det max ORIGINAL sample= TRUNCATED)\n",
    "#k_min, k_max = -7.5, 0.5\n",
    "k_min, k_max = X_log[:, 3].min(), X_log[:, 3].max()\n",
    "\n",
    "print(x_min,x_max,y_min,y_max,z_min,z_max,k_min,k_max)\n",
    "\n",
    "hx=0.1\n",
    "hy=0.1  # step size in the mesh\n",
    "hz=0.1\n",
    "hk=0.1\n",
    "\n",
    "xx, yy, zz, kk= np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy),np.arange(z_min, z_max, hz),np.arange(k_min, k_max, hk))\n",
    "\n",
    "#xx, yy, zz, kk= np.meshgrid(np.linspace(x_min, x_max, 100, endpoint=True), np.linspace(y_min, y_max, 120, endpoint=True),np.linspace(z_min, z_max, 100, endpoint=True),np.linspace(k_min, k_max, 100, endpoint=True))\n",
    "\n",
    "Z_classifier_sample= ANN_fit.predict_proba(np.c_[xx.ravel(), yy.ravel(),zz.ravel(), kk.ravel()])\n",
    "\n",
    "print(Z_classifier_sample.shape)\n",
    "print(Z_classifier_sample[0:5])\n",
    "#print(Z_liblin[1267728-5:1267728])\n",
    "print(xx.shape)\n",
    "print(yy.shape)\n",
    "#print(zz[1,:].shape)\n",
    "#print(xx[:,0:5])\n",
    "print('xsize',np.arange(x_min, x_max, hx).shape)\n",
    "print('ysize',np.arange(y_min, y_max, hy).shape)\n",
    "#print('zsize',np.arange(z_min, z_max, hz).shape)\n",
    "print(xx.shape)\n",
    "#print(yy[:,:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_color=np.asarray(Y_test_split[:],dtype='str')\n",
    "\n",
    "print(len(Y_test_split))\n",
    "print(Y_test_color.shape)\n",
    "\n",
    "for i in range(0,len(Y_test_split)):\n",
    "    if Y_test_split[i]==1:\n",
    "        Y_test_color[i]='k'\n",
    "    elif Y_test_split[i]==0:\n",
    "         Y_test_color[i]='w'\n",
    "            \n",
    "\n",
    "print(Y_test_color) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "#Z_liblin = Z_liblin.reshape((xx.shape, yy.shape))\n",
    "Z_classifier_sample_1 = Z_classifier_sample[:,1].reshape(yy.shape)\n",
    "print(Z_classifier_sample.shape)\n",
    "#f=plt.figure(1, figsize=(4, 3))\n",
    "f=plt.figure()\n",
    "plt.pcolormesh(xx[:,:,0,0], yy[:,:,0,0], Z_classifier_sample_1[:,:,0,0])\n",
    "#plt.colorbar()\n",
    "#plt.pcolormesh(xx, yy, Z_classifier_0, cmap=plt.cm.PiYG)\n",
    "\n",
    "# Plot also the training points\n",
    "\n",
    "Y_test_split_proba_std_check\n",
    "\n",
    "test=plt.scatter(X_test_split[:, 0],X_test_split[:, 1], c=Y_test_split_01_std_check[:], edgecolors=Y_test_color[:])\n",
    "#train=plt.scatter(X_train_split[:, 0],X_test[:, 1], c=Y_test_classifier[:,1], edgecolors='k')\n",
    "#unids=plt.scatter(unids_cut[:, 2], unids_cut[:, 0], c=Y_unids_liblin, edgecolors='m', cmap=plt.cm.Paired)\n",
    "#test=plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_test_fit_liblin, edgecolors='c', cmap=plt.cm.Paired)\n",
    "\n",
    "#plt.yscale('log',basey=10) \n",
    "#plt.xscale('log',basex=10) \n",
    "plt.title('Last class')\n",
    "plt.ylabel('Log(beta)')\n",
    "plt.xlabel('Log(Epeak)')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#plt.ylim(zz.min(), zz.max()+1000)\n",
    "#plt.legend((test, ), ('test',))\n",
    "\n",
    "\n",
    "#plt.legend((train, test, unids),\n",
    " #          ('train', 'test', 'unids'),\n",
    "  #         scatterpoints=1,\n",
    "   #        loc='lower left',\n",
    "    #       ncol=3,\n",
    "     #      fontsize=8)\n",
    "#plt.show()\n",
    "\n",
    "#plt.imshow(Z_classifier_sample_1[:,:,0,0],aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.clim(0.0,1.0);\n",
    "\n",
    "\n",
    "#f.savefig(\"NN_proba_4F_bal.pdf\", bbox_inches='tight')\n",
    "#f.savefig(\"plot/test_Fit11_prob_class_E_beta.jpg\", format='jpg',bbox_inches='tight', dpi=100, quality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the result into a color plot\n",
    "#Z_liblin = Z_liblin.reshape((xx.shape, yy.shape))\n",
    "Z_classifier_sample_1 = Z_classifier_sample[:,1].reshape(yy.shape)\n",
    "print(Z_classifier_sample.shape)\n",
    "#f=plt.figure(1, figsize=(4, 3))\n",
    "f=plt.figure()\n",
    "plt.pcolormesh(xx[:,:,0,0], yy[:,:,0,0], Z_classifier_sample_1[:,:,0,0])\n",
    "plt.colorbar()\n",
    "plt.clim(0.0,1.0);\n",
    "#plt.pcolormesh(xx, yy, Z_classifier_0, cmap=plt.cm.PiYG)\n",
    "\n",
    "# Plot also the training points\n",
    "\n",
    "\n",
    "#test=plt.scatter(X_test_split[:, 0],X_test_split[:, 1], c=Y_test_split_proba_std_check[:,1], edgecolors=Y_test_color[:])\n",
    "\n",
    "#test=plt.scatter(X_test[:, 0],X_test[:, 1], c=Y_test, edgecolors='k', cmap=plt.cm.PiYG)\n",
    "#test=plt.scatter(X_test[:, 0],X_test[:, 1], c=Y_test_classifier[:,1], edgecolors='k')\n",
    "unids=plt.scatter(unids_log[:, 0], unids_log[:, 1], c=unIDs_std_proba_check_repeated_kfold[:,1], edgecolors='c')\n",
    "#test=plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_test_fit_liblin, edgecolors='c', cmap=plt.cm.Paired)\n",
    "\n",
    "#plt.yscale('log',basey=10) \n",
    "#plt.xscale('log',basex=10) \n",
    "plt.title('ANN- Adam  - 4F - Bal -  UnIDs')\n",
    "plt.ylabel('Log(beta)')\n",
    "plt.xlabel('Log(Epeak)')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#plt.ylim(zz.min(), zz.max()+1000)\n",
    "#plt.legend('train')\n",
    "\n",
    "\n",
    "plt.legend((unids,),\n",
    "          ('unids',),\n",
    "          scatterpoints=1,\n",
    "           loc='lower left',\n",
    "           ncol=3,\n",
    "          fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "#plt.imshow(Z_classifier_sample_1[:,:,0,0],aspect='auto')\n",
    "\n",
    "\n",
    "\n",
    "#f.savefig(\"NN_4F_BAL_unIDs.pdf\", bbox_inches='tight')\n",
    "#f.savefig(\"plot/test_Fit11_prob_class_E_beta.jpg\", format='jpg',bbox_inches='tight', dpi=100, quality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OA.shape)\n",
    "#print(OA)\n",
    "print('OA mean', OA.mean(), 'OA std',OA.std(ddof=1))\n",
    "print('TN mean', TN.mean(), 'TN std',TN.std(ddof=1))\n",
    "print('TP mean', TP.mean(), 'TP std',TP.std(ddof=1))\n",
    "print(unIDs_std_proba_check_repeated_kfold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "f = plt.figure()\n",
    "\n",
    "for j in range(0,len(X_train_split_std)):\n",
    "        if 10**X_train_split[j,2]>=20:      \n",
    "            plt.errorbar(10**X_train_split[j,0],10**X_train_split[j,1], yerr=10**X_train_split[j,3]*10**X_train_split[j,1], fmt=\".\", color='orange',label='X_train_split')\n",
    "\n",
    "for i in range(0,len(unids_log)):\n",
    "    if 10**unids_log[i,2]>=20:\n",
    "            plt.errorbar(10**unids_log[i,0],10**unids_log[i,1], yerr=10**unids_log[i,3]*10**unids_log[i,1], fmt=\".\", color='red',label='unIDs')    \n",
    "\n",
    "#plt.errorbar(10**unids_std_check[i,0],10**unids_std_check[i,1], yerr=10**unids_std_check[i,3]*10**unids_std_check[i,1], fmt=\".\", color='red')\n",
    "        \n",
    "        #and X_train_split_std[j,2]>=50: \n",
    "#plt.scatter(10**unids_log[:,0],10**unids_log[:,1], color='red',label='unids log',s=5)\n",
    "#plt.errorbar(10**unids_log[:,0],10**unids_log[:,1], yerr=10**unids_log[:,3]*10**unids_log[:,1],\n",
    "            # fmt=\".\", color='red',label='unIDs')\n",
    "#plt.scatter(np.log10(unids_3F_data[:,0]),np.log10(unids_3F_data[:,1]), \n",
    " #         color='green',label='unids lin data',s=1)\n",
    "#plt.errorbar(10**X_train_split_std[:,0],10**X_train_split_std[:,1], yerr=10**X_train_split_std[:,3]*10**X_train_split_std[:,1],\n",
    "            # fmt=\".\", color='orange',label='X_train_split')     \n",
    "           \n",
    "#plt.scatter(unids_std_check[:,0],unids_std_check[:,1],\n",
    "        #  color='blue',label='unids norm',s=1)\n",
    "  # yerr=10**unids_log[index_selected[i],3]*10**unids_log[index_selected[i],1]\n",
    "          # plt.scatter(10**X_train_split_std[j,0],10**X_train_split_std[j,1], color='orange',label='X_train_split',s=1)\n",
    "#plt.errorbar(selected_unIDs_80[:,0], selected_unIDs_80[:,1], yerr=selected_unIDs_80[:,3], fmt=\"o\")\n",
    "\n",
    "plt.ylabel(r' $\\beta$')\n",
    "plt.xlabel(r' $E_{peak}$')\n",
    "plt.yscale('log',base=10) \n",
    "plt.xscale('log',base=10) \n",
    "\n",
    "orange_patch = mpatches.Patch(color='orange', label='Training set')\n",
    "red_patch = mpatches.Patch(color='red', label='unIDs')\n",
    "plt.legend(handles=[orange_patch,red_patch])\n",
    "\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#fig.savefig(\"Trainig_unIDs_errors_sigma20.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    )\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n",
    "    ax.set(\n",
    "        yticks=np.arange(n_splits + 2) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[n_splits + 2.2, -0.5],\n",
    "        xlim=[0, len(X_log)],\n",
    "    )\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cv = rskf\n",
    "plot_cv_indices(cv, X_log , Y, Y ,ax, N_splits*N_Repeats)\n",
    "ax.axes\n",
    "ax.legend(\n",
    "        [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.2))],\n",
    "        [\"Testing set\", \"Training set\"],\n",
    "        loc=(1.02, 0.8),\n",
    "    )\n",
    "\n",
    "#fig.savefig(\"Reapeated5_Kfold5_split.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unIDs classification with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unids_3F = np.genfromtxt('../data/unids_3F_beta_err_names.txt',dtype='str') \n",
    "unids_3F_data = np.asarray(unids_3F[1::,:],dtype=float)\n",
    "\n",
    "print(unids_3F[0,:])\n",
    "\n",
    "unids_log=np.log10(unids_3F_data[:,[0,1,2,3]])\n",
    "print(unids_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unids_DM_std_proba_repeated_kfold= np.genfromtxt('../data/results/ann/4F/unids_DM_std_proba_check_repeated_kfold_rskf_4F_21.txt',dtype='str') \n",
    "unids_DM_std_proba_data_repeated_kfold=np.asarray(unids_DM_std_proba_repeated_kfold[1::],dtype=float)\n",
    "print(unids_DM_std_proba_data_repeated_kfold[0,:])\n",
    "print(unids_DM_std_proba_data_repeated_kfold[1,:])\n",
    "\n",
    "#N_splits=5\n",
    "#N_Repeats=20\n",
    "\n",
    "print('unids_DM_std_proba_data_repeated_kfold.shape',unids_DM_std_proba_data_repeated_kfold.shape)\n",
    "print(unids_DM_std_proba_data_repeated_kfold[0,0:3])\n",
    "print(unids_DM_std_proba_data_repeated_kfold[1,0:3])\n",
    "print(unids_DM_std_proba_data_repeated_kfold[2,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defining an array where each column is the probability to be DM for each unIDs in each split\n",
    "\n",
    "import itertools \n",
    "\n",
    "N_unids=unids_log.shape[0]\n",
    "print('N_unids',N_unids)\n",
    "\n",
    "N_sample=N_splits*N_Repeats\n",
    "print('N_sample',N_sample)\n",
    "\n",
    "print(unids_DM_std_proba_data_repeated_kfold.shape)\n",
    "\n",
    "unids_number=unids_DM_std_proba_data_repeated_kfold[0:N_unids,0]\n",
    "\n",
    "print('unids number',unids_number)\n",
    "print('unids number shape',unids_number.shape)\n",
    "\n",
    "unids_DM_std_proba_N_sample_repeated_kfold=np.zeros((N_unids,(N_sample+1)))\n",
    "\n",
    "unids_DM_std_proba_N_sample_repeated_kfold[:,0]=unids_number[:].astype(int)\n",
    "unids_DM_std_proba_N_sample_repeated_kfold[:,1:(N_sample+1)]=np.reshape(\n",
    "unids_DM_std_proba_data_repeated_kfold[:,1],(N_unids,(N_sample)))\n",
    "\n",
    "print('unids_DM_std_proba_N_sample_repeated_kfold.shape', \n",
    "      unids_DM_std_proba_N_sample_repeated_kfold.shape)\n",
    "print(unids_DM_std_proba_N_sample_repeated_kfold)\n",
    "\n",
    "#unids_DM_std_proba_N_sample_repeated_kfold=np.array(unids_DM_std_proba_N_sample_repeated_kfold)\n",
    "\n",
    "#unids_int_prob=float((unids_DM_std_proba_N_sample_repeated_kfold[:,1].T))\n",
    "#print(unids_int_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unids_mean=unids_DM_std_proba_N_sample_repeated_kfold[:,1:(N_sample+1)].mean(axis=1)\n",
    "unids_std=unids_DM_std_proba_N_sample_repeated_kfold[:,1:(N_sample+1)].std(axis=1,ddof=1)\n",
    "print('unids_mean', unids_mean, 'unids std',unids_std)\n",
    "print(unids_mean.shape)\n",
    "p_cut=0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "#ax0, ax1, ax2 = axes.flatten()\n",
    "\n",
    "#for i in range(0,len(unids_DM_std_proba_N_sample_repeated_kfold)):\n",
    "counts_all, bins_all, ignored = plt.hist(unids_DM_std_proba_N_sample_repeated_kfold[:,1:(N_sample+1)], bins,histtype='bar', stacked='True', density=False, label='all p hist')\n",
    "plt.axvline(0.50, color='magenta', linestyle=':', linewidth=2)\n",
    "plt.axvline(0.68, color='red', linestyle=':', linewidth=2)\n",
    "plt.axvline(0.95, color='blue', linestyle='--', linewidth=2)\n",
    "plt.axvline(0.99, color='black', linestyle='-', linewidth=2)\n",
    "#plt.axvline(value_unID, color='k', linestyle='-', linewidth=1)\n",
    "#plt.axvline(unids_std.all(), color='green', linestyle='-', linewidth=1)\n",
    "#plt.axvline(unids_std.all(), color='green', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.xlabel(r' $p_k(DM)$',size=20)\n",
    "plt.ylabel('count',size=20)\n",
    "\n",
    "\n",
    "#fig.savefig(\"full_histo_4F.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_color=np.asarray(unids_DM_std_proba_N_sample_repeated_kfold[:,1:(N_sample+1)],dtype='str')\n",
    "\n",
    "print(all_color.shape)\n",
    "\n",
    "for i in range(0,len(unids_DM_std_proba_N_sample_repeated_kfold[:,1:(N_sample+1)])):\n",
    "    for j in range(0,N_sample):\n",
    "        all_color[i,j]='midnightblue'\n",
    "\n",
    "print(len(unids_log))    \n",
    "print(all_color.shape)\n",
    "print(all_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "#ax0, ax1, ax2 = axes.flatten()\n",
    "\n",
    "counts_all, bins_all, ignored = plt.hist(unids_DM_std_proba_N_sample_repeated_kfold[:,1:(N_sample+1)], \n",
    "                                         bins, histtype='barstacked',density=False, color=all_color[i,:])\n",
    "plt.axvline(0.50, color='magenta', linestyle=':', linewidth=1)\n",
    "plt.axvline(0.68, color='red', linestyle='-.', linewidth=1)\n",
    "plt.axvline(0.90, color='blue', linestyle='--', linewidth=1)\n",
    "plt.axvline(0.95, color='black', linestyle='--', linewidth=1)\n",
    "plt.axvline(0.99, color='gray', linestyle='--', linewidth=1)\n",
    "#plt.axvline(value_unID, color='k', linestyle='-', linewidth=1)\n",
    "#plt.axvline(unids_std.all(), color='green', linestyle='-', linewidth=1)\n",
    "#plt.axvline(unids_std.all(), color='green', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.xlabel(r' $p_k^{DM}$',size=15)\n",
    "plt.ylabel('count',size=15)\n",
    "plt.title(r' $NN, 4F$', y=10**(0), x=10**(-0.5), pad=-30)\n",
    "\n",
    "#fig.savefig(\"full_histo_4F_single_count.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFG Marta - Additional analysis 4F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_splits = 5\n",
    "N_Repeats = 1\n",
    "N_sample = N_splits * N_Repeats\n",
    "\n",
    "print(f\"Cross-validation setup: {N_splits} splits √ó {N_Repeats} repeats = {N_sample} total folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unID source data\n",
    "unids_3F = np.genfromtxt('../data/unids_3F_beta_err_names.txt', dtype='str') \n",
    "unids_3F_data = np.asarray(unids_3F[1::, :], dtype=float)\n",
    "feature_names = unids_3F[0, :]\n",
    "\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"UnID data shape: {unids_3F_data.shape}\")\n",
    "\n",
    "# Create log-transformed features - CAMBIO: usar las 4 caracter√≠sticas\n",
    "unids_log = np.log10(unids_3F_data[:, [0, 1, 2, 3]])  # E_peak, beta, sigma_det, beta_Rel\n",
    "N_unids = len(unids_log)\n",
    "print(f\"Log-transformed features shape: {unids_log.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Extract probability data (excluding index column)\n",
    "prob_data = unids_DM_std_proba_N_sample_repeated_kfold[:, 1:(N_sample + 1)]\n",
    "\n",
    "# Basic statistics\n",
    "unids_mean = prob_data.mean(axis=1)\n",
    "unids_std = prob_data.std(axis=1, ddof=1)\n",
    "unids_median = np.median(prob_data, axis=1)\n",
    "unids_q25 = np.percentile(prob_data, 25, axis=1)\n",
    "unids_q75 = np.percentile(prob_data, 75, axis=1)\n",
    "unids_min = prob_data.min(axis=1)\n",
    "unids_max = prob_data.max(axis=1)\n",
    "\n",
    "unids_iqr = unids_q75 - unids_q25\n",
    "unids_cv = unids_std / (unids_mean + 1e-8)  # Coeficiente de variaci√≥n\n",
    "unids_range = unids_max - unids_min\n",
    "unids_skew = stats.skew(prob_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nOVERALL DATASET STATISTICS:\")\n",
    "print(f\"Total sources analyzed: {N_unids}\")\n",
    "print(f\"Mean probability: {unids_mean.mean():.4f} ¬± {unids_mean.std():.4f}\")\n",
    "print(f\"Mean uncertainty (std): {unids_std.mean():.4f} ¬± {unids_std.std():.4f}\")\n",
    "print(f\"Median probability: {np.median(unids_mean):.4f}\")\n",
    "print(f\"Mean coefficient of variation: {unids_cv.mean():.4f}\")\n",
    "print(f\"Sources with CV < 0.2 (low variability): {np.sum(unids_cv < 0.2)}\")\n",
    "\n",
    "# Estad√≠sticas de las 4 caracter√≠sticas\n",
    "print(f\"\\nFEATURE STATISTICS:\")\n",
    "feature_stats = {\n",
    "    'E_peak': unids_3F_data[:, 0],\n",
    "    'Beta': unids_3F_data[:, 1], \n",
    "    'Sigma_det': unids_3F_data[:, 2],\n",
    "    'Beta_Rel': unids_3F_data[:, 3]\n",
    "}\n",
    "\n",
    "for name, values in feature_stats.items():\n",
    "    print(f\"{name}: [{values.min():.3e}, {values.max():.3e}] (mean: {values.mean():.3e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "p_cut_very_high = 0.95\n",
    "p_cut_high = 0.90\n",
    "p_cut_moderate = 0.50\n",
    "uncertainty_threshold = 0.1\n",
    "consistency_threshold = 0.2\n",
    "\n",
    "# Create masks\n",
    "very_high_conf_mask = unids_mean >= p_cut_very_high\n",
    "high_conf_mask = (unids_mean >= p_cut_high) & (unids_mean < p_cut_very_high)\n",
    "moderate_conf_mask = (unids_mean >= p_cut_moderate) & (unids_mean < p_cut_high)\n",
    "low_uncertainty_mask = unids_std <= uncertainty_threshold\n",
    "consistent_mask = unids_cv <= consistency_threshold\n",
    "\n",
    "premium_candidates_mask = (unids_mean >= p_cut_high) & (unids_std <= uncertainty_threshold)\n",
    "stable_candidates_mask = unids_cv <= 0.15\n",
    "\n",
    "# Get indices\n",
    "very_high_conf_indices = np.where(very_high_conf_mask)[0]\n",
    "high_conf_indices = np.where(high_conf_mask)[0]\n",
    "moderate_conf_indices = np.where(moderate_conf_mask)[0]\n",
    "low_uncertainty_indices = np.where(low_uncertainty_mask)[0]\n",
    "premium_candidates_indices = np.where(premium_candidates_mask)[0]\n",
    "stable_candidates_indices = np.where(stable_candidates_mask)[0]\n",
    "\n",
    "# Report counts\n",
    "print(f\"VERY HIGH CONFIDENCE (‚â•{p_cut_very_high:.0%}): {len(very_high_conf_indices)}\")\n",
    "print(f\"HIGH CONFIDENCE ({p_cut_high:.0%}-{p_cut_very_high:.0%}): {len(high_conf_indices)}\")\n",
    "print(f\"MODERATE CONFIDENCE ({p_cut_moderate:.0%}-{p_cut_high:.0%}): {len(moderate_conf_indices)}\")\n",
    "print(f\"LOW UNCERTAINTY (std ‚â§{uncertainty_threshold}): {len(low_uncertainty_indices)}\")\n",
    "print(f\"PREMIUM CANDIDATES (high prob + low uncert): {len(premium_candidates_indices)}\")\n",
    "print(f\"STABLE CANDIDATES (CV ‚â§ 0.15): {len(stable_candidates_indices)}\")\n",
    "\n",
    "# Show top candidates if any\n",
    "if len(very_high_conf_indices) > 0:\n",
    "    print(f\"\\nTOP VERY HIGH CONFIDENCE CANDIDATES:\")\n",
    "    for i in very_high_conf_indices[:min(5, len(very_high_conf_indices))]:\n",
    "        print(f\"  Source {i}: p={unids_mean[i]:.4f} ¬± {unids_std[i]:.4f} (CV={unids_cv[i]:.3f})\")\n",
    "\n",
    "if len(premium_candidates_indices) > 0:\n",
    "    print(f\"\\nPREMIUM CANDIDATES (high prob + low uncertainty):\")\n",
    "    for i in premium_candidates_indices[:min(5, len(premium_candidates_indices))]:\n",
    "        print(f\"  Source {i}: p={unids_mean[i]:.4f} ¬± {unids_std[i]:.4f} (CV={unids_cv[i]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive probability distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. All predictions histogram (original style)\n",
    "ax1 = axes[0, 0]\n",
    "bins = np.arange(0.0, 1.1, 0.1)\n",
    "counts, bins_edges, _ = ax1.hist(prob_data.flatten(), bins=bins, \n",
    "                                alpha=0.7, color='skyblue', edgecolor='black')\n",
    "\n",
    "ax1.axvline(0.50, color='magenta', linestyle=':', linewidth=2, label='p=0.50')\n",
    "ax1.axvline(0.68, color='red', linestyle=':', linewidth=2, label='p=0.68')\n",
    "ax1.axvline(0.90, color='blue', linestyle='--', linewidth=2, label='p=0.90')\n",
    "ax1.axvline(0.95, color='green', linestyle='--', linewidth=2, label='p=0.95')\n",
    "ax1.axvline(0.99, color='black', linestyle='-', linewidth=2, label='p=0.99')\n",
    "\n",
    "ax1.set_xlabel('DM Probability')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Distribution of All CV Predictions')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Mean probability distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(unids_mean, bins=bins, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "ax2.axvline(0.50, color='magenta', linestyle=':', linewidth=2, label='p=0.50')\n",
    "ax2.axvline(0.90, color='blue', linestyle='--', linewidth=2, label='p=0.90')\n",
    "ax2.axvline(0.95, color='green', linestyle='--', linewidth=2, label='p=0.95')\n",
    "ax2.set_xlabel('Mean DM Probability')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distribution of Mean Predictions')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Uncertainty distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(unids_std, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "ax3.axvline(0.1, color='red', linestyle='--', linewidth=2, label='std=0.1')\n",
    "ax3.axvline(0.2, color='orange', linestyle='--', linewidth=2, label='std=0.2')\n",
    "ax3.set_xlabel('Standard Deviation')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Prediction Uncertainty Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Mean vs Std scatter plot\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(unids_mean, unids_std, alpha=0.6, c=unids_mean, \n",
    "                     cmap='viridis', s=30)\n",
    "plt.colorbar(scatter, ax=ax4, label='Mean Probability')\n",
    "ax4.axvline(0.50, color='magenta', linestyle=':', alpha=0.7, label='p=0.50')\n",
    "ax4.axvline(0.90, color='blue', linestyle='--', alpha=0.7, label='p=0.90')\n",
    "ax4.axhline(0.1, color='red', linestyle='--', alpha=0.7, label='std=0.1')\n",
    "ax4.set_xlabel('Mean DM Probability')\n",
    "ax4.set_ylabel('Standard Deviation')\n",
    "ax4.set_title('Mean vs Uncertainty')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature space analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Feature space colored by mean probability\n",
    "ax1 = axes[0]\n",
    "scatter = ax1.scatter(unids_log[:, 0], unids_log[:, 1], \n",
    "                     c=unids_mean, cmap='plasma', alpha=0.7, s=40)\n",
    "plt.colorbar(scatter, ax=ax1, label='Mean DM Prob')\n",
    "ax1.set_xlabel('log10(E_peak)')\n",
    "ax1.set_ylabel('log10(beta)')\n",
    "ax1.set_title('Feature Space: Colored by DM Probability')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight high confidence candidates\n",
    "if len(high_conf_indices) > 0:\n",
    "    ax1.scatter(unids_log[high_conf_indices, 0], unids_log[high_conf_indices, 1], \n",
    "               s=100, facecolors='none', edgecolors='red', linewidth=2, \n",
    "               label=f'High Conf (‚â•{p_cut_high:.0%})')\n",
    "    ax1.legend()\n",
    "\n",
    "# Feature space colored by uncertainty\n",
    "ax2 = axes[1]\n",
    "scatter2 = ax2.scatter(unids_log[:, 0], unids_log[:, 1], \n",
    "                      c=unids_std, cmap='viridis_r', alpha=0.7, s=40)\n",
    "plt.colorbar(scatter2, ax=ax2, label='Std Dev')\n",
    "ax2.set_xlabel('log10(E_peak)')\n",
    "ax2.set_ylabel('log10(beta)')\n",
    "ax2.set_title('Feature Space: Colored by Uncertainty')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight low uncertainty candidates\n",
    "if len(low_uncertainty_indices) > 0:\n",
    "    ax2.scatter(unids_log[low_uncertainty_indices, 0], unids_log[low_uncertainty_indices, 1], \n",
    "               s=100, facecolors='none', edgecolors='red', linewidth=2, \n",
    "               label=f'Low Uncert (‚â§{uncertainty_threshold})')\n",
    "    ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 candidates\n",
    "top_20_indices = np.argsort(unids_mean)[-20:]\n",
    "top_candidates_df = pd.DataFrame({\n",
    "    'Source_ID': top_20_indices,\n",
    "    'Mean_Prob': unids_mean[top_20_indices],\n",
    "    'Std_Dev': unids_std[top_20_indices],\n",
    "    'Coeff_Var': unids_cv[top_20_indices],\n",
    "    'Median_Prob': unids_median[top_20_indices],\n",
    "    'IQR': unids_iqr[top_20_indices],\n",
    "    'Min_Prob': unids_min[top_20_indices],\n",
    "    'Max_Prob': unids_max[top_20_indices],\n",
    "    'Range': unids_range[top_20_indices],\n",
    "    'Skewness': unids_skew[top_20_indices],\n",
    "    # Caracter√≠sticas originales\n",
    "    'E_peak': unids_3F_data[top_20_indices, 0],\n",
    "    'Beta': unids_3F_data[top_20_indices, 1],\n",
    "    'Sigma_det': unids_3F_data[top_20_indices, 2],\n",
    "    'Beta_Rel': unids_3F_data[top_20_indices, 3],\n",
    "    # Log-transformadas\n",
    "    'log_E_peak': unids_log[top_20_indices, 0],\n",
    "    'log_Beta': unids_log[top_20_indices, 1],\n",
    "    'log_Sigma_det': unids_log[top_20_indices, 2],\n",
    "    'log_Beta_Rel': unids_log[top_20_indices, 3]\n",
    "})\n",
    "\n",
    "top_candidates_df = top_candidates_df.sort_values('Mean_Prob', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"TOP 20 DARK MATTER CANDIDATES - ANN 4F:\")\n",
    "display(top_candidates_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Source_ID': range(N_unids),\n",
    "    'Mean_Prob': unids_mean,\n",
    "    'Std_Dev': unids_std,\n",
    "    'Coeff_Var': unids_cv,\n",
    "    'Median_Prob': unids_median,\n",
    "    'Q25_Prob': unids_q25,\n",
    "    'Q75_Prob': unids_q75,\n",
    "    'IQR': unids_iqr,\n",
    "    'Min_Prob': unids_min,\n",
    "    'Max_Prob': unids_max,\n",
    "    'Range': unids_range,\n",
    "    'Skewness': unids_skew,\n",
    "    # Caracter√≠sticas originales\n",
    "    'E_peak': unids_3F_data[:, 0],\n",
    "    'Beta': unids_3F_data[:, 1],\n",
    "    'Sigma_det': unids_3F_data[:, 2],\n",
    "    'Beta_Rel': unids_3F_data[:, 3],\n",
    "    # Log-transformadas\n",
    "    'log_E_peak': unids_log[:, 0],\n",
    "    'log_Beta': unids_log[:, 1],\n",
    "    'log_Sigma_det': unids_log[:, 2],\n",
    "    'log_Beta_Rel': unids_log[:, 3]\n",
    "})\n",
    "\n",
    "# Add classification flags\n",
    "results_df['Very_High_Conf'] = very_high_conf_mask\n",
    "results_df['High_Conf'] = high_conf_mask\n",
    "results_df['Moderate_Conf'] = moderate_conf_mask\n",
    "results_df['Low_Uncertainty'] = low_uncertainty_mask\n",
    "results_df['Consistent'] = consistent_mask\n",
    "results_df['Premium_Candidate'] = premium_candidates_mask\n",
    "results_df['Stable_Candidate'] = stable_candidates_mask\n",
    "\n",
    "# Sort by mean probability\n",
    "results_df = results_df.sort_values('Mean_Prob', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nQUALITY CONTROL ANALYSIS:\")\n",
    "\n",
    "# Identificar fuentes problem√°ticas\n",
    "high_variance_sources = np.where(unids_cv > 0.5)[0]\n",
    "extreme_skew_sources = np.where(np.abs(unids_skew) > 2)[0]\n",
    "\n",
    "print(f\"High variance sources (CV > 0.5): {len(high_variance_sources)}\")\n",
    "print(f\"Extreme skewness sources (|skew| > 2): {len(extreme_skew_sources)}\")\n",
    "\n",
    "if len(high_variance_sources) > 0:\n",
    "    print(\"Most variable predictions:\")\n",
    "    worst_cv_indices = high_variance_sources[np.argsort(unids_cv[high_variance_sources])[-3:]]\n",
    "    for i in worst_cv_indices:\n",
    "        print(f\"  Source {i}: p={unids_mean[i]:.4f} ¬± {unids_std[i]:.4f} (CV={unids_cv[i]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/results/ann/4F/unid_dm_analysis_complete_results_4F_improved.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nComplete results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSUMMARY BY CANDIDATE CATEGORY:\")\n",
    "categories = {\n",
    "    'Very_High_Conf': 'Very High Confidence',\n",
    "    'High_Conf': 'High Confidence',\n",
    "    'Moderate_Conf': 'Moderate Confidence', \n",
    "    'Low_Uncertainty': 'Low Uncertainty',\n",
    "    'Premium_Candidate': 'Premium Candidates',\n",
    "    'Stable_Candidate': 'Stable Candidates'\n",
    "}\n",
    "\n",
    "for cat_col, cat_name in categories.items():\n",
    "    count = results_df[cat_col].sum()\n",
    "    if count > 0:\n",
    "        subset = results_df[results_df[cat_col]]\n",
    "        mean_prob = subset['Mean_Prob'].mean()\n",
    "        mean_uncert = subset['Std_Dev'].mean()\n",
    "        mean_cv = subset['Coeff_Var'].mean()\n",
    "        print(f\"{cat_name}: {count} sources\")\n",
    "        print(f\"  ‚Üí Avg prob: {mean_prob:.3f}, Avg uncert: {mean_uncert:.3f}, Avg CV: {mean_cv:.3f}\")\n",
    "\n",
    "print(f\"\\nANN 4F analysis completed successfully!\")\n",
    "print(f\"Ready for consensus analysis and OCSVM 4F comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lisis de consenso entre 3 diferentes ejecuciones - 4F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los resultados de las 3 ejecuciones\n",
    "execution_1 = pd.read_csv('../data/results/ann/4F/unid_dm_analysis_complete_results_4F_run1.csv')\n",
    "execution_2 = pd.read_csv('../data/results/ann/4F/unid_dm_analysis_complete_results_4F_run2.csv')\n",
    "execution_3 = pd.read_csv('../data/results/ann/4F/unid_dm_analysis_complete_results_4F_run3.csv')\n",
    "\n",
    "print(f\"Ejecuci√≥n 1: {len(execution_1)} fuentes\")\n",
    "print(f\"Ejecuci√≥n 2: {len(execution_2)} fuentes\") \n",
    "print(f\"Ejecuci√≥n 3: {len(execution_3)} fuentes\")\n",
    "\n",
    "# Verificar que todas las ejecuciones tienen las mismas fuentes\n",
    "assert len(execution_1) == len(execution_2) == len(execution_3), \"Las ejecuciones tienen diferente n√∫mero de fuentes\"\n",
    "\n",
    "n_sources = len(execution_1)\n",
    "print(f\"Total de fuentes analizadas: {n_sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear DataFrame combinado con las probabilidades de las 3 ejecuciones\n",
    "consensus_df = pd.DataFrame({\n",
    "    'Source_ID': execution_1['Source_ID'],\n",
    "    # CAMBIO: Incluir las 4 caracter√≠sticas\n",
    "    'E_peak': execution_1['E_peak'],\n",
    "    'Beta': execution_1['Beta'],\n",
    "    'Sigma_det': execution_1['Sigma_det'],\n",
    "    'Beta_Rel': execution_1['Beta_Rel'],\n",
    "    # CAMBIO: Log-transformadas de las 4 caracter√≠sticas\n",
    "    'log_E_peak': execution_1['log_E_peak'],\n",
    "    'log_Beta': execution_1['log_Beta'],\n",
    "    'log_Sigma_det': execution_1['log_Sigma_det'],\n",
    "    'log_Beta_Rel': execution_1['log_Beta_Rel'],\n",
    "    # Probabilidades de las 3 ejecuciones\n",
    "    'Prob_Run1': execution_1['Mean_Prob'],\n",
    "    'Prob_Run2': execution_2['Mean_Prob'], \n",
    "    'Prob_Run3': execution_3['Mean_Prob'],\n",
    "    'Std_Run1': execution_1['Std_Dev'],\n",
    "    'Std_Run2': execution_2['Std_Dev'],\n",
    "    'Std_Run3': execution_3['Std_Dev']\n",
    "})\n",
    "\n",
    "consensus_df['Source_ID'] = consensus_df['Source_ID'].astype(int)\n",
    "\n",
    "# Calcular estad√≠sticas de consenso entre ejecuciones\n",
    "consensus_df['Mean_Prob_Consensus'] = consensus_df[['Prob_Run1', 'Prob_Run2', 'Prob_Run3']].mean(axis=1)\n",
    "consensus_df['Std_Prob_Consensus'] = consensus_df[['Prob_Run1', 'Prob_Run2', 'Prob_Run3']].std(axis=1, ddof=1)\n",
    "consensus_df['Min_Prob_Consensus'] = consensus_df[['Prob_Run1', 'Prob_Run2', 'Prob_Run3']].min(axis=1)\n",
    "consensus_df['Max_Prob_Consensus'] = consensus_df[['Prob_Run1', 'Prob_Run2', 'Prob_Run3']].max(axis=1)\n",
    "\n",
    "# Calcular incertidumbre promedio dentro de cada ejecuci√≥n\n",
    "consensus_df['Mean_Uncertainty_Consensus'] = consensus_df[['Std_Run1', 'Std_Run2', 'Std_Run3']].mean(axis=1)\n",
    "\n",
    "print(\"Estad√≠sticas de consenso calculadas (ANN 4F):\")\n",
    "print(f\"Probabilidad media general: {consensus_df['Mean_Prob_Consensus'].mean():.4f}\")\n",
    "print(f\"Variabilidad entre ejecuciones: {consensus_df['Std_Prob_Consensus'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir criterios de robustez\n",
    "MIN_PROB_THRESHOLD = 0.40  # Probabilidad m√≠nima en todas las ejecuciones\n",
    "MEAN_PROB_THRESHOLD = 0.45  # Probabilidad media entre ejecuciones\n",
    "MAX_VARIABILITY = 0.15     # M√°xima variabilidad entre ejecuciones\n",
    "\n",
    "# Identificar candidatos robustos\n",
    "robust_candidates = consensus_df[\n",
    "    (consensus_df['Min_Prob_Consensus'] >= MIN_PROB_THRESHOLD) &\n",
    "    (consensus_df['Mean_Prob_Consensus'] >= MEAN_PROB_THRESHOLD) &\n",
    "    (consensus_df['Std_Prob_Consensus'] <= MAX_VARIABILITY)\n",
    "].copy()\n",
    "\n",
    "# Identificar candidatos consistentemente altos\n",
    "consistent_high = consensus_df[\n",
    "    (consensus_df['Mean_Prob_Consensus'] >= 0.45) &\n",
    "    (consensus_df['Std_Prob_Consensus'] <= 0.10)\n",
    "].copy()\n",
    "\n",
    "# Identificar candidatos que aparecen en top 20 de todas las ejecuciones\n",
    "top_20_run1 = set(execution_1.nlargest(20, 'Mean_Prob')['Source_ID'])\n",
    "top_20_run2 = set(execution_2.nlargest(20, 'Mean_Prob')['Source_ID'])\n",
    "top_20_run3 = set(execution_3.nlargest(20, 'Mean_Prob')['Source_ID'])\n",
    "\n",
    "# Candidatos que aparecen en top 20 de al menos 2 ejecuciones\n",
    "candidates_2_of_3 = (top_20_run1 & top_20_run2) | (top_20_run1 & top_20_run3) | (top_20_run2 & top_20_run3)\n",
    "# Candidatos que aparecen en top 20 de todas las ejecuciones\n",
    "candidates_all_3 = top_20_run1 & top_20_run2 & top_20_run3\n",
    "\n",
    "print(\"AN√ÅLISIS DE ROBUSTEZ:\")\n",
    "print(f\"Candidatos robustos (criterios m√∫ltiples): {len(robust_candidates)}\")\n",
    "print(f\"Candidatos consistentemente altos: {len(consistent_high)}\")\n",
    "print(f\"Candidatos en top 20 de 2/3 ejecuciones: {len(candidates_2_of_3)}\")\n",
    "print(f\"Candidatos en top 20 de 3/3 ejecuciones: {len(candidates_all_3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar por probabilidad media de consenso\n",
    "consensus_df_sorted = consensus_df.sort_values('Mean_Prob_Consensus', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"TOP 15 CANDIDATOS DE CONSENSO - ANN 4F:\")\n",
    "print(\"Source | Prob_Consensus | Variabilidad | Run1   | Run2   | Run3   | E_peak    | Beta     | Sigma_det | Beta_Rel\")\n",
    "\n",
    "for i in range(min(15, len(consensus_df_sorted))):\n",
    "    row = consensus_df_sorted.iloc[i]\n",
    "    source_id = int(row['Source_ID'])\n",
    "    mean_prob = float(row['Mean_Prob_Consensus'])\n",
    "    std_prob = float(row['Std_Prob_Consensus'])\n",
    "    prob1 = float(row['Prob_Run1'])\n",
    "    prob2 = float(row['Prob_Run2'])\n",
    "    prob3 = float(row['Prob_Run3'])\n",
    "    e_peak = float(row['E_peak'])\n",
    "    beta = float(row['Beta'])\n",
    "    # CAMBIO: Incluir las nuevas caracter√≠sticas\n",
    "    sigma_det = float(row['Sigma_det'])\n",
    "    beta_rel = float(row['Beta_Rel'])\n",
    "    \n",
    "    print(f\"{source_id:6d} | {mean_prob:12.4f} | {std_prob:10.4f} | \"\n",
    "          f\"{prob1:5.3f} | {prob2:5.3f} | {prob3:5.3f} | \"\n",
    "          f\"{e_peak:8.3e} | {beta:7.4f} | {sigma_det:8.4f} | {beta_rel:7.4f}\")\n",
    "\n",
    "# El resto del an√°lisis de candidatos permanece igual...\n",
    "print(f\"\\nCANDIDATOS EN TOP 20 DE TODAS LAS EJECUCIONES:\")\n",
    "if candidates_all_3:\n",
    "    for source_id in candidates_all_3:\n",
    "        source_id = int(source_id)\n",
    "        row = consensus_df[consensus_df['Source_ID'] == source_id].iloc[0]\n",
    "        mean_prob = float(row['Mean_Prob_Consensus'])\n",
    "        std_prob = float(row['Std_Prob_Consensus'])\n",
    "        print(f\"Source {source_id:4d}: {mean_prob:.4f} ¬± {std_prob:.4f}\")\n",
    "else:\n",
    "    print(\"No hay candidatos que aparezcan en el top 20 de todas las ejecuciones\")\n",
    "\n",
    "print(f\"\\nCANDIDATOS EN TOP 20 DE AL MENOS 2 EJECUCIONES:\")\n",
    "for source_id in candidates_2_of_3:\n",
    "    source_id = int(source_id)\n",
    "    row = consensus_df[consensus_df['Source_ID'] == source_id].iloc[0]\n",
    "    mean_prob = float(row['Mean_Prob_Consensus'])\n",
    "    std_prob = float(row['Std_Prob_Consensus'])\n",
    "    in_run1 = source_id in top_20_run1\n",
    "    in_run2 = source_id in top_20_run2  \n",
    "    in_run3 = source_id in top_20_run3\n",
    "    runs_in_top20 = sum([in_run1, in_run2, in_run3])\n",
    "    print(f\"Source {source_id:4d}: {mean_prob:.4f} ¬± {std_prob:.4f} \"\n",
    "          f\"(en {runs_in_top20}/3 ejecuciones)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_content_4f = f\"\"\"\n",
    "{'='*80}\n",
    "REPORTE DE CONSENSO - MODELO ANN 4F (3 EJECUCIONES)\n",
    "{'='*80}\n",
    "\n",
    "PAR√ÅMETROS DEL AN√ÅLISIS:\n",
    "- N√∫mero de fuentes analizadas: {n_sources}\n",
    "- Caracter√≠sticas utilizadas: E_peak, Beta, Sigma_det, Beta_Rel (4 caracter√≠sticas)\n",
    "- N√∫mero de ejecuciones independientes: 3\n",
    "- Validaci√≥n cruzada por ejecuci√≥n: 5 folds √ó 1 repetici√≥n = 5 evaluaciones\n",
    "\n",
    "ESTAD√çSTICAS DE CONSENSO:\n",
    "- Probabilidad media general: {consensus_df['Mean_Prob_Consensus'].mean():.4f} ¬± {consensus_df['Mean_Prob_Consensus'].std():.4f}\n",
    "- Variabilidad media entre ejecuciones: {consensus_df['Std_Prob_Consensus'].mean():.4f}\n",
    "- Rango de probabilidades: [{consensus_df['Mean_Prob_Consensus'].min():.4f}, {consensus_df['Mean_Prob_Consensus'].max():.4f}]\n",
    "\n",
    "ESTAD√çSTICAS DE CARACTER√çSTICAS (4F):\n",
    "- E_peak: [{consensus_df['E_peak'].min():.3e}, {consensus_df['E_peak'].max():.3e}]\n",
    "- Beta: [{consensus_df['Beta'].min():.4f}, {consensus_df['Beta'].max():.4f}]\n",
    "- Sigma_det: [{consensus_df['Sigma_det'].min():.4f}, {consensus_df['Sigma_det'].max():.4f}]\n",
    "- Beta_Rel: [{consensus_df['Beta_Rel'].min():.4f}, {consensus_df['Beta_Rel'].max():.4f}]\n",
    "\n",
    "CANDIDATOS ROBUSTOS:\n",
    "- Candidatos con criterios m√∫ltiples: {len(robust_candidates)}\n",
    "- Candidatos en top 20 de todas las ejecuciones: {len(candidates_all_3)}\n",
    "- Candidatos en top 20 de al menos 2 ejecuciones: {len(candidates_2_of_3)}\n",
    "- Candidatos consistentemente altos: {len(consistent_high)}\n",
    "\n",
    "TOP 10 CANDIDATOS DE CONSENSO:\n",
    "{'-'*70}\n",
    "\"\"\"\n",
    "\n",
    "for i in range(min(10, len(consensus_df_sorted))):\n",
    "    row = consensus_df_sorted.iloc[i]\n",
    "    source_id = int(row['Source_ID'])\n",
    "    mean_prob = float(row['Mean_Prob_Consensus'])\n",
    "    std_prob = float(row['Std_Prob_Consensus'])\n",
    "    e_peak = float(row['E_peak'])\n",
    "    beta = float(row['Beta'])\n",
    "    sigma_det = float(row['Sigma_det'])\n",
    "    beta_rel = float(row['Beta_Rel'])\n",
    "    \n",
    "    report_content_4f += f\"Source {source_id:4d}: p={mean_prob:.4f}¬±{std_prob:.4f}\\\\n\"\n",
    "    report_content_4f += f\"  E_peak={e_peak:.3e}, Beta={beta:.4f}\\\\n\"\n",
    "    report_content_4f += f\"  Sigma_det={sigma_det:.4f}, Beta_Rel={beta_rel:.4f}\\\\n\\\\n\"\n",
    "\n",
    "# Guardar reporte\n",
    "with open('../data/results/ann/4F/consensus_report_ann_4f.txt', 'w') as f:\n",
    "    f.write(report_content_4f)\n",
    "\n",
    "print(\"AN√ÅLISIS DE CONSENSO ANN 4F COMPLETADO:\")\n",
    "print(\"Archivos generados:\")\n",
    "print(\"- consensus_analysis_ann_4f.csv: Datos completos de consenso\")\n",
    "print(\"- consensus_report_ann_4f.txt: Reporte detallado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_df_sorted.to_csv('../data/results/ann/4F/consensus_analysis_ann_4f_improved.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv DarkMatter_TFG)",
   "language": "python",
   "name": "venv-tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
