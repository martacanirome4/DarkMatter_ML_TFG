{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modelo de OneClassSVM entrenado con 4F de datos Astro, y predicción sobre datos Unid (no identificados)\n",
    "\n",
    "**Proyecto**: Detección de posibles fuentes de materia oscura usando ML en datos Fermi-LAT  \n",
    "**Autor**: Marta Canino Romero  \n",
    "**Fecha**: febrero-mayo 20225\n",
    "\n",
    "---\n",
    "\n",
    "## Descripción:\n",
    "\n",
    "Este notebook aplica un modelo **One-Class SVM** entrenado con datos de fuentes astrofísicas conocidas (ASTRO) usando las siguientes características:\n",
    "\n",
    "- E_peak\n",
    "- beta\n",
    "- sigma\n",
    "- betaRel\n",
    "\n",
    "Este modelo se entrena para identificar anomalías que puedan corresponder a posibles fuentes de materia oscura (UNIDs) en los datos no identificados del catálogo 4FGL.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos específicos:\n",
    "\n",
    "- Entrenar modelo OCSVM con 4 features\n",
    "- Optimizar hiperparámetros (grid search sobre `nu` y `gamma`)\n",
    "- Evaluar sobre datos de validación y prueba\n",
    "- Aplicar modelo final sobre datos UNID para predicción\n",
    "- Comparar los resultados de anomalía/outliers para OCSVM 4F con los resultados de probabilidad de DM de la ANN 4F (sobre UNIDs)\n",
    "\n",
    "---\n",
    "\n",
    "## Entrada de datos:\n",
    "\n",
    "- '../data/astro_df.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"../data/astro_df.txt\"\n",
    "\n",
    "data_path = \"../data/astro_data_with_labels.txt\"\n",
    "\n",
    "df_astro = pd.read_csv(data_path, sep='\\s+')\n",
    "print(f\" Nombres de las columnas: {list(df_astro.columns)}\")\n",
    "\n",
    "# Renombramos la columna target por claridad\n",
    "df_astro = df_astro.rename(columns={\"astro_DM\": \"class\"})\n",
    "\n",
    "print(f\"Dataset cargado. Forma: {df_astro.shape}\")\n",
    "print(f\"Nombres de las columnas: {list(df_astro.columns)}\")\n",
    "\n",
    "display(df_astro.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selección de características\n",
    "features = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)']\n",
    "\"\"\"Establecemos la columna objetivo aunque al ser un problema de detección de anomalías no la utilizaremos para entrenar el modelo.\n",
    "Además de que todos los datos están etiquetados como 'astro_DM' = 0.0\"\"\"\n",
    "target = 'class'\n",
    "\n",
    "print(f\"Modelo 4D con features: {features}\")\n",
    "print(f\"Dimensionalidad del espacio: {len(features)}D\")\n",
    "\n",
    "# Comprobamos valores nulos\n",
    "print(\"\\n Valores faltantes por columna:\")\n",
    "print(df_astro[features + [target]].isnull().sum())\n",
    "\n",
    "print(\"\\n Muestra del dataset:\")\n",
    "display(df_astro[features + [target]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear figura con todas las combinaciones 2D\n",
    "features = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)']\n",
    "feature_pairs = list(combinations(features, 2))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (feat1, feat2) in enumerate(feature_pairs):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=df_astro,\n",
    "        x=feat1,\n",
    "        y=feat2,\n",
    "        color=\"cornflowerblue\",\n",
    "        edgecolor='k',\n",
    "        alpha=0.7,\n",
    "        s=40,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"2D ASTRO Data: {feat1} vs {feat2}\")\n",
    "    ax.set_xlabel(feat1)\n",
    "    ax.set_ylabel(feat2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear múltiples gráficos 3D con diferentes combinaciones\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Combinaciones 3D más representativas\n",
    "combinations_3d = [\n",
    "    ('Log(E_peak)', 'Log(beta)', 'Log(sigma)'),\n",
    "    ('Log(E_peak)', 'Log(beta)', 'Log(beta_Rel)'),\n",
    "    ('Log(E_peak)', 'Log(sigma)', 'Log(beta_Rel)'),\n",
    "    ('Log(beta)', 'Log(sigma)', 'Log(beta_Rel)')\n",
    "]\n",
    "\n",
    "for idx, (feat1, feat2, feat3) in enumerate(combinations_3d[:3]):  # Solo 3 gráficos\n",
    "    ax = fig.add_subplot(1, 3, idx+1, projection='3d')\n",
    "    \n",
    "    x = df_astro[feat1]\n",
    "    y = df_astro[feat2] \n",
    "    z = df_astro[feat3]\n",
    "    labels = df_astro['class']\n",
    "    \n",
    "    scatter = ax.scatter(x, y, z, c=labels, cmap='cool', edgecolor='k', alpha=0.7, s=30)\n",
    "    \n",
    "    ax.set_xlabel(feat1)\n",
    "    ax.set_ylabel(feat2)\n",
    "    ax.set_zlabel(feat3)\n",
    "    ax.set_title(f'3D ASTRO Data: {feat1}, {feat2}, {feat3}')\n",
    "    ax.view_init(elev=20, azim=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleccionamos las features dinámicamente\n",
    "X = df_astro[features].values\n",
    "y = df_astro[target].values\n",
    "\n",
    "print(f\"Forma del dataset: {X.shape}\")\n",
    "print(f\"Distribución de clases: {np.unique(y, return_counts=True)}\")\n",
    "\n",
    "# Como todos los datos son clase 0, stratify no es necesario y puede causar errores\n",
    "# Simplificamos a:\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo de barrido en malla de hiperparámetros para OneClassSVM\n",
    "# Sseleccionamos más abajo los valores manualmente para permitir al modelo obtener un nº de outliers razonable en las fuentes UNID\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "nu_values    = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05]\n",
    "gamma_values = ['scale', 'auto', 0.002, 0.001, 0.01, 0.02, 0.1, 1, 10]\n",
    "\n",
    "results        = []\n",
    "best_outliers  = np.inf\n",
    "best_model     = None\n",
    "best_params    = {}\n",
    "\n",
    "for nu in nu_values:\n",
    "    for gamma in gamma_values:\n",
    "        model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma)\n",
    "        model.fit(X_train_scaled)  # ← usar X_train_scaled aquí\n",
    "\n",
    "        # Predecir sobre validación escalada\n",
    "        preds       = model.predict(X_val_scaled)       # 1=inlier, -1=outlier\n",
    "        n_outliers  = np.sum(preds == -1)\n",
    "\n",
    "        results.append({\n",
    "            'nu': nu,\n",
    "            'gamma': gamma,\n",
    "            'val_outliers': n_outliers\n",
    "        })\n",
    "\n",
    "        # Elegir el modelo que arroje MENOS outliers\n",
    "        if n_outliers < best_outliers:\n",
    "            best_outliers = n_outliers\n",
    "            best_model    = model\n",
    "            best_params   = {'nu': nu, 'gamma': gamma}\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Mejor combinación de hiperparámetros:\")\n",
    "print(f\"   - nu    = {best_params['nu']}\")\n",
    "print(f\"   - gamma = {best_params['gamma']}\")\n",
    "print(f\"Outliers (val set): {best_outliers} de {len(X_val_scaled)} muestras\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "display(df_results.sort_values(by='val_outliers'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PARAMETROS DE OneClassSVM:\n",
    "\n",
    "- kernel: rbf (solo cambiar si conocemos la fórmula para la forma de la distribución)\n",
    "- gamma: \n",
    "    - coeficiente del kernel \n",
    "    - usado para hiperplanos no lineares\n",
    "    - influye en la forma de la frontera de decisión + el desempeño predictivo\n",
    "    - define la influencia de un único ejemplo de entrenamiento\n",
    "    - cuanto más grande, más cerca tienen que estar los otros ejemplos para 'verse afectados'\n",
    "    - (por defecto 'scale' (antes 'auto')) \n",
    "- nu: \n",
    "    - límite superior de la fracción de errores permitidos en entrenamiento \n",
    "    - límite inferior de la fracción de vectores de soporte en relación con nº total de datos de entrenamiento\n",
    "        -   ejemplo: si se establece en 0,05 se tiene la garantía de encontrar como máximo el 5 % de los datos de entrenamiento mal clasificados \n",
    "            y al menos el 5 % de los datos de entrenamiento siendo vectores de soporte\n",
    "    - ( (0,1] - 0.5 por defecto )\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "EJEMPLOS DE OVERFITTING Y UNDERFITTING:\n",
    "\n",
    "    Underfitting severo\n",
    "        gamma = 0.001   # muy bajo\n",
    "        nu = 0.5        # muy alto\n",
    "\n",
    "    Overfitting severo\n",
    "    gamma = 1.0     # muy alto\n",
    "    nu = 0.0001     # extremadamente bajo\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Aunque podríamos utilizar el barrido en malla de más arriba,\n",
    "seleccionamos manualmente los valores de nu y gamma en base a experimentos anteriores \n",
    "para permitir al modelo obtener un nº de outliers razonable en las fuentes UNID que podamos analizar más adelante.\n",
    "\"\"\"\n",
    "\n",
    "# Parámetros optimizados para 4D\n",
    "selected_gamma = 0.02  # 0.02 - Menor que 0.1 usado en 2D\n",
    "selected_nu = 0.001    # 0.001 - Mantenemos la misma expectativa de outliers\n",
    "\n",
    "print(f\"Parámetros: γ={selected_gamma}, ν={selected_nu}\")\n",
    "\n",
    "# Entrenar modelo final con todos los datos astro (train + val)\n",
    "X_final_train = np.vstack([X_train, X_val])\n",
    "y_final_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "# Scaler final - SOLO ajustar con datos de entrenamiento\n",
    "scaler_final = StandardScaler()\n",
    "X_final_train_scaled = scaler_final.fit_transform(X_final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelo final\n",
    "final_model = OneClassSVM(kernel='rbf', gamma=selected_gamma, nu=selected_nu)\n",
    "final_model.fit(X_final_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Hacer predicciones con el modelo final entrenado sobre datos de prueba\n",
    "\n",
    "# CRÍTICO: Solo transform (NO fit_transform) para datos de test\n",
    "X_test_scaled = scaler_final.transform(X_test)\n",
    "\n",
    "# Análisis de la distribución de scores\n",
    "decision_scores_test = final_model.decision_function(X_test_scaled)\n",
    "\n",
    "print(f\"\\nDecision scores estadísticas:\")\n",
    "print(f\"  Media: {np.mean(decision_scores_test):.4f}\")\n",
    "print(f\"  Std: {np.std(decision_scores_test):.4f}\")\n",
    "print(f\"  Min: {np.min(decision_scores_test):.4f}\")\n",
    "print(f\"  Max: {np.max(decision_scores_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluar en test\n",
    "test_preds = final_model.predict(X_test_scaled)\n",
    "test_labels = np.where(test_preds == 1, 0, 1)\n",
    "\n",
    "# Separar inliers y outliers\n",
    "inliers_mask = test_preds == 1\n",
    "outliers_mask = test_preds == -1\n",
    "\n",
    "n_inliers = np.sum(test_preds == 1)\n",
    "n_outliers = np.sum(test_preds == -1)\n",
    "total_samples = len(test_preds)\n",
    "\n",
    "print(f\"\\nEvaluación en el conjunto de test:\")\n",
    "print(f\"Total de muestras: {total_samples}\")\n",
    "print(f\"Inliers detectados: {n_inliers} ({n_inliers/total_samples*100:.2f}%)\")\n",
    "print(f\"Outliers detectados: {n_outliers} ({n_outliers/total_samples*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metricas\n",
    "print(\"Matriz de confusión:\")\n",
    "cm = confusion_matrix(y_test, test_labels)\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_test, test_labels, target_names=unique_labels(y_test, test_labels).astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de outliers detectados\n",
    "if n_outliers > 0:\n",
    "    outlier_features = X_test[outliers_mask]\n",
    "    print(f\"\\nCaracterísticas de los {n_outliers} outliers detectados:\")\n",
    "    for i, feature in enumerate(features):\n",
    "        values = outlier_features[:, i]\n",
    "        print(f\"  {feature}: [{np.min(values):.3f}, {np.max(values):.3f}]\")\n",
    "        \n",
    "    # Decision scores de los outliers\n",
    "    outlier_scores = decision_scores_test[outliers_mask]\n",
    "    print(f\"\\nDecision scores de outliers:\")\n",
    "    print(f\"  Media: {np.mean(outlier_scores):.4f}\")\n",
    "    print(f\"  Rango: [{np.min(outlier_scores):.4f}, {np.max(outlier_scores):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)']\n",
    "\n",
    "feature_pairs = list(combinations(range(len(feature_names)), 2))\n",
    "n_pairs = len(feature_pairs)\n",
    "\n",
    "# Grid layout\n",
    "cols = 3\n",
    "rows = int(np.ceil(n_pairs / cols))\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 6*rows))\n",
    "\n",
    "# Asegurar que axes sea siempre 2D\n",
    "if rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "if cols == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "# Calcular máscaras de inliers/outliers una sola vez\n",
    "predictions = final_model.predict(X_test_scaled)\n",
    "inliers_mask = predictions == 1\n",
    "outliers_mask = predictions == -1\n",
    "\n",
    "# Crear visualizaciones 2D\n",
    "for i, (feature_idx1, feature_idx2) in enumerate(feature_pairs):\n",
    "    row = i // cols\n",
    "    col = i % cols\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Dimensiones que no se están mostrando\n",
    "    other_dims = [j for j in range(len(feature_names)) if j not in [feature_idx1, feature_idx2]]\n",
    "    \n",
    "    # Usar medianas en lugar de medias para las dimensiones no mostradas\n",
    "    median_values = [np.median(X_test_scaled[:, dim]) for dim in other_dims]\n",
    "    \n",
    "    # Límites para el grid\n",
    "    x_min = X_test_scaled[:, feature_idx1].min() - 0.5\n",
    "    x_max = X_test_scaled[:, feature_idx1].max() + 0.5\n",
    "    y_min = X_test_scaled[:, feature_idx2].min() - 0.5\n",
    "    y_max = X_test_scaled[:, feature_idx2].max() + 0.5\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 150), \n",
    "                         np.linspace(y_min, y_max, 150))\n",
    "    \n",
    "    # Crear grid 4D completo de manera más eficiente\n",
    "    grid_size = xx.ravel().shape[0]\n",
    "    grid_4d = np.zeros((grid_size, len(feature_names)))\n",
    "    \n",
    "    # Asignar valores para las dos dimensiones que se muestran\n",
    "    grid_4d[:, feature_idx1] = xx.ravel()\n",
    "    grid_4d[:, feature_idx2] = yy.ravel()\n",
    "    \n",
    "    # Asignar medianas para las otras dimensiones\n",
    "    for j, other_dim in enumerate(other_dims):\n",
    "        grid_4d[:, other_dim] = median_values[j]\n",
    "    \n",
    "    # Calcular función de decisión\n",
    "    Z = final_model.decision_function(grid_4d)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Asegurar que tenemos un rango válido para los niveles\n",
    "    z_min = Z.min()\n",
    "    z_max = 0  # OneClassSVM usa 0 como frontera\n",
    "    \n",
    "    if z_min >= z_max:\n",
    "        z_max = z_min + 1e-3  # Añadir pequeño valor para evitar error\n",
    "    \n",
    "    # Crear niveles para contourf\n",
    "    levels = np.linspace(z_min, z_max, 8)\n",
    "    \n",
    "    # Plotear contornos\n",
    "    contour_fill = ax.contourf(xx, yy, Z, levels=levels, cmap='RdYlBu_r', alpha=0.6)\n",
    "    ax.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red', linestyles='--')\n",
    "    \n",
    "    # Plotear puntos\n",
    "    ax.scatter(X_test_scaled[inliers_mask, feature_idx1], \n",
    "              X_test_scaled[inliers_mask, feature_idx2], \n",
    "              c='darkblue', s=20, alpha=0.8, \n",
    "              label=f'Inliers ({np.sum(inliers_mask)})')\n",
    "    \n",
    "    if np.any(outliers_mask):\n",
    "        ax.scatter(X_test_scaled[outliers_mask, feature_idx1], \n",
    "                  X_test_scaled[outliers_mask, feature_idx2], \n",
    "                  c='red', s=50, alpha=1.0, marker='D', \n",
    "                  label=f'Outliers ({np.sum(outliers_mask)})')\n",
    "    \n",
    "    # Etiquetas y título\n",
    "    ax.set_xlabel(feature_names[feature_idx1])\n",
    "    ax.set_ylabel(feature_names[feature_idx2])\n",
    "    \n",
    "    # Obtener los parámetros del modelo\n",
    "    gamma_val = final_model.gamma if isinstance(final_model.gamma, (int, float)) else final_model.gamma\n",
    "    nu_val = final_model.nu\n",
    "    \n",
    "    ax.set_title(f'{feature_names[feature_idx1]} vs {feature_names[feature_idx2]}\\n'\n",
    "                f'ν={nu_val}, γ={gamma_val}\\n'\n",
    "                f'Outliers: {np.sum(outliers_mask)}/{len(X_test_scaled)}')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "# Ocultar subplots vacíos si los hay\n",
    "for i in range(len(feature_pairs), rows * cols):\n",
    "    row = i // cols\n",
    "    col = i % cols\n",
    "    if row < axes.shape[0] and col < axes.shape[1]:\n",
    "        axes[row, col].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Información adicional del modelo\n",
    "print(f\"\\nModelo One-Class SVM 4F:\")\n",
    "print(f\"  - Kernel: {final_model.kernel}\")\n",
    "print(f\"  - Gamma: {final_model.gamma}\")\n",
    "print(f\"  - Nu: {final_model.nu}\")\n",
    "print(f\"  - Número de vectores soporte: {final_model.n_support_[0]}\")\n",
    "print(f\"  - Outliers detectados: {np.sum(outliers_mask)}/{len(X_test_scaled)} ({100*np.sum(outliers_mask)/len(X_test_scaled):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_triplets = list(combinations(range(len(feature_names)), 3))\n",
    "n_triplets = len(feature_triplets)\n",
    "\n",
    "# Grid layout 3D\n",
    "cols_3d = 2\n",
    "rows_3d = int(np.ceil(n_triplets / cols_3d))\n",
    "fig = plt.figure(figsize=(15, 7*rows_3d))\n",
    "\n",
    "# Crear visualizaciones 3D\n",
    "for i, (f1_idx, f2_idx, f3_idx) in enumerate(feature_triplets):\n",
    "    ax = fig.add_subplot(rows_3d, cols_3d, i+1, projection='3d')\n",
    "    \n",
    "    # Plotear inliers con borde para más definición\n",
    "    ax.scatter(X_test_scaled[inliers_mask, f1_idx], \n",
    "               X_test_scaled[inliers_mask, f2_idx], \n",
    "               X_test_scaled[inliers_mask, f3_idx],\n",
    "               c='lightblue', s=30, alpha=0.7, \n",
    "               edgecolor='blue', linewidth=0.5,\n",
    "               label=f'Inliers ({np.sum(inliers_mask)})')\n",
    "    \n",
    "    if np.any(outliers_mask):\n",
    "        ax.scatter(X_test_scaled[outliers_mask, f1_idx], \n",
    "                   X_test_scaled[outliers_mask, f2_idx], \n",
    "                   X_test_scaled[outliers_mask, f3_idx],\n",
    "                   c='red', s=100, alpha=1.0, marker='D', \n",
    "                   edgecolor='darkred', linewidth=1,\n",
    "                   label=f'Outliers ({np.sum(outliers_mask)})')\n",
    "    \n",
    "    ax.set_xlabel(feature_names[f1_idx])\n",
    "    ax.set_ylabel(feature_names[f2_idx])\n",
    "    ax.set_zlabel(feature_names[f3_idx])\n",
    "    ax.set_title(f'{feature_names[f1_idx]} vs {feature_names[f2_idx]} vs {feature_names[f3_idx]}\\n'\n",
    "                f'ν={selected_nu}, γ={selected_gamma} | Outliers: {np.sum(outliers_mask)}/{len(X_final_train)}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Importar UNIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar datos UnIDs\n",
    "\n",
    "# unids_path = \"../data/unids_log.txt\"\n",
    "unids_path = \"../data/unids_transformed_complete.txt\"\n",
    "\n",
    "# df_unids = pd.read_csv(unids_path, sep='\\s+')\n",
    "df_unids = pd.read_csv(unids_path, sep='\\t')\n",
    "\n",
    "print(\"Datos UnIDs cargados:\")\n",
    "print(f\"Shape: {df_unids.shape}\")\n",
    "print(\"Primeras filas:\")\n",
    "display(df_unids.head())\n",
    "\n",
    "# Verificar columnas disponibles\n",
    "print(f\"Columnas disponibles: {list(df_unids.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraer características 4F\n",
    "feature_cols = [\"Log(E_peak)\", \"Log(beta)\", \"Log(sigma)\", \"Log(beta_Rel)\"]\n",
    "X_unids = df_unids[feature_cols].values\n",
    "\n",
    "print(f\"Características extraídas: {feature_cols}\")\n",
    "print(f\"Shape características UnIDs: {X_unids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Predecir sobre UNIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Escalar características UnIDs\n",
    "X_unids_scaled = scaler_final.transform(X_unids)\n",
    "\n",
    "decision_scores = final_model.decision_function(X_unids_scaled)\n",
    "\n",
    "unids_preds = final_model.predict(X_unids_scaled)\n",
    "\n",
    "inliers_mask = unids_preds == 1\n",
    "outliers_mask = unids_preds == -1\n",
    "\n",
    "total_unids = len(unids_preds)\n",
    "n_outliers = np.sum(unids_preds == -1)\n",
    "n_inliers = np.sum(unids_preds == 1)\n",
    "outlier_percentage = n_outliers / total_unids * 100\n",
    "inlier_percentage = n_inliers / total_unids * 100\n",
    "\n",
    "# Máscaras para análisis posterior\n",
    "inliers_mask = unids_preds == 1\n",
    "outliers_mask = unids_preds == -1\n",
    "\n",
    "print(f\"RESULTADOS DE PREDICCIÓN:\")\n",
    "print(f\"  - Total UnIDs analizadas: {len(unids_preds)}\")\n",
    "print(f\"  - Outliers detectados: {n_outliers} ({outlier_percentage:.1f}%)\")\n",
    "print(f\"  - Inliers detectados: {n_inliers} ({inlier_percentage:.1f}%)\")\n",
    "\n",
    "print(f\"Outliers detectados con sus IDs:\")\n",
    "\n",
    "outlier_ids = df_unids.loc[outliers_mask, 'number']\n",
    "print(outlier_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calcular anomaly scores y rankings\n",
    "# Invertir scores: valores más altos = más anómalos\n",
    "anom_scores = -decision_scores\n",
    "\n",
    "# Escalar a percentiles [0, 100]\n",
    "anom_percent = MinMaxScaler(feature_range=(0, 100)).fit_transform(anom_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Agregar resultados al DataFrame\n",
    "df_unids_results = df_unids.copy()\n",
    "df_unids_results[\"svm_score\"] = decision_scores\n",
    "df_unids_results[\"prediction\"] = unids_preds\n",
    "df_unids_results[\"Anomaly_Score\"] = anom_scores\n",
    "df_unids_results[\"Anomaly_Rank(%)\"] = anom_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtrar solo outliers y ordenar por anomaly rank\n",
    "outliers_only = df_unids_results[df_unids_results[\"prediction\"] == -1].copy()\n",
    "outliers_sorted = outliers_only.sort_values(by=\"Anomaly_Rank(%)\", ascending=False)\n",
    "\n",
    "# Top 10 outliers más anómalos\n",
    "top_10_outliers = outliers_sorted.head(10)\n",
    "top_10_indices = top_10_outliers.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"TOP 10 CANDIDATOS MÁS ANÓMALOS:\")\n",
    "# Mostrar información relevante\n",
    "display_cols = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)', \n",
    "                'svm_score', 'Anomaly_Score', 'Anomaly_Rank(%)']\n",
    "\n",
    "if 'number' in df_unids_results.columns:\n",
    "    display_cols = ['number'] + display_cols\n",
    "\n",
    "display(top_10_outliers[display_cols].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Comprobamos los valores mínimos y máximos de astro_df\n",
    "print(\"\\nValores mínimos y máximos de las características en astro_df:\")\n",
    "for feature in features:\n",
    "    min_val = df_astro[feature].min()\n",
    "    max_val = df_astro[feature].max()\n",
    "    print(f\"{feature}: [{min_val:.4f}, {max_val:.4f}]\")\n",
    "    \"\"\"\n",
    "# Comprobamos los valores mínimos y máximos de unids\n",
    "print(\"\\nValores mínimos y máximos de las características en unids:\")\n",
    "for feature in feature_cols:\n",
    "    min_val = df_unids[feature].min()\n",
    "    max_val = df_unids[feature].max()\n",
    "    print(f\"{feature}: [{min_val:.4f}, {max_val:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTOP 10 ANOMALÍAS ETIQUETADAS:\")\n",
    "for i, idx in enumerate(top_10_indices):\n",
    "    if 'number' in df_unids_results.columns:\n",
    "        unid_id = int(df_unids_results.loc[idx, 'number'])\n",
    "        rank = df_unids_results.loc[idx, 'Anomaly_Rank(%)']\n",
    "        print(f\"  {i+1:2d}. UNID {unid_id:3d} (Rank: {rank:5.1f}%)\")\n",
    "    else:\n",
    "        rank = df_unids_results.loc[idx, 'Anomaly_Rank(%)']\n",
    "        print(f\"  {i+1:2d}. Índice {idx:3d} (Rank: {rank:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estadísticas de los outliers\n",
    "print(f\"\\nESTADÍSTICAS DE OUTLIERS:\")\n",
    "print(f\"  - Anomaly Score promedio: {outliers_only['Anomaly_Score'].mean():.4f} ± {outliers_only['Anomaly_Score'].std():.4f}\")\n",
    "print(f\"  - Anomaly Rank promedio: {outliers_only['Anomaly_Rank(%)'].mean():.1f}% ± {outliers_only['Anomaly_Rank(%)'].std():.1f}%\")\n",
    "print(f\"  - SVM Score promedio: {outliers_only['svm_score'].mean():.4f} ± {outliers_only['svm_score'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Análisis Estadístico del Modelo OCSVM 4F en UNIDs', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# 1. Distribución de Decision Scores CON ESTADÍSTICAS\n",
    "ax1 = axes[0, 0]\n",
    "n, bins, patches = ax1.hist(decision_scores, bins=50, alpha=0.7, color='skyblue', \n",
    "                           edgecolor='black', density=True)\n",
    "\n",
    "# Añadir curva de densidad suavizada\n",
    "x_smooth = np.linspace(decision_scores.min(), decision_scores.max(), 100)\n",
    "density = stats.gaussian_kde(decision_scores)\n",
    "ax1.plot(x_smooth, density(x_smooth), 'navy', linewidth=2, label='Densidad estimada')\n",
    "\n",
    "# Líneas de referencia\n",
    "ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='Frontera (score=0)')\n",
    "ax1.axvline(decision_scores.mean(), color='green', linestyle='--', linewidth=2,\n",
    "           label=f'Media: {decision_scores.mean():.3f}')\n",
    "ax1.axvline(np.median(decision_scores), color='orange', linestyle='--', linewidth=2,\n",
    "           label=f'Mediana: {np.median(decision_scores):.3f}')\n",
    "\n",
    "# Estadísticas adicionales\n",
    "std_dev = decision_scores.std()\n",
    "ax1.fill_betweenx([0, ax1.get_ylim()[1]], decision_scores.mean()-std_dev, \n",
    "                  decision_scores.mean()+std_dev, alpha=0.2, color='green', \n",
    "                  label=f'±1σ: {std_dev:.3f}')\n",
    "\n",
    "ax1.set_xlabel('SVM Decision Score')\n",
    "ax1.set_ylabel('Densidad de Probabilidad')\n",
    "ax1.set_title('Distribución de Decision Scores\\n(Análisis de Normalidad)')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Texto con estadísticas\n",
    "stats_text = f'n = {len(decision_scores)}\\nSkewness: {stats.skew(decision_scores):.3f}\\nKurtosis: {stats.kurtosis(decision_scores):.3f}'\n",
    "ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes, fontsize=9, \n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 2. Distribución de Anomaly Ranks CON PERCENTILES\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(anom_percent, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "\n",
    "# Percentiles importantes\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "colors = ['blue', 'orange', 'red', 'darkred', 'purple']\n",
    "for p, color in zip(percentiles, colors):\n",
    "    value = np.percentile(anom_percent, p)\n",
    "    ax2.axvline(value, color=color, linestyle='--', linewidth=2, \n",
    "               label=f'P{p}: {value:.1f}%')\n",
    "\n",
    "ax2.set_xlabel('Anomaly Rank (%)')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Distribución de Anomaly Rankings\\n(Análisis de Percentiles)')\n",
    "ax2.legend(fontsize=9, loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Comparación Inliers vs Outliers\n",
    "ax3 = axes[1, 0]\n",
    "outliers_scores = anom_percent[unids_preds == -1]\n",
    "inliers_scores = anom_percent[unids_preds == 1]\n",
    "\n",
    "# Histograma\n",
    "bins = np.linspace(0, 100, 31)\n",
    "ax3.hist([inliers_scores, outliers_scores], bins=bins, alpha=0.7,\n",
    "         color=['lightblue', 'red'], label=[f'Inliers (n={len(inliers_scores)})', \n",
    "                                           f'Outliers (n={len(outliers_scores)})'],\n",
    "         edgecolor='black')\n",
    "\n",
    "# Añadir líneas de medias\n",
    "ax3.axvline(inliers_scores.mean(), color='blue', linestyle='-', linewidth=2,\n",
    "           label=f'Media Inliers: {inliers_scores.mean():.1f}%')\n",
    "ax3.axvline(outliers_scores.mean(), color='darkred', linestyle='-', linewidth=2,\n",
    "           label=f'Media Outliers: {outliers_scores.mean():.1f}%')\n",
    "\n",
    "ax3.set_xlabel('Anomaly Rank (%)')\n",
    "ax3.set_ylabel('Frecuencia')\n",
    "ax3.set_title('Separación Inliers vs Outliers\\n(Validación del Modelo)')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Test estadístico\n",
    "stat, p_value = stats.mannwhitneyu(outliers_scores, inliers_scores, alternative='greater')\n",
    "test_text = f'Mann-Whitney U test:\\np-value: {p_value:.2e}'\n",
    "ax3.text(0.02, 0.98, test_text, transform=ax3.transAxes, fontsize=9,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 4. Scatter con regiones de interés\n",
    "ax4 = axes[1, 1]\n",
    "colors = ['red' if pred == -1 else 'blue' for pred in unids_preds]\n",
    "sizes = [40 if pred == -1 else 20 for pred in unids_preds]\n",
    "\n",
    "scatter = ax4.scatter(decision_scores, anom_percent, c=colors, alpha=0.6, s=sizes,\n",
    "                     edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Regiones de interés\n",
    "ax4.axvline(0, color='black', linestyle='--', alpha=0.7, linewidth=2, label='Frontera decisión')\n",
    "ax4.axhline(90, color='red', linestyle='--', alpha=0.7, label='Top 10% anomalías')\n",
    "ax4.axhline(95, color='darkred', linestyle='--', alpha=0.7, label='Top 5% anomalías')\n",
    "\n",
    "# Destacar zona de interés (outliers extremos)\n",
    "extreme_mask = (unids_preds == -1) & (anom_percent > 95)\n",
    "if np.any(extreme_mask):\n",
    "    ax4.scatter(decision_scores[extreme_mask], anom_percent[extreme_mask], \n",
    "               s=100, facecolors='none', edgecolors='yellow', linewidth=3,\n",
    "               label=f'Outliers extremos (n={np.sum(extreme_mask)})')\n",
    "\n",
    "# Correlación\n",
    "correlation = np.corrcoef(decision_scores, anom_percent)[0, 1]\n",
    "ax4.text(0.02, 0.98, f'Correlación: {correlation:.3f}', transform=ax4.transAxes, \n",
    "         fontsize=11, verticalalignment='top', fontweight='bold',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "\n",
    "ax4.set_xlabel('SVM Decision Score')\n",
    "ax4.set_ylabel('Anomaly Rank (%)')\n",
    "ax4.set_title('Relación Decision Score vs Anomaly Rank\\n(Coherencia del Modelo)')\n",
    "ax4.legend(fontsize=9, loc='lower right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID DE VISUALIZACIONES 2D DE UNIDs CON IDs DE ANOMALÍAS\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle(f'Modelo OCSVM 4F: Proyecciones 2D de UNIDs\\n'\n",
    "            f'Outliers detectados: {np.sum(outliers_mask)} de {len(unids_preds)}', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# Todas las combinaciones 2D de las 4 características\n",
    "feature_pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "feature_names = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)']\n",
    "\n",
    "# Separar inliers y outliers\n",
    "inliers_mask = unids_preds == 1\n",
    "outliers_mask = unids_preds == -1\n",
    "top_10_indices = top_10_outliers.index\n",
    "\n",
    "for idx, (i, j) in enumerate(feature_pairs):\n",
    "    ax = axes[idx//3, idx%3]\n",
    "    \n",
    "    # Plot inliers con el estilo gold\n",
    "    ax.scatter(X_unids[inliers_mask, i], X_unids[inliers_mask, j],\n",
    "               c='gold', alpha=0.6, s=30, edgecolors='black', linewidth=0.3,\n",
    "               label=f'Inliers (astro-like) ({np.sum(inliers_mask)})')\n",
    "    \n",
    "    # Plot outliers con gradient de colores según anomaly rank\n",
    "    if np.any(outliers_mask):\n",
    "        scatter = ax.scatter(X_unids[outliers_mask, i], X_unids[outliers_mask, j],\n",
    "                            c=anom_percent[outliers_mask], cmap='Reds', s=60,\n",
    "                            alpha=0.8, edgecolors='black', linewidth=0.5, \n",
    "                            label=f'Outliers (anomalous) ({np.sum(outliers_mask)})')\n",
    "        \n",
    "        \"\"\"# Más control sobre el colorbar\n",
    "        if idx == 2:  # subplot (0,2)\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('Anomaly Rank %', fontsize=10)\n",
    "            cbar.ax.tick_params(labelsize=9)\"\"\"\n",
    "    \n",
    "    # Destacar top 10 con círculos amarillos\n",
    "    if len(top_10_indices) > 0:\n",
    "        ax.scatter(X_unids[top_10_indices, i], X_unids[top_10_indices, j],\n",
    "                   s=120, facecolors='none', edgecolors='yellow', linewidth=3,\n",
    "                   label=f'Top 10 Anomalies', marker='o')\n",
    "        \n",
    "        # Añadir etiquetas de ID para top 10\n",
    "        for unid_idx in top_10_indices:\n",
    "            x = X_unids[unid_idx, i]\n",
    "            y = X_unids[unid_idx, j]\n",
    "            \n",
    "            # Obtener el número del UNID\n",
    "            if 'number' in df_unids_results.columns:\n",
    "                unid_id = int(df_unids_results.loc[unid_idx, 'number'])\n",
    "            else:\n",
    "                unid_id = unid_idx\n",
    "            \n",
    "            # Posicionar texto ligeramente desplazado para evitar solapamiento\n",
    "            offset_x = (X_unids[:, i].max() - X_unids[:, i].min()) * 0.02\n",
    "            offset_y = (X_unids[:, j].max() - X_unids[:, j].min()) * 0.02\n",
    "            \n",
    "            ax.text(x + offset_x, y + offset_y, str(unid_id), \n",
    "                   color='black', fontsize=8, fontweight='bold',\n",
    "                   ha='left', va='bottom',\n",
    "                   bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    # Configuración de ejes y título\n",
    "    ax.set_xlabel(feature_names[i])\n",
    "    ax.set_ylabel(feature_names[j])\n",
    "    ax.set_title(f'{feature_names[i]} vs {feature_names[j]}\\n'\n",
    "            f'UNIDs: Modelo 4F - Top 10 Anomalías Etiquetadas')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Sin colorbar, pero con información en la leyenda:\n",
    "if np.any(outliers_mask):\n",
    "    scatter = ax.scatter(X_unids[outliers_mask, i], X_unids[outliers_mask, j],\n",
    "                        c=anom_percent[outliers_mask], cmap='Reds', s=60,\n",
    "                        alpha=0.8, edgecolors='black', linewidth=0.5, \n",
    "                        label=f'Outliers (color = Anomaly Rank)')\n",
    "    \n",
    "    # Añadir texto explicativo\n",
    "    ax.text(0.02, 0.98, 'Color: Rojo más intenso = Mayor anomalía', \n",
    "           transform=ax.transAxes, fontsize=8, \n",
    "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID DE VISUALIZACIONES 3D DE UNIDs\n",
    "feature_names = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)']\n",
    "feature_triplets = list(combinations(range(len(feature_names)), 3))\n",
    "n_triplets = len(feature_triplets)\n",
    "\n",
    "# Grid layout 3D optimizado\n",
    "cols_3d = 2\n",
    "rows_3d = int(np.ceil(n_triplets / cols_3d))\n",
    "fig = plt.figure(figsize=(15, 7*rows_3d))\n",
    "\n",
    "\"\"\"# Separar inliers y outliers\n",
    "inliers_mask = unids_preds == 1\n",
    "outliers_mask = unids_preds == -1\n",
    "top_10_indices = top_10_outliers.index\n",
    "\n",
    "# Conteos para leyenda unificada\n",
    "n_inliers = np.sum(inliers_mask)\n",
    "n_outliers = np.sum(outliers_mask)\n",
    "\"\"\"\n",
    "\n",
    "n_top10 = len(top_10_indices)\n",
    "\n",
    "# Crear visualizaciones 3D\n",
    "for i, (f1_idx, f2_idx, f3_idx) in enumerate(feature_triplets):\n",
    "    ax = fig.add_subplot(rows_3d, cols_3d, i+1, projection='3d')\n",
    "    \n",
    "    # Plot inliers\n",
    "    ax.scatter(X_unids[inliers_mask, f1_idx], \n",
    "               X_unids[inliers_mask, f2_idx], \n",
    "               X_unids[inliers_mask, f3_idx],\n",
    "               c='gold', s=25, alpha=0.6, \n",
    "               edgecolor='black', linewidth=0.3)\n",
    "    \n",
    "    # Plot outliers con gradient de color según anomaly rank\n",
    "    if np.any(outliers_mask):\n",
    "        scatter = ax.scatter(X_unids[outliers_mask, f1_idx], \n",
    "                           X_unids[outliers_mask, f2_idx], \n",
    "                           X_unids[outliers_mask, f3_idx],\n",
    "                           c=anom_percent[outliers_mask], cmap='Reds', s=60,\n",
    "                           alpha=0.8, edgecolors='black', linewidth=0.4)\n",
    "    \n",
    "    # Destacar top 10 anomalías\n",
    "    if len(top_10_indices) > 0:\n",
    "        ax.scatter(X_unids[top_10_indices, f1_idx], \n",
    "                   X_unids[top_10_indices, f2_idx], \n",
    "                   X_unids[top_10_indices, f3_idx],\n",
    "                   s=120, facecolors='none', edgecolors='yellow', \n",
    "                   linewidth=2.5)\n",
    "        \n",
    "        # Añadir etiquetas de ID con mejor posicionamiento\n",
    "        for idx in top_10_indices:\n",
    "            x = X_unids[idx, f1_idx]\n",
    "            y = X_unids[idx, f2_idx] \n",
    "            z = X_unids[idx, f3_idx]\n",
    "            \n",
    "            # Obtener el número del UNID\n",
    "            if 'number' in df_unids_results.columns:\n",
    "                unid_id = int(df_unids_results.loc[idx, 'number'])\n",
    "            else:\n",
    "                unid_id = idx\n",
    "            \n",
    "            ax.text(x, y, z, str(unid_id), fontsize=7, color='black', \n",
    "                   fontweight='bold', ha='center', va='bottom',\n",
    "                   bbox=dict(boxstyle='round,pad=0.15', facecolor='white', \n",
    "                            alpha=0.8, edgecolor='none'))\n",
    "    \n",
    "    # Configuración de ejes más compacta\n",
    "    ax.set_xlabel(feature_names[f1_idx], fontsize=9)\n",
    "    ax.set_ylabel(feature_names[f2_idx], fontsize=9)\n",
    "    ax.set_zlabel(feature_names[f3_idx], fontsize=9)\n",
    "    \n",
    "    # Título más compacto\n",
    "    title_short = f'{feature_names[f1_idx]} vs {feature_names[f2_idx]} vs {feature_names[f3_idx]}'\n",
    "    ax.set_title(title_short, fontsize=10, pad=10)\n",
    "    \n",
    "    # Ajustar tamaño de ticks\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "\"\"\"\n",
    "# Colorbar unificado en la parte superior\n",
    "if np.any(outliers_mask):\n",
    "    # Crear un scatter dummy para el colorbar\n",
    "    dummy_scatter = plt.scatter([], [], c=[], cmap='Reds', s=60, alpha=0.8)\n",
    "    dummy_scatter.set_array(anom_percent[outliers_mask])\n",
    "    \n",
    "    # Colorbar en la parte superior\n",
    "    cbar_ax = fig.add_axes([0.15, 0.95, 0.7, 0.02])  # [left, bottom, width, height]\n",
    "    cbar = plt.colorbar(dummy_scatter, cax=cbar_ax, orientation='horizontal')\n",
    "    cbar.set_label('Anomaly Rank %', fontsize=11, labelpad=10)\n",
    "\"\"\"\n",
    "# Leyenda unificada en la parte inferior\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gold', \n",
    "               markeredgecolor='black', markersize=8, alpha=0.8,\n",
    "               label=f'Inliers (astro-like): {n_inliers}'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', \n",
    "               markeredgecolor='black', markersize=8, alpha=0.8,\n",
    "               label=f'Outliers (anomalous): {n_outliers}'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='none', \n",
    "               markeredgecolor='yellow', markersize=10, markeredgewidth=2,\n",
    "               label=f'Top 10 Anomalies: {n_top10}')\n",
    "]\n",
    "\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=3, \n",
    "          bbox_to_anchor=(0.5, 0.02), fontsize=11, frameon=True, \n",
    "          fancybox=True, shadow=True)\n",
    "\n",
    "# Ajustar layout con más espacio para leyenda y colorbar\n",
    "plt.subplots_adjust(top=0.90, bottom=0.15, left=0.05, right=0.95, \n",
    "                   hspace=0.3, wspace=0.2)\n",
    "\n",
    "fig.suptitle('Modelo OCSVM 4F: Proyecciones 3D de UNIDs\\n'\n",
    "            f'Outliers detectados: {n_outliers} de {len(unids_preds)}', \n",
    "            fontsize=14, fontweight='bold', y=0.97)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID DE VISUALIZACIONES 3D DE UNIDs - SIN COLORBAR\n",
    "feature_names = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)']\n",
    "feature_triplets = list(combinations(range(len(feature_names)), 3))\n",
    "\n",
    "# Layout compacto 2x2\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "\"\"\"# Separar inliers y outliers\n",
    "inliers_mask = unids_preds == 1\n",
    "outliers_mask = unids_preds == -1\n",
    "top_10_indices = top_10_outliers.index\n",
    "\n",
    "n_inliers = np.sum(inliers_mask)\n",
    "n_outliers = np.sum(outliers_mask)\"\"\"\n",
    "\n",
    "# Crear visualizaciones 3D\n",
    "for i, (f1_idx, f2_idx, f3_idx) in enumerate(feature_triplets):\n",
    "    ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
    "    \n",
    "    # Plot inliers\n",
    "    ax.scatter(X_unids[inliers_mask, f1_idx], \n",
    "               X_unids[inliers_mask, f2_idx], \n",
    "               X_unids[inliers_mask, f3_idx],\n",
    "               c='lightblue', s=20, alpha=0.6, \n",
    "               edgecolor='blue', linewidth=0.2,\n",
    "               label=f'Inliers ({n_inliers})')\n",
    "    \n",
    "    # Plot outliers en rojo sólido\n",
    "    if np.any(outliers_mask):\n",
    "        ax.scatter(X_unids[outliers_mask, f1_idx], \n",
    "                   X_unids[outliers_mask, f2_idx], \n",
    "                   X_unids[outliers_mask, f3_idx],\n",
    "                   c='red', s=60, alpha=0.9, \n",
    "                   edgecolors='darkred', linewidth=0.4,\n",
    "                   label=f'Outliers ({n_outliers})')\n",
    "    \n",
    "    # Top 10 con círculos amarillos\n",
    "    if len(top_10_indices) > 0:\n",
    "        ax.scatter(X_unids[top_10_indices, f1_idx], \n",
    "                   X_unids[top_10_indices, f2_idx], \n",
    "                   X_unids[top_10_indices, f3_idx],\n",
    "                   s=100, facecolors='none', edgecolors='yellow', \n",
    "                   linewidth=3, label='Top 10')\n",
    "    \n",
    "    # Configuración\n",
    "    ax.set_xlabel(feature_names[f1_idx], fontsize=10)\n",
    "    ax.set_ylabel(feature_names[f2_idx], fontsize=10)\n",
    "    ax.set_zlabel(feature_names[f3_idx], fontsize=10)\n",
    "    \n",
    "    # Título corto\n",
    "    ax.set_title(f'{feature_names[f1_idx][:8]} vs {feature_names[f2_idx][:8]} vs {feature_names[f3_idx][:8]}', \n",
    "                fontsize=11)\n",
    "    \n",
    "    # Leyenda solo en el primer subplot\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=9, loc='upper left')\n",
    "    \n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "# Título general con información clave\n",
    "fig.suptitle(f'Modelo OCSVM 4F: Proyecciones 3D\\n'\n",
    "            f'{n_outliers} Outliers detectados de {len(unids_preds)} UNIDs', \n",
    "            fontsize=13, fontweight='bold')\n",
    "\n",
    "# Layout más compacto\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Análisis de consenso entre modelos OCSVM 2F y OCSVM 4F - Resultados de UNIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de intersección entre modelos\n",
    "candidatos_2f = set([1054, 275, 1116, 1017])  # Top 4 del modelo 2F\n",
    "candidatos_4f = set([307, 285, 166, 923, 1116])  # Top 5 del modelo 4F\n",
    "\n",
    "consenso = candidatos_2f.intersection(candidatos_4f)\n",
    "unicos_2f = candidatos_2f - candidatos_4f\n",
    "unicos_4f = candidatos_4f - candidatos_2f\n",
    "\n",
    "print(f\"\\nAnálisis de intersección entre modelos:\")\n",
    "print(f\"  - Candidatos comunes (consenso): {sorted(consenso)}\")\n",
    "print(f\"  - Candidatos únicos en 2F: {sorted(unicos_2f)}\")\n",
    "print(f\"  - Candidatos únicos en 4F: {sorted(unicos_4f)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Guardar modelo para API extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export trained model to disk\n",
    "import joblib\n",
    "model_path = \"../models/model_ocsvm_4f.pkl\"\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"Modelo One-Class SVM guardado en: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# OCSVM 4F vs ANN 4F - Comparar los resultados (sobre UNIDs) con los de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANÁLISIS COMPARATIVO: ANN 4F vs OCSVM 4F\\n\")\n",
    "\n",
    "# Cargar resultados de consenso ANN 4F\n",
    "ann_consensus_path = \"../data/results/ann/4F/consensus_analysis_ann_4f_improved.csv\"\n",
    "df_ann_consensus = pd.read_csv(ann_consensus_path)\n",
    "\n",
    "print(f\"Datos ANN 4F cargados:\")\n",
    "print(f\"  - Shape: {df_ann_consensus.shape}\")\n",
    "print(f\"  - Columnas principales: Source_ID, Mean_Consensus, Std_Consensus\\n\")\n",
    "\n",
    "df_ann_consensus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener candidatos OCSVM 2F (outliers detectados)\n",
    "ocsvm_outliers = set()\n",
    "if np.any(unids_preds == -1):\n",
    "    outlier_indices = np.where(unids_preds == -1)[0]\n",
    "    for idx in outlier_indices:\n",
    "        if 'number' in df_unids_results.columns:\n",
    "            unid_id = int(df_unids_results.loc[idx, 'number'])\n",
    "            ocsvm_outliers.add(unid_id)\n",
    "\n",
    "# Top 10 OCSVM por anomaly rank\n",
    "ocsvm_top10 = set()\n",
    "if len(df_unids_results) > 0:\n",
    "    top_10_ocsvm = df_unids_results.nlargest(10, 'Anomaly_Rank(%)')\n",
    "    for idx, row in top_10_ocsvm.iterrows():\n",
    "        if 'number' in row:\n",
    "            ocsvm_top10.add(int(row['number']))\n",
    "\n",
    "print(f\"Candidatos OCSVM 4F:\")\n",
    "print(f\"  - Outliers detectados: {len(ocsvm_outliers)} → {sorted(ocsvm_outliers)}\")\n",
    "print(f\"  - Top 10 anomalías: {len(ocsvm_top10)} → {sorted(ocsvm_top10)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener candidatos ANN 2F\n",
    "ann_top4 = set(df_ann_consensus.head(4)['Source_ID'].astype(int))\n",
    "ann_top10 = set(df_ann_consensus.head(10)['Source_ID'].astype(int))\n",
    "\n",
    "# Candidatos ANN por threshold de consenso\n",
    "consensus_threshold = 0.7  # Definir umbral de consenso\n",
    "high_consensus_mask = df_ann_consensus['Mean_Prob_Consensus'] >= consensus_threshold\n",
    "ann_high_consensus = set()\n",
    "if high_consensus_mask.any():\n",
    "    ann_high_consensus = set(df_ann_consensus[high_consensus_mask]['Source_ID'].astype(int))\n",
    "\n",
    "print(f\"Candidatos ANN 4F:\")\n",
    "print(f\"  - Top 4: {sorted(ann_top4)}\")\n",
    "print(f\"  - Top 10: {sorted(ann_top10)}\")\n",
    "print(f\"  - Alto consenso (≥{consensus_threshold}): {len(ann_high_consensus)} → {sorted(ann_high_consensus)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICACIÓN DE CONSISTENCIA DE DATOS\n",
    "print(\"VERIFICACIÓN DE CONSISTENCIA DE DATOS\\n\")\n",
    "\n",
    "# Obtener algunos candidatos para verificar\n",
    "verification_candidates = list(ocsvm_outliers.union(ann_top4))[:5]  # Primeros 5 para verificar\n",
    "\n",
    "print(\"Verificando que Source_ID (ANN) = number (OCSVM) corresponden a las mismas fuentes:\\n\")\n",
    "\n",
    "consistent_data = True\n",
    "for candidate in verification_candidates:\n",
    "    # Buscar en datos ANN\n",
    "    ann_row = df_ann_consensus[df_ann_consensus['Source_ID'] == candidate]\n",
    "    # Buscar en datos OCSVM\n",
    "    ocsvm_row = df_unids_results[df_unids_results['number'] == candidate]\n",
    "    \n",
    "    if len(ann_row) > 0 and len(ocsvm_row) > 0:\n",
    "        # Extraer valores de features\n",
    "        ann_epeak = ann_row['log_E_peak'].iloc[0] if 'log_E_peak' in ann_row.columns else None\n",
    "        ann_beta = ann_row['log_Beta'].iloc[0] if 'log_Beta' in ann_row.columns else None\n",
    "        \n",
    "        ocsvm_epeak = ocsvm_row['Log(E_peak)'].iloc[0] if 'Log(E_peak)' in ocsvm_row.columns else None\n",
    "        ocsvm_beta = ocsvm_row['Log(beta)'].iloc[0] if 'Log(beta)' in ocsvm_row.columns else None\n",
    "        \n",
    "        if ann_epeak is not None and ocsvm_epeak is not None:\n",
    "            epeak_diff = abs(ann_epeak - ocsvm_epeak)\n",
    "            beta_diff = abs(ann_beta - ocsvm_beta)\n",
    "            \n",
    "            # Tolerancia para diferencias de precisión numérica\n",
    "            tolerance = 1e-6\n",
    "            epeak_match = epeak_diff < tolerance\n",
    "            beta_match = beta_diff < tolerance\n",
    "            \n",
    "            print(f\"UNID {candidate}:\")\n",
    "            print(f\"  Log(E_peak): ANN={ann_epeak:.6f}, OCSVM={ocsvm_epeak:.6f}, diff={epeak_diff:.2e} {'✓' if epeak_match else '✗'}\")\n",
    "            print(f\"  Log(beta):   ANN={ann_beta:.6f}, OCSVM={ocsvm_beta:.6f}, diff={beta_diff:.2e} {'✓' if beta_match else '✗'}\")\n",
    "            \n",
    "            if not (epeak_match and beta_match):\n",
    "                consistent_data = False\n",
    "                print(f\"  ⚠ INCONSISTENCIA detectada en UNID {candidate}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"UNID {candidate}: No encontrado en uno de los datasets\")\n",
    "        if len(ann_row) == 0:\n",
    "            print(f\"  - Falta en datos ANN\")\n",
    "        if len(ocsvm_row) == 0:\n",
    "            print(f\"  - Falta en datos OCSVM\")\n",
    "        print()\n",
    "\n",
    "if consistent_data:\n",
    "    print(\"✓ VERIFICACIÓN EXITOSA: Los datos son consistentes entre ANN y OCSVM\")\n",
    "else:\n",
    "    print(\"✗ ADVERTENCIA: Se detectaron inconsistencias en los datos\")\n",
    "    print(\"  Revisar preprocesado y fuentes de datos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS DE INTERSECCIONES\n",
    "print(\"ANÁLISIS DE CONSENSO ENTRE ALGORITMOS\\n\")\n",
    "\n",
    "# Outliers OCSVM vs Top ANN\n",
    "intersection_outliers_ann4 = ocsvm_outliers.intersection(ann_top4)\n",
    "intersection_outliers_ann10 = ocsvm_outliers.intersection(ann_top10)\n",
    "intersection_outliers_consensus = ocsvm_outliers.intersection(ann_high_consensus)\n",
    "\n",
    "print(f\"Outliers OCSVM (4) vs ANN:\")\n",
    "print(f\"  - vs Top 4 ANN: {len(intersection_outliers_ann4)} común(es) → {sorted(intersection_outliers_ann4)}\")\n",
    "print(f\"  - vs Top 10 ANN: {len(intersection_outliers_ann10)} común(es) → {sorted(intersection_outliers_ann10)}\")\n",
    "print(f\"  - vs Alto consenso ANN: {len(intersection_outliers_consensus)} común(es) → {sorted(intersection_outliers_consensus)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 OCSVM vs Top 10 ANN\n",
    "intersection_top10 = ocsvm_top10.intersection(ann_top10)\n",
    "\n",
    "print(f\"Top 10 vs Top 10:\")\n",
    "print(f\"  - Candidatos comunes: {len(intersection_top10)} → {sorted(intersection_top10)}\")\n",
    "print(f\"  - Overlap rate: {len(intersection_top10)/10*100:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CARACTERIZACIÓN DE CANDIDATOS COMUNES\n",
    "print(\"CARACTERIZACIÓN DE CANDIDATOS COMUNES\\n\")\n",
    "\n",
    "all_common_candidates = intersection_outliers_ann10.union(intersection_top10)\n",
    "\n",
    "if len(all_common_candidates) > 0:\n",
    "    print(f\"Candidatos con consenso entre algoritmos: {sorted(all_common_candidates)}\\n\")\n",
    "    \n",
    "    for candidate in sorted(all_common_candidates):\n",
    "        # Info OCSVM\n",
    "        ocsvm_info = df_unids_results[df_unids_results['number'] == candidate]\n",
    "        if len(ocsvm_info) > 0:\n",
    "            ocsvm_rank = ocsvm_info['Anomaly_Rank(%)'].iloc[0]\n",
    "            ocsvm_score = ocsvm_info['svm_score'].iloc[0]\n",
    "            is_outlier = ocsvm_info['prediction'].iloc[0] == -1\n",
    "        else:\n",
    "            ocsvm_rank, ocsvm_score, is_outlier = 0, 0, False\n",
    "            \n",
    "        # Info ANN\n",
    "        ann_info = df_ann_consensus[df_ann_consensus['Source_ID'] == candidate]\n",
    "        if len(ann_info) > 0:\n",
    "            ann_rank = ann_info.index[0] + 1\n",
    "            ann_consensus = ann_info['Mean_Prob_Consensus'].iloc[0]\n",
    "            ann_std = ann_info['Std_Consensus'].iloc[0]\n",
    "        else:\n",
    "            ann_rank, ann_consensus, ann_std = 999, 0, 0\n",
    "            \n",
    "        print(f\"UNID {candidate}:\")\n",
    "        print(f\"  OCSVM: Rank {ocsvm_rank:5.1f}% | Score {ocsvm_score:8.4f} | Outlier: {'Sí' if is_outlier else 'No'}\")\n",
    "        print(f\"  ANN:   Pos #{ann_rank:2d}     | Consenso {ann_consensus:5.3f}±{ann_std:.3f}\\n\")\n",
    "else:\n",
    "    print(\"NO HAY CANDIDATOS COMUNES entre outliers OCSVM y top ANN\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISIS DE DIVERGENCIAS\n",
    "print(\"ANÁLISIS DE DIVERGENCIAS\\n\")\n",
    "\n",
    "ocsvm_only = ocsvm_outliers - ann_top10\n",
    "ann_only = ann_top10 - ocsvm_top10\n",
    "\n",
    "print(f\"Candidatos únicos por método:\")\n",
    "print(f\"  - Solo OCSVM outliers: {len(ocsvm_only)} → {sorted(ocsvm_only)}\")\n",
    "print(f\"  - Solo ANN top 10: {len(ann_only)} → {sorted(ann_only)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSICIONES CRUZADAS\n",
    "print(\"POSICIONES CRUZADAS DE CANDIDATOS\\n\")\n",
    "\n",
    "print(f\"Posición de outliers OCSVM en ranking ANN:\")\n",
    "for candidate in sorted(ocsvm_outliers):\n",
    "    ann_info = df_ann_consensus[df_ann_consensus['Source_ID'] == candidate]\n",
    "    if len(ann_info) > 0:\n",
    "        ann_rank = ann_info.index[0] + 1\n",
    "        ann_consensus = ann_info['Mean_Prob_Consensus'].iloc[0]\n",
    "        print(f\"  UNID {candidate}: Posición #{ann_rank:3d} en ANN (consenso: {ann_consensus:.3f})\")\n",
    "    else:\n",
    "        print(f\"  UNID {candidate}: No encontrado en resultados ANN\")\n",
    "\n",
    "print(f\"\\nPosición de top 4 ANN en ranking OCSVM:\")\n",
    "for candidate in sorted(ann_top4):\n",
    "    ocsvm_info = df_unids_results[df_unids_results['number'] == candidate]\n",
    "    if len(ocsvm_info) > 0:\n",
    "        ocsvm_rank = ocsvm_info['Anomaly_Rank(%)'].iloc[0]\n",
    "        is_outlier = ocsvm_info['prediction'].iloc[0] == -1\n",
    "        print(f\"  UNID {candidate}: Anomaly Rank {ocsvm_rank:5.1f}% ({'Outlier' if is_outlier else 'Inlier'})\")\n",
    "    else:\n",
    "        print(f\"  UNID {candidate}: No encontrado en resultados OCSVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESUMEN COMPARATIVO\n",
    "print(f\"\\nRESUMEN COMPARATIVO\\n\")\n",
    "\n",
    "total_candidates_combined = len(ocsvm_top10.union(ann_top10))\n",
    "overlap_rate = len(intersection_top10) / 10 * 100\n",
    "\n",
    "print(f\"Métricas de consenso:\")\n",
    "print(f\"  - Candidatos únicos combinados: {total_candidates_combined}\")\n",
    "print(f\"  - Overlap Top 10 vs Top 10: {len(intersection_top10)}/10 ({overlap_rate:.1f}%)\")\n",
    "print(f\"  - Complementariedad: {100-overlap_rate:.1f}%\\n\")\n",
    "\n",
    "print(f\"Niveles de consenso:\")\n",
    "if len(intersection_outliers_ann4) > 0:\n",
    "    print(f\"  Consenso alto: {len(intersection_outliers_ann4)} candidato(s) detectado(s) por ambos métodos\")\n",
    "else:\n",
    "    print(f\"  Consenso bajo: Sin overlap entre outliers OCSVM y top 4 ANN\")\n",
    "\n",
    "print(f\"\\nInterpretación:\")\n",
    "if overlap_rate >= 30:\n",
    "    print(f\"  Convergencia significativa entre algoritmos\")\n",
    "elif overlap_rate >= 10:\n",
    "    print(f\"  Convergencia moderada - métodos complementarios\")\n",
    "else:\n",
    "    print(f\"  Divergencia alta - algoritmos capturan anomalías diferentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# VERSIÓN 2 - ANTERIOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar resultados del consenso ANN 4F\n",
    "ann_consensus = pd.read_csv('../data/results/ann/4F/consensus_analysis_ann_4f.csv')\n",
    "print(f\"Resultados ANN 4F cargados: {len(ann_consensus)} fuentes\")\n",
    "\n",
    "# Cargar resultados OCSVM 4F\n",
    "ocsvm_results = df_unids_results.copy()  # O cargar desde archivo si está guardado\n",
    "print(f\"Resultados OCSVM 4F cargados: {len(ocsvm_results)} fuentes\")\n",
    "\n",
    "# Verificar mismo número de fuentes\n",
    "if len(ann_consensus) != len(ocsvm_results):\n",
    "    print(f\" ADVERTENCIA: Diferentes números de fuentes - ANN: {len(ann_consensus)}, OCSVM: {len(ocsvm_results)}\")\n",
    "else:\n",
    "    print(f\"Mismo número de fuentes: {len(ann_consensus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultados (top unids) consenso ANN 4F:\")\n",
    "ann_consensus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resultados (todos) OCSVM 2F:')\n",
    "ocsvm_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar alineamiento de IDs\n",
    "ann_ids = ann_consensus['Source_ID'].values\n",
    "ocsvm_ids = ocsvm_results['number'].values if 'number' in ocsvm_results.columns else ocsvm_results.index.values\n",
    "\n",
    "ids_aligned = np.array_equal(ann_ids, ocsvm_ids)\n",
    "\n",
    "if ids_aligned:\n",
    "    print(\"Los IDs están alineados correctamente\")\n",
    "    ann_final = ann_consensus.copy()\n",
    "    ocsvm_final = ocsvm_results.copy()\n",
    "else:\n",
    "    print(\"Los IDs NO están alineados - procediendo a alinear...\")\n",
    "    print(f\"   Primeros 5 IDs ANN: {ann_ids[:5]}\")\n",
    "    print(f\"   Primeros 5 IDs OCSVM: {ocsvm_ids[:5]}\")\n",
    "    \n",
    "    # Alinear por ID común\n",
    "    if 'number' in ocsvm_results.columns:\n",
    "        ann_final = ann_consensus.sort_values('Source_ID').reset_index(drop=True)\n",
    "        ocsvm_final = ocsvm_results.sort_values('number').reset_index(drop=True)\n",
    "        \n",
    "        # Verificar alineamiento después de ordenar\n",
    "        if np.array_equal(ann_final['Source_ID'].values, ocsvm_final['number'].values):\n",
    "            print(\"IDs alineados correctamente después de ordenar\")\n",
    "        else:\n",
    "            print(\"ERROR: No se pudieron alinear los IDs - revisar datos\")\n",
    "            # Mostrar diferencias\n",
    "            ann_set = set(ann_final['Source_ID'].values)\n",
    "            ocsvm_set = set(ocsvm_final['number'].values)\n",
    "            print(f\"   IDs solo en ANN: {ann_set - ocsvm_set}\")\n",
    "            print(f\"   IDs solo en OCSVM: {ocsvm_set - ann_set}\")\n",
    "    else:\n",
    "        print(\"ERROR: Columna 'number' no encontrada en OCSVM results\")\n",
    "        ann_final = ann_consensus.copy()\n",
    "        ocsvm_final = ocsvm_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAR DATAFRAME COMBINADO\n",
    "\n",
    "# Verificar columnas necesarias existen\n",
    "required_ann_cols = ['Source_ID', 'E_peak', 'Beta', 'log_E_peak', 'log_Beta', 'log_Sigma_det', 'log_Beta_Rel', 'Mean_Prob_Consensus', 'Std_Prob_Consensus']\n",
    "required_ocsvm_cols = ['prediction', 'Anomaly_Score', 'Anomaly_Rank(%)']\n",
    "\n",
    "missing_ann = [col for col in required_ann_cols if col not in ann_final.columns]\n",
    "missing_ocsvm = [col for col in required_ocsvm_cols if col not in ocsvm_final.columns]\n",
    "\n",
    "if missing_ann:\n",
    "    print(f\"Columnas faltantes en ANN: {missing_ann}\")\n",
    "if missing_ocsvm:\n",
    "    print(f\"Columnas faltantes en OCSVM: {missing_ocsvm}\")\n",
    "\n",
    "if not missing_ann and not missing_ocsvm:\n",
    "    comparison_df = pd.DataFrame({\n",
    "        # IDs y características originales\n",
    "        'Source_ID': ann_final['Source_ID'].values,\n",
    "        'E_peak': ann_final['E_peak'].values,\n",
    "        'Beta': ann_final['Beta'].values,\n",
    "        'Log_E_peak': ann_final['log_E_peak'].values,\n",
    "        'Log_Beta': ann_final['log_Beta'].values,\n",
    "        'Log_Sigma_det': ann_final['log_Sigma_det'].values,\n",
    "        'log_Beta_Rel': ann_final['log_Beta_Rel'].values,\n",
    "        \n",
    "        # Resultados ANN 2F\n",
    "        'ANN_Prob_Mean': ann_final['Mean_Prob_Consensus'].values,\n",
    "        'ANN_Prob_Std': ann_final['Std_Prob_Consensus'].values,\n",
    "        \n",
    "        # Resultados OCSVM 2F\n",
    "        'OCSVM_Prediction': ocsvm_final['prediction'].values,  # 1=inlier, -1=outlier\n",
    "        'OCSVM_Anomaly_Score': ocsvm_final['Anomaly_Score'].values,\n",
    "        'OCSVM_Anomaly_Rank': ocsvm_final['Anomaly_Rank(%)'].values,\n",
    "    })\n",
    "    \n",
    "else:\n",
    "    print(\"No se pudo crear el DataFrame combinado debido a columnas faltantes\")\n",
    "    comparison_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNÓSTICO DIRECTO ANN 4F vs OCSVM 4F\n",
    "print(\"DIAGNÓSTICO ANN 4F vs OCSVM 4F\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"ANN 4F - Media: {comparison_df['ANN_Prob_Mean'].mean():.4f} | \"\n",
    "      f\"Mediana: {comparison_df['ANN_Prob_Mean'].median():.4f} | \"\n",
    "      f\"Rango: [{comparison_df['ANN_Prob_Mean'].min():.4f}-{comparison_df['ANN_Prob_Mean'].max():.4f}]\")\n",
    "\n",
    "print(f\"OCSVM 4F - Media: {comparison_df['OCSVM_Anomaly_Rank'].mean():.1f}% | \"\n",
    "      f\"Mediana: {comparison_df['OCSVM_Anomaly_Rank'].median():.1f}% | \"\n",
    "      f\"Rango: [{comparison_df['OCSVM_Anomaly_Rank'].min():.1f}%-{comparison_df['OCSVM_Anomaly_Rank'].max():.1f}%]\")\n",
    "\n",
    "# IDENTIFICACIÓN DE CANDIDATOS\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nIDENTIFICACIÓN DE CANDIDATOS:\")\n",
    "ann_threshold = np.percentile(comparison_df['ANN_Prob_Mean'], 95)\n",
    "ocsvm_threshold = np.percentile(comparison_df['OCSVM_Anomaly_Rank'], 95)\n",
    "\n",
    "top_ann = comparison_df[comparison_df['ANN_Prob_Mean'] >= ann_threshold].sort_values('ANN_Prob_Mean', ascending=False)\n",
    "ocsvm_outliers = comparison_df[comparison_df['OCSVM_Prediction'] == -1].sort_values('OCSVM_Anomaly_Rank', ascending=False)\n",
    "top_ocsvm = comparison_df[comparison_df['OCSVM_Anomaly_Rank'] >= ocsvm_threshold].sort_values('OCSVM_Anomaly_Rank', ascending=False)\n",
    "\n",
    "print(f\"Top ANN (P95={ann_threshold:.4f}): {len(top_ann)} fuentes\")\n",
    "print(f\"OCSVM Outliers: {len(ocsvm_outliers)} fuentes\")\n",
    "print(f\"Top OCSVM (P95={ocsvm_threshold:.1f}%): {len(top_ocsvm)} fuentes\")\n",
    "\n",
    "# SOLAPAMIENTOS\n",
    "print(f\"\\nSOLAPAMIENTOS:\")\n",
    "overlap_restrictivo = set(top_ann['Source_ID']) & set(ocsvm_outliers['Source_ID'])\n",
    "overlap_amplio = set(top_ann['Source_ID']) & set(top_ocsvm['Source_ID'])\n",
    "\n",
    "print(f\"Top ANN ∩ OCSVM Outliers: {len(overlap_restrictivo)} fuentes\")\n",
    "if overlap_restrictivo:\n",
    "    print(f\"  IDs: {sorted(list(overlap_restrictivo))}\")\n",
    "\n",
    "print(f\"Top ANN ∩ Top OCSVM: {len(overlap_amplio)} fuentes\")\n",
    "if overlap_amplio:\n",
    "    print(f\"  IDs: {sorted(list(overlap_amplio))}\")\n",
    "\n",
    "# TOP 10 COMPARACIÓN DIRECTA\n",
    "print(f\"\\nTOP 10 ANN vs OCSVM:\")\n",
    "print(\"-\" * 60)\n",
    "for i, (_, row) in enumerate(top_ann.head(10).iterrows()):\n",
    "    status = \"Outlier\" if row['OCSVM_Prediction'] == -1 else \"Normal\"\n",
    "    print(f\"{i+1:2d}. ID {int(row['Source_ID']):4d}: ANN={row['ANN_Prob_Mean']:.4f} | OCSVM={row['OCSVM_Anomaly_Rank']:5.1f}% {status}\")\n",
    "\n",
    "print(f\"\\nOUTLIERS OCSVM vs ANN:\")\n",
    "print(\"-\" * 60)\n",
    "if len(ocsvm_outliers) > 0:\n",
    "    for i, (_, row) in enumerate(ocsvm_outliers.iterrows()):\n",
    "        print(f\"{i+1:2d}. ID {int(row['Source_ID']):4d}: OCSVM={row['OCSVM_Anomaly_Rank']:5.1f}% | ANN={row['ANN_Prob_Mean']:.4f}\")\n",
    "else:\n",
    "    print(\"No hay outliers detectados por OCSVM\")\n",
    "\n",
    "# ANÁLISIS DE CONCORDANCIA\n",
    "print(f\"\\nANÁLISIS DE CONCORDANCIA:\")\n",
    "# Correlación entre métricas\n",
    "correlation = np.corrcoef(comparison_df['ANN_Prob_Mean'], comparison_df['OCSVM_Anomaly_Rank'])[0, 1]\n",
    "print(f\"Correlación ANN-OCSVM: {correlation:.4f}\")\n",
    "\n",
    "# Concordancia en clasificación binaria\n",
    "ann_high = comparison_df['ANN_Prob_Mean'] >= ann_threshold\n",
    "ocsvm_high = comparison_df['OCSVM_Anomaly_Rank'] >= ocsvm_threshold\n",
    "\n",
    "concordancia = np.mean(ann_high == ocsvm_high) * 100\n",
    "print(f\"Concordancia en top 5%: {concordancia:.1f}%\")\n",
    "\n",
    "# Casos discrepantes\n",
    "discrepantes_ann_high_ocsvm_low = comparison_df[(comparison_df['ANN_Prob_Mean'] >= ann_threshold) & \n",
    "                                               (comparison_df['OCSVM_Anomaly_Rank'] < 50)]\n",
    "discrepantes_ocsvm_high_ann_low = comparison_df[(comparison_df['OCSVM_Anomaly_Rank'] >= ocsvm_threshold) & \n",
    "                                               (comparison_df['ANN_Prob_Mean'] < 0.5)]\n",
    "\n",
    "print(f\"Discrepantes (ANN alta, OCSVM baja): {len(discrepantes_ann_high_ocsvm_low)}\")\n",
    "print(f\"Discrepantes (OCSVM alta, ANN baja): {len(discrepantes_ocsvm_high_ann_low)}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "# RESUMEN EJECUTIVO\n",
    "print(f\"\\nRESUMEN EJECUTIVO:\")\n",
    "if len(overlap_restrictivo) > 0:\n",
    "    print(f\"Consenso encontrado: {len(overlap_restrictivo)} fuentes identificadas por ambos métodos\")\n",
    "else:\n",
    "    print(\"Sin consenso directo entre métodos\")\n",
    "\n",
    "if abs(correlation) > 0.5:\n",
    "    print(f\"Correlación moderada-alta: {correlation:.3f}\")\n",
    "else:\n",
    "    print(f\"Correlación baja: {correlation:.3f}\")\n",
    "\n",
    "if concordancia > 70:\n",
    "    print(f\"Buena concordancia en top candidatos: {concordancia:.1f}%\")\n",
    "else:\n",
    "    print(f\"Baja concordancia en top candidatos: {concordancia:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv DarkMatter_TFG)",
   "language": "python",
   "name": "venv-tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
