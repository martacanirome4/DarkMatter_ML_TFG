{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üìì OCSVM - 3F (sigma) - Validaci√≥n por Repetici√≥n\n",
    "\n",
    "**Proyecto**: Detecci√≥n de posibles fuentes de materia oscura usando ML en datos Fermi-LAT  \n",
    "**Autor**: Marta Canino Romero  \n",
    "**Fecha**: mayo 2025\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Descripci√≥n:\n",
    "\n",
    "Este notebook implementa un modelo **One-Class SVM** entrenado con datos de fuentes astrof√≠sicas conocidas (ASTRO) usando las siguientes caracter√≠sticas:\n",
    "\n",
    "- E_peak\n",
    "- beta\n",
    "- sigma\n",
    "\n",
    "**A diferencia de otros notebooks, aqu√≠ el proceso se repite m√∫ltiples veces (con diferentes semillas aleatorias en los splits) para comprobar la robustez y variabilidad del modelo.** Cada iteraci√≥n realiza:\n",
    "\n",
    "1. Divisi√≥n de datos (train/val/test)\n",
    "2. Escalado\n",
    "3. B√∫squeda de hiperpar√°metros (grid search sobre `nu` y `gamma`)\n",
    "4. Evaluaci√≥n sobre validaci√≥n y prueba\n",
    "5. Registro de m√©tricas clave (f1-score, n√∫mero de outliers, matriz de confusi√≥n)\n",
    "\n",
    "Los resultados de cada iteraci√≥n se almacenan y analizan para estimar la estabilidad del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Objetivos espec√≠ficos:\n",
    "\n",
    "- Evaluar la variabilidad del modelo OCSVM tras `N` repeticiones\n",
    "- Analizar la dispersi√≥n del mejor f1-score entre repeticiones\n",
    "- Comparar n√∫mero de anomal√≠as detectadas en cada iteraci√≥n\n",
    "- Obtener m√©tricas medias y desviaciones est√°ndar\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Entrada de datos:\n",
    "\n",
    "- `../../data/processed/XY_bal_log_Rel/astro/XY_bal_log_Rel_astro.txt`\n",
    "\n",
    "## üíæ Salida esperada:\n",
    "\n",
    "- Tabla resumen de m√©tricas por iteraci√≥n\n",
    "- Mejor combinaci√≥n de hiperpar√°metros promedio\n",
    "- Exportaci√≥n de anomal√≠as m√°s recurrentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cargar dataset ---\n",
    "data_path = \"../../data/processed/XY_bal_log_Rel/astro/XY_bal_log_Rel_astro.txt\"\n",
    "df_astro = pd.read_csv(data_path, sep=r\"\\s+\")\n",
    "\n",
    "# Renombramos la columna target por claridad\n",
    "df_astro = df_astro.rename(columns={\"0,1=astro,DM\": \"class\"})\n",
    "\n",
    "# --- Comprobamos distribuci√≥n del dataset ---\n",
    "print(f\"üìÅ Dataset cargado. Forma: {df_astro.shape}\")\n",
    "print(f\"üß† Nombres de las columnas: {list(df_astro.columns)}\")\n",
    "\n",
    "display(df_astro.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Selecci√≥n de caracter√≠sticas ---\n",
    "features = ['E_peak', 'beta', 'sigma']\n",
    "target = 'class'\n",
    "\n",
    "print(f\"‚úÖ Features seleccionadas: {features}\")\n",
    "print(f\"üéØ Columna objetivo: {target}\")\n",
    "\n",
    "# --- Comprobamos valores nulos ---\n",
    "print(\"\\nüîç Valores faltantes por columna:\")\n",
    "print(df_astro[features + [target]].isnull().sum())\n",
    "\n",
    "print(\"\\nüìå Muestra del dataset:\")\n",
    "display(df_astro[features + [target]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representaci√≥n 2D\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_astro,\n",
    "    x=\"E_peak\",\n",
    "    y=\"beta\",\n",
    "    color=\"turquoise\",  # \"skyblue\" \"cornflowerblue\"\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40\n",
    ")\n",
    "\n",
    "plt.title(\"2D Unscaled ASTRO Data: E_peak vs Beta\")\n",
    "plt.xlabel(\"E_peak\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representaci√≥n 3D\n",
    "x = df_astro['E_peak']\n",
    "y = df_astro['beta']\n",
    "z = df_astro['sigma']\n",
    "\n",
    "labels = df_astro['class']\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(x, y, z, c=labels, cmap='cool', edgecolor='k')\n",
    "\n",
    "ax.set_xlabel('E_peak')\n",
    "ax.set_ylabel('beta')\n",
    "ax.set_zlabel('sigma')\n",
    "plt.title('3D Unscaled ASTRO Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# UNIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unids_path = \"../../data/raw/unids_3F_beta_err_names.txt\"\n",
    "df_unids = pd.read_csv(unids_path, sep='\\s+')\n",
    "df_unids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos a logaritmo\n",
    "cols_to_log = [\"E_peak\", \"beta\", \"sigma_det\", \"beta_Rel\"]\n",
    "df_unids_log = df_unids.copy()\n",
    "df_unids_log[cols_to_log] = df_unids_log[cols_to_log].apply(lambda x: np.log10(x.clip(lower=1e-10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create subplots ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "\n",
    "# --- Plot 1: Raw UNIDS data ---\n",
    "sns.scatterplot(\n",
    "    data=df_unids,\n",
    "    x=\"E_peak\",\n",
    "    y=\"beta\",\n",
    "    color=\"gold\",\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"UNIDS Data: E_peak vs Beta\")\n",
    "axes[0].set_xlabel(\"E_peak\")\n",
    "axes[0].set_ylabel(\"beta\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- Plot 2: Log-transformed UNIDS ---\n",
    "sns.scatterplot(\n",
    "    data=df_unids_log,\n",
    "    x=\"E_peak\",\n",
    "    y=\"beta\",\n",
    "    color=\"gold\",\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"UNIDS (Log): E_peak vs Beta\")\n",
    "axes[1].set_xlabel(\"E_peak (log10)\")\n",
    "axes[1].set_ylabel(\"\")  # hide repeated ylabel\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Entrenar modelo con X iteraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "El random_state est√° fijado (controlado) con valores fijos: random_state=42+i y random_state=99+i\n",
    "\n",
    "Cada iteraci√≥n use la misma semilla de aleatoriedad y, por tanto, las mismas divisiones de datos ‚Üí\n",
    "‚Üí mismos splits ‚Üí mismos datos de entrenamiento y validaci√≥n ‚Üí mismos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "El random_state est√° fijado (controlado) con valores fijos: random_state=42+i y random_state=99+i\n",
    "\n",
    "Cada iteraci√≥n use la misma semilla de aleatoriedad y, por tanto, las mismas divisiones de datos \n",
    "‚Üí mismos splits ‚Üí mismos datos de entrenamiento y validaci√≥n ‚Üí mismos resultados.\"\"\"\n",
    "\n",
    "n_iterations = 5\n",
    "\n",
    "all_iterations_results = []\n",
    "best_models = []\n",
    "all_top_unids_ids = []  # aqu√≠ guardaremos los IDs top an√≥malos de cada iteraci√≥n\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"\\nüîÑ Iteraci√≥n {i+1}/{n_iterations}\")\n",
    "\n",
    "    # === 1Ô∏è‚É£ Split din√°mico\n",
    "    X = df_astro[features].values\n",
    "    y = df_astro[\"class\"].values\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, stratify=y, random_state=42 + i\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=99 + i\n",
    "    )\n",
    "\n",
    "    # === 2Ô∏è‚É£ Escalado\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # === 3Ô∏è‚É£ Grid search\n",
    "    nu_values = [0.005, 0.01, 0.02, 0.05]\n",
    "    gamma_values = ['scale', 'auto'] + list(np.logspace(-3, 1, 5))\n",
    "\n",
    "    best_score = 0.0\n",
    "    best_model_iter = None\n",
    "    best_params_iter = {}\n",
    "    iteration_results = []\n",
    "\n",
    "    for nu in nu_values:\n",
    "        for gamma in gamma_values:\n",
    "            model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma)\n",
    "            model.fit(X_train_scaled)\n",
    "\n",
    "            preds = model.predict(X_val_scaled)\n",
    "            pred_labels = np.where(preds == 1, 0, 1)\n",
    "            true_labels = y_val.astype(int)\n",
    "\n",
    "            f1 = f1_score(true_labels, pred_labels, pos_label=0)\n",
    "\n",
    "            iteration_results.append({'nu': nu, 'gamma': gamma, 'f1_score': f1})\n",
    "\n",
    "            if f1 > best_score:\n",
    "                best_score = f1\n",
    "                best_model_iter = model\n",
    "                best_params_iter = {'nu': nu, 'gamma': gamma}\n",
    "\n",
    "    print(f\"‚úÖ Iter {i+1}: nu={best_params_iter['nu']}, gamma={best_params_iter['gamma']}, F1={best_score:.4f}\")\n",
    "    all_iterations_results.extend([dict(iter=i+1, **res) for res in iteration_results])\n",
    "    best_models.append(best_model_iter)\n",
    "\n",
    "    # === 4Ô∏è‚É£ PREDECIR SOBRE UNIDs usando best_model_iter\n",
    "    # (asumiendo que tienes ya df_unids_log preparado previamente)\n",
    "\n",
    "    # Transformar UNIDs con el escalador de esta iteraci√≥n\n",
    "    X_unids_log = df_unids_log[[\"E_peak\", \"beta\", \"sigma_det\"]].values\n",
    "    X_unids_scaled = scaler.transform(X_unids_log)\n",
    "\n",
    "    # Predicciones\n",
    "    unids_preds = best_model_iter.predict(X_unids_scaled)\n",
    "    decision_scores = best_model_iter.decision_function(X_unids_scaled)\n",
    "\n",
    "    # Separar inliers y outliers\n",
    "    inliers = X_unids_scaled[unids_preds == 1]\n",
    "    outliers = X_unids_scaled[unids_preds == -1]\n",
    "\n",
    "    # === üé® SCATTER PLOT DE PREDICCIONES ===\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(inliers[:, 0], inliers[:, 1], c='gold', edgecolor='k', label='Inlier', alpha=0.6)\n",
    "    plt.scatter(outliers[:, 0], outliers[:, 1], c='red', edgecolor='k', label='Outlier', alpha=0.8)\n",
    "    plt.title(f\"Iteraci√≥n {i+1}: UNIDs predichos por OCSVM\")\n",
    "    plt.xlabel(\"E_peak (scaled)\")\n",
    "    plt.ylabel(\"beta (scaled)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    anom_scores = -decision_scores\n",
    "    anom_percent = MinMaxScaler(feature_range=(0, 100)).fit_transform(anom_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # A√±adir columnas al dataframe\n",
    "    df_unids_log[f\"svm_score_iter_{i+1}\"] = decision_scores\n",
    "    df_unids_log[f\"Anomaly_Score_iter_{i+1}\"] = anom_scores\n",
    "    df_unids_log[f\"Anomaly_Rank(%)_iter_{i+1}\"] = anom_percent\n",
    "    df_unids_log[f\"prediction_iter_{i+1}\"] = unids_preds\n",
    "\n",
    "    # Obtener top anomal√≠as\n",
    "    top_anomalies = df_unids_log[df_unids_log[f\"prediction_iter_{i+1}\"] == -1].sort_values(by=f\"Anomaly_Rank(%)_iter_{i+1}\", ascending=False).head(10)\n",
    "\n",
    "    # Guardar los IDs top anomal√≠as\n",
    "    top_ids = top_anomalies['number'].tolist()\n",
    "    all_top_unids_ids.append({'iter': i+1, 'top_unids_ids': top_ids})\n",
    "\n",
    "    print(f\"üìù Iteraci√≥n {i+1}: {len(top_ids)} anomal√≠as detectadas. Top IDs: {top_ids}\")\n",
    "\n",
    "# === 5Ô∏è‚É£ Convertir resultados\n",
    "df_all_results = pd.DataFrame(all_iterations_results)\n",
    "df_all_top_ids = pd.DataFrame(all_top_unids_ids)\n",
    "\n",
    "# Mostrar resumen\n",
    "display(df_all_results.sort_values(by='f1_score', ascending=False))\n",
    "display(df_all_top_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(best_models):\n",
    "    test_preds = model.predict(X_test_scaled)\n",
    "    test_labels = np.where(test_preds == 1, 0, 1)\n",
    "    print(f\"\\nüìù Iteraci√≥n {i+1}\")\n",
    "    print(confusion_matrix(y_test, test_labels))\n",
    "    print(classification_report(y_test, test_labels, target_names=['Normal', 'Anomal√≠a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: identificar hiperpar√°metros globales\n",
    "best_global = df_all_results.sort_values(by='f1_score', ascending=False).iloc[0]\n",
    "print(\"Mejor combinaci√≥n global:\")\n",
    "print(best_global)\n",
    "\n",
    "# Paso 2: reentrenar con X_train + X_val\n",
    "X_final_train = np.vstack([X_train, X_val])\n",
    "y_final_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "scaler_final = StandardScaler()\n",
    "X_final_train_scaled = scaler_final.fit_transform(X_final_train)\n",
    "X_test_scaled = scaler_final.transform(X_test)\n",
    "\n",
    "# Paso 3: entrenar modelo final\n",
    "final_model = OneClassSVM(kernel='rbf', nu=best_global['nu'], gamma=best_global['gamma'])\n",
    "final_model.fit(X_final_train_scaled)\n",
    "\n",
    "# Paso 4: evaluar en test\n",
    "test_preds = final_model.predict(X_test_scaled)\n",
    "test_labels = np.where(test_preds == 1, 0, 1)\n",
    "\n",
    "print(confusion_matrix(y_test, test_labels))\n",
    "print(classification_report(y_test, test_labels, target_names=['Normal', 'Anomal√≠a']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv DarkMatter_TFG)",
   "language": "python",
   "name": "venv-tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
