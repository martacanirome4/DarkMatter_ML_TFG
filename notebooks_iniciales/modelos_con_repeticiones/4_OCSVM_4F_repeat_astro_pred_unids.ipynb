{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCSVM - 4F - Validación por Repetición\n",
    "# Basado en nº de outliers\n",
    "\n",
    "**Proyecto**: Detección de posibles fuentes de materia oscura usando ML en datos Fermi-LAT  \n",
    "**Autor**: Marta Canino Romero  \n",
    "**Fecha**: mayo 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Descripción:\n",
    "\n",
    "Este notebook implementa un modelo **One-Class SVM** entrenado con datos de fuentes astrofísicas conocidas (ASTRO) usando las siguientes características:\n",
    "\n",
    "- E_peak\n",
    "- beta\n",
    "- sigma\n",
    "- beta_rel\n",
    "\n",
    "**A diferencia de otros notebooks, aquí el proceso se repite múltiples veces (con diferentes semillas aleatorias en los splits) para comprobar la robustez y variabilidad del modelo.** Cada iteración realiza:\n",
    "\n",
    "1. División de datos (train/val/test)\n",
    "2. Escalado\n",
    "3. Búsqueda de hiperparámetros (grid search sobre `nu` y `gamma`)\n",
    "4. Evaluación sobre validación y prueba\n",
    "5. Registro de métricas clave (f1-score, número de outliers, matriz de confusión)\n",
    "\n",
    "Los resultados de cada iteración se almacenan y analizan para estimar la estabilidad del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos específicos:\n",
    "\n",
    "- Evaluar la variabilidad del modelo OCSVM tras `N` repeticiones\n",
    "- Analizar la dispersión del mejor f1-score entre repeticiones\n",
    "- Comparar número de anomalías detectadas en cada iteración\n",
    "- Obtener métricas medias y desviaciones estándar\n",
    "\n",
    "---\n",
    "\n",
    "## Entrada de datos:\n",
    "\n",
    "- `../../data/processed/XY_bal_log_Rel/astro/XY_bal_log_Rel_astro.txt`\n",
    "\n",
    "## Salida esperada:\n",
    "\n",
    "- Tabla resumen de métricas por iteración\n",
    "- Mejor combinación de hiperparámetros promedio\n",
    "- Exportación de anomalías más recurrentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = \"../../data/processed/XY_bal_log_Rel/astro/XY_bal_log_Rel_astro.txt\"\n",
    "df_astro = pd.read_csv(data_path, sep='\\s+')\n",
    "\n",
    "df_astro = df_astro.rename(columns={\"0,1=astro,DM\": \"class\"})\n",
    "print(df_astro.columns)\n",
    "\n",
    "df_astro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Selección de características ---\n",
    "features = ['E_peak', 'beta', 'sigma', 'beta_Rel']\n",
    "target = 'class'\n",
    "\n",
    "print(f\"Features seleccionadas: {features}\")\n",
    "print(f\"Columna objetivo: {target}\")\n",
    "\n",
    "# --- Comprobamos valores nulos ---\n",
    "print(\"\\n Valores faltantes por columna:\")\n",
    "print(df_astro[features + [target]].isnull().sum())\n",
    "\n",
    "print(\"\\n Muestra del dataset:\")\n",
    "display(df_astro[features + [target]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación 2D\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_astro,\n",
    "    x=\"E_peak\",\n",
    "    y=\"beta\",\n",
    "    color=\"turquoise\",  # \"skyblue\" \"cornflowerblue\"\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40\n",
    ")\n",
    "\n",
    "plt.title(\"2D Unscaled ASTRO Data: E_peak vs Beta\")\n",
    "plt.xlabel(\"E_peak\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación 3D\n",
    "x = df_astro['E_peak']\n",
    "y = df_astro['beta']\n",
    "z = df_astro['sigma']\n",
    "\n",
    "labels = df_astro['class']\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(x, y, z, c=labels, cmap='cool', edgecolor='k')\n",
    "\n",
    "ax.set_xlabel('E_peak')\n",
    "ax.set_ylabel('beta')\n",
    "ax.set_zlabel('sigma')\n",
    "plt.title('3D Unscaled ASTRO Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Número de repeticiones\n",
    "n_iterations = 5\n",
    "\n",
    "# Guardar resultados de todas las iteraciones\n",
    "all_iterations_results = []\n",
    "best_models = []  # opcional: guardar modelo por iteración\n",
    "\n",
    "print(\"Buscando hiperparámetros que minimicen outliers en ASTRO (validación)...\")\n",
    "for i in range(n_iterations):\n",
    "    print(f\"\\nIteración {i+1}/{n_iterations}\")\n",
    "\n",
    "    # =============== 1️⃣ Split dinámico por iteración ===============\n",
    "    X = df_astro[features].values\n",
    "    y = df_astro[\"class\"].values\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, stratify=y, random_state=42 + i\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=99 + i\n",
    "    )\n",
    "\n",
    "    # =============== 2️⃣ Escalado ===============\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # =============== 3️⃣ Grid search en esta iteración ===============\n",
    "    nu_values = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05]\n",
    "    gamma_values = ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "    best_score = 0.0\n",
    "    iteration_results = []\n",
    "    best_outliers = np.inf\n",
    "    best_model_iter = None\n",
    "    best_params_iter = {}\n",
    "\n",
    "    for nu in nu_values:\n",
    "        for gamma in gamma_values:\n",
    "            model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma)\n",
    "            model.fit(X_train_scaled)\n",
    "\n",
    "            preds = model.predict(X_val_scaled)\n",
    "            pred_labels = np.where(preds == 1, 0, 1)  # 1→normal, -1→outlier\n",
    "            n_outliers = np.sum(preds == -1)\n",
    "            true_labels = y_val.astype(int)\n",
    "\n",
    "            f1 = f1_score(true_labels, pred_labels, pos_label=0)\n",
    "\n",
    "            iteration_results.append({'nu': nu, 'gamma': gamma, 'val_outliers': n_outliers, 'f1_score': f1})\n",
    "\n",
    "            \"\"\"\n",
    "            if f1 > best_score:\n",
    "                best_score = f1\n",
    "                best_model_iter = model\n",
    "                best_params_iter = {'nu': nu, 'gamma': gamma}\n",
    "            \"\"\"\n",
    "            if n_outliers < best_outliers:\n",
    "                best_outliers = n_outliers\n",
    "                best_model_iter = model\n",
    "                best_params_iter = {'nu': nu, 'gamma': gamma}\n",
    "\n",
    "    print(f\"Mejor iteración {i+1}: nu={best_params_iter['nu']}, gamma={best_params_iter['gamma']}, Outliers (val set): {best_outliers} de {len(X_val_scaled)} muestras, F1={best_score:.4f}\")\n",
    "\n",
    "    # Guardar resultados de esta iteración\n",
    "    all_iterations_results.extend(\n",
    "        [dict(iter=i+1, **res) for res in iteration_results]\n",
    "    )\n",
    "\n",
    "    best_models.append(best_model_iter)  # opcional\n",
    "\n",
    "# Convertimos a DataFrame global\n",
    "df_all_results = pd.DataFrame(all_iterations_results)\n",
    "\n",
    "# Mostrar el top global\n",
    "display(df_all_results.sort_values(by='f1_score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# Paso 1: identificar hiperparámetros globales\n",
    "best_global = df_all_results.sort_values(by='f1_score', ascending=False).iloc[0]\n",
    "print(\"Mejor combinación global:\")\n",
    "print(best_global)\n",
    "\n",
    "# Paso 2: reentrenar con X_train + X_val\n",
    "X_final_train = np.vstack([X_train, X_val])\n",
    "y_final_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "scaler_final = StandardScaler()\n",
    "X_final_train_scaled = scaler_final.fit_transform(X_final_train)\n",
    "X_test_scaled = scaler_final.transform(X_test)\n",
    "\n",
    "# Paso 3: entrenar modelo final\n",
    "final_model = OneClassSVM(kernel='rbf', nu=best_global['nu'], gamma=best_global['gamma'])\n",
    "final_model.fit(X_final_train_scaled)\n",
    "\n",
    "# Paso 4: evaluar en test\n",
    "test_preds = final_model.predict(X_test_scaled)\n",
    "test_labels = np.where(test_preds == 1, 0, 1)\n",
    "\n",
    "# Paso 5: métricas\n",
    "print(\"\\nEvaluación en el conjunto de test:\")\n",
    "print(f\"Outliers (test set): {np.sum(test_preds == -1)} de {len(X_test_scaled)} muestras\")\n",
    "print(f\"F1 Score: {f1_score(y_test, test_labels, pos_label=0):.4f}\")\n",
    "print(\"\\nMatriz de confusión:\")\n",
    "cm = confusion_matrix(y_test, test_labels)\n",
    "print(cm)\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_test, test_labels, target_names=unique_labels(y_test, test_labels).astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluar el modelo en el conjunto de validación\n",
    "val_preds = final_model.predict(X_val_scaled)\n",
    "n_val_outliers = np.sum(val_preds == -1)\n",
    "print(f\" Outliers (val set): {n_val_outliers} de {len(X_val_scaled)} muestras\")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de test\n",
    "# (no se ha visto en el entrenamiento)\n",
    "test_preds = final_model.predict(X_test_scaled)\n",
    "n_test_outliers = np.sum(test_preds == -1)\n",
    "print(f\" Outliers (test set): {n_test_outliers} de {len(X_test_scaled)} muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Fit OCSVM on original high-dim data\n",
    "ocsvm = OneClassSVM(kernel='rbf', nu=best_global['nu'], gamma=best_global['gamma'])\n",
    "ocsvm.fit(X_test_scaled)\n",
    "\n",
    "# Predict outliers\n",
    "y_pred_ocsvm = ocsvm.predict(X_test_scaled)\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_2d = tsne.fit_transform(X_test_scaled)\n",
    "\n",
    "# Plot the t-SNE representation, coloring by model prediction\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_2d[y_pred_ocsvm == 1, 0], X_2d[y_pred_ocsvm == 1, 1], c='skyblue', edgecolors='k', label='Inliers')\n",
    "plt.scatter(X_2d[y_pred_ocsvm == -1, 0], X_2d[y_pred_ocsvm == -1, 1], c='red', edgecolors='k', label='Outliers')\n",
    "plt.title(\"t-SNE projection colored by OCSVM prediction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos predicciones en 3D sobre datos de validación\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X_3d = X_val_scaled\n",
    "\n",
    "# Predecimos de nuevo sobre validación para la visualización\n",
    "preds = final_model.predict(X_3d)\n",
    "\n",
    "ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=preds, cmap='cool', edgecolor='k', s=30)\n",
    "\n",
    "ax.set_xlabel(features[0])\n",
    "ax.set_ylabel(features[1])\n",
    "ax.set_zlabel(features[2])\n",
    "plt.title(\"Distribución 3D con predicción (validación)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos predicciones en 3D sobre datos de prueba\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X_3d = X_test_scaled\n",
    "preds = final_model.predict(X_3d)\n",
    "ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=preds, cmap='cool', edgecolor='k', s=30)\n",
    "\n",
    "ax.set_xlabel(features[0])\n",
    "ax.set_ylabel(features[1])\n",
    "ax.set_zlabel(features[2])\n",
    "plt.title(\"Distribución 3D con predicción (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on UNIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unids_path = \"../../data/raw/unids_3F_beta_err_names.txt\"\n",
    "df_unids = pd.read_csv(unids_path, sep='\\s+')\n",
    "df_unids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos a logaritmo\n",
    "cols_to_log = [\"E_peak\", \"beta\", \"sigma_det\", \"beta_Rel\"]\n",
    "df_unids_log = df_unids.copy()\n",
    "df_unids_log[cols_to_log] = df_unids_log[cols_to_log].apply(lambda x: np.log10(x.clip(lower=1e-10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer y escalar\n",
    "X_unids_log = df_unids_log[[\"E_peak\", \"beta\", \"sigma_det\", \"beta_Rel\"]].values\n",
    "X_unids_scaled = scaler.transform(X_unids_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create subplots ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "\n",
    "# --- Plot 1: Raw UNIDS data ---\n",
    "sns.scatterplot(\n",
    "    data=df_unids,\n",
    "    x=\"E_peak\",\n",
    "    y=\"beta\",\n",
    "    color=\"gold\",\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"UNIDS Data: E_peak vs Beta\")\n",
    "axes[0].set_xlabel(\"E_peak\")\n",
    "axes[0].set_ylabel(\"beta\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- Plot 2: Log-transformed UNIDS ---\n",
    "sns.scatterplot(\n",
    "    data=df_unids_log,\n",
    "    x=\"E_peak\",\n",
    "    y=\"beta\",\n",
    "    color=\"gold\",\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"UNIDS (Log): E_peak vs Beta\")\n",
    "axes[1].set_xlabel(\"E_peak (log10)\")\n",
    "axes[1].set_ylabel(\"\")  # hide repeated ylabel\n",
    "axes[1].grid(True)\n",
    "\n",
    "# --- Plot 3: Scaled UNIDS ---\n",
    "sns.scatterplot(\n",
    "    x=X_unids_scaled[:, 0],\n",
    "    y=X_unids_scaled[:, 1],\n",
    "    color=\"gold\",\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title(\"Scaled UNIDS: E_peak vs Beta\")\n",
    "axes[2].set_xlabel(\"E_peak (scaled)\")\n",
    "axes[2].set_ylabel(\"\")  # hide repeated ylabel\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de 3D de UNIDS\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_unids_scaled[:, 0], X_unids_scaled[:, 1], X_unids_scaled[:, 2], color=\"gold\", edgecolor='k', alpha=0.7, s=40)\n",
    "ax.set_xlabel('E_peak (scaled)')\n",
    "ax.set_ylabel('beta (scaled)')\n",
    "ax.set_zlabel('sigma_det (scaled)')\n",
    "plt.title('3D UNIDS Data (scaled)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter de UNIDs vs ASTRO 3D\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter ASTRO (training data)\n",
    "ax.scatter(\n",
    "    X_train_scaled[:, 0], X_train_scaled[:, 1], X_train_scaled[:, 2],\n",
    "    color=\"steelblue\", edgecolor='k', alpha=0.2, s=60, label='ASTRO (train)',\n",
    ")\n",
    "\n",
    "# Scatter UNIDs (to predict)\n",
    "ax.scatter(\n",
    "    X_unids_scaled[:, 0], X_unids_scaled[:, 1], X_unids_scaled[:, 2],\n",
    "    color=\"gold\", edgecolor='k', alpha=0.9, s=60, label='UNIDs',\n",
    "    marker='^'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('E_peak (scaled)')\n",
    "ax.set_ylabel('beta (scaled)')\n",
    "ax.set_zlabel('sigma_det (scaled)')\n",
    "ax.set_title('3D Scatter: ASTRO vs UNIDs (scaled)')\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter de unids escalados vs datos de entrenamiento escalados\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_final_train_scaled[:, 0],\n",
    "    y=X_final_train_scaled[:, 1],\n",
    "    color=\"skyblue\",  # \"skyblue\" \"cornflowerblue\"\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    label='Astro'\n",
    ")\n",
    "sns.scatterplot(\n",
    "    x=X_unids_scaled[:, 0],\n",
    "    y=X_unids_scaled[:, 1],\n",
    "    color=\"gold\",  # \"skyblue\" \"cornflowerblue\"\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    label='Unids'\n",
    ")\n",
    "plt.title(\"Scaled UNIDS Data vs Train Data: E_peak vs Beta\")\n",
    "plt.xlabel(\"E_peak (scaled)\")\n",
    "plt.ylabel(\"beta (scaled)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones sobre UNIDS\n",
    "unids_preds = final_model.predict(X_unids_scaled)\n",
    "\n",
    "n_unids_outliers = np.sum(unids_preds == -1)\n",
    "n_unids_normals = np.sum(unids_preds == 1)\n",
    "\n",
    "print(f\" Predicted ASTRO-like: {n_unids_normals}\")\n",
    "print(f\" Predicted not ASTRO-like (anomalies): {n_unids_outliers}\")\n",
    "unids_labels = np.where(unids_preds == 1, 0, 1)  # 1 = normal, -1 = outlier → mapeado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers = X_unids_scaled[unids_preds == 1]\n",
    "outliers = X_unids_scaled[unids_preds == -1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(inliers[:, 0], inliers[:, 1], c='gold', edgecolors='k', label='Inlier (likely astro)', alpha=0.6)\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], c='red', edgecolors='k', label='Outlier (potentially new)', alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"E_peak (scaled)\")\n",
    "plt.ylabel(\"beta (scaled)\")\n",
    "\n",
    "plt.title(\"UNIDs: Inlier vs Outlier (One-Class SVM)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos predicciones en 3D sobre UNIDS\n",
    "\n",
    "# Get predictions from best model on UNIDs\n",
    "preds = final_model.predict(X_unids_scaled)  # 1 = inlier, -1 = outlier\n",
    "\n",
    "# Separate indices\n",
    "inlier_idx = preds == 1\n",
    "outlier_idx = preds == -1\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Inliers (normal)\n",
    "ax.scatter(\n",
    "    X_unids_scaled[inlier_idx, 0], X_unids_scaled[inlier_idx, 1], X_unids_scaled[inlier_idx, 2],\n",
    "    c='gold', edgecolor='k', s=30, label='Inlier (normal)', alpha=0.8\n",
    ")\n",
    "\n",
    "# Outliers (potential dark matter)\n",
    "ax.scatter(\n",
    "    X_unids_scaled[outlier_idx, 0], X_unids_scaled[outlier_idx, 1], X_unids_scaled[outlier_idx, 2],\n",
    "    c='crimson', marker='^', edgecolor='k', s=50, label='Outlier (anomaly)', alpha=0.9\n",
    ")\n",
    "\n",
    "ax.set_xlabel('E_peak (scaled)')\n",
    "ax.set_ylabel('beta (scaled)')\n",
    "ax.set_zlabel('sigma_det (scaled)')\n",
    "ax.set_title(\"3D Prediction Results on UNIDs\")\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Scoring - UNIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_scores = final_model.decision_function(X_unids_scaled)  # Higher = more normal, lower = more anomalous\n",
    "\n",
    "unids_preds = final_model.predict(X_unids_scaled)  # 1 = inlier, -1 = outlier\n",
    "\n",
    "df_unids_log[\"svm_score\"] = decision_scores\n",
    "df_unids_log[\"prediction\"] = unids_preds\n",
    "\n",
    "anom_scores = -decision_scores  # Invert: higher = more anomalous\n",
    "anom_percent = MinMaxScaler(feature_range=(0, 100)).fit_transform(anom_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "df_unids_log[\"Anomaly_Score\"] = anom_scores\n",
    "df_unids_log[\"Anomaly_Rank(%)\"] = anom_percent\n",
    "\n",
    "top_anomalies = df_unids_log[df_unids_log[\"prediction\"] == -1].sort_values(by=\"Anomaly_Rank(%)\", ascending=False).head(10)\n",
    "most_anomalous_idx = top_anomalies.index\n",
    "\n",
    "top_anomalies.to_csv(\"../../data/processed/unids_most_anomalous_4F_repeat.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Top Most Anomalous UNID Sources (4F One-Class SVM):\")\n",
    "display(top_anomalies[['number', 'Anomaly_Score', 'Anomaly_Rank(%)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort anomaly scores and grab top N labels\n",
    "N = 10\n",
    "sorted_idx = np.argsort(-anom_percent)  # high anomaly % = more anomalous\n",
    "top_N_idx = sorted_idx[:N]\n",
    "\n",
    "top_labels = df_unids_log.iloc[top_N_idx]['number'].astype(str).values\n",
    "top_scores = anom_percent[top_N_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(anom_percent, bins=50, color='blue', edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Anomaly Percentage Distribution (UNID Sources)\")\n",
    "plt.xlabel(\"Anomaly % (higher = more anomalous)\")\n",
    "plt.ylabel(\"Number of Sources\")\n",
    "\n",
    "for i in range(N):\n",
    "    x = top_scores[i]\n",
    "    label = top_labels[i]\n",
    "    plt.axvline(x, color='crimson', linestyle='--', alpha=0.8)\n",
    "    plt.text(x + 0.5, 3 + (i % 2) * 2, f\"ID {label}\", rotation=90, color='crimson', ha='left', fontsize=9)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Inliers and outliers from scaled data\n",
    "inliers = X_unids_scaled[unids_preds == 1]\n",
    "outliers = X_unids_scaled[unids_preds == -1]\n",
    "\n",
    "# Plot inliers (gold)\n",
    "plt.scatter(inliers[:, 0], inliers[:, 1], c='gold', edgecolors='k', label='Inlier (astro-like)', alpha=0.6)\n",
    "\n",
    "# Plot outliers (red)\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], c='red', edgecolors='k', label='Outlier (anomalous)', alpha=0.6)\n",
    "\n",
    "# Annotate top 10 anomalies by ID\n",
    "for idx in most_anomalous_idx:\n",
    "    x = X_unids_scaled[idx, 0]  # E_peak (scaled)\n",
    "    y = X_unids_scaled[idx, 1]  # beta (scaled)\n",
    "    label = int(df_unids_log.loc[idx, 'number'])\n",
    "    plt.text(x + 0.1, y, str(label), color='black', fontsize=9)\n",
    "\n",
    "# Axis labels and styling\n",
    "plt.xlabel(\"E_peak (scaled)\")\n",
    "plt.ylabel(\"beta (scaled)\")\n",
    "plt.title(\"UNID Sources (2F) – Inliers vs Anomalies with ID Labels\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../../outputs/figures/scaled/2F_UNIDs_OneClassSVM_2D_scaled.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Original/log-transformed values for 3 relevant features\n",
    "x_vals = df_unids_log['E_peak'].values\n",
    "y_vals = df_unids_log['beta'].values\n",
    "z_vals = df_unids_log['sigma_det'].values  # or 'beta_Rel' for 3F.2\n",
    "\n",
    "# Use model predictions for coloring\n",
    "inlier_idx = df_unids_log['prediction'] == 1\n",
    "outlier_idx = df_unids_log['prediction'] == -1\n",
    "\n",
    "# Inliers\n",
    "ax.scatter(\n",
    "    x_vals[inlier_idx], y_vals[inlier_idx], z_vals[inlier_idx],\n",
    "    c='#1f77b4', edgecolor='k', s=30, label='Inlier (astro-like)', alpha=0.5\n",
    ")\n",
    "\n",
    "# Outliers\n",
    "ax.scatter(\n",
    "    x_vals[outlier_idx], y_vals[outlier_idx], z_vals[outlier_idx],\n",
    "    c='#d62728', marker='^', edgecolor='k', s=50, label='Outlier (anomaly)', alpha=0.9\n",
    ")\n",
    "\n",
    "# Annotate top anomalies\n",
    "top_anomalies = df_unids_log[df_unids_log['prediction'] == -1].sort_values('Anomaly_Rank(%)', ascending=False).head(10)\n",
    "for idx in top_anomalies.index:\n",
    "    ax.scatter(\n",
    "        x_vals[idx], y_vals[idx], z_vals[idx],\n",
    "        facecolors='none', edgecolors='black', linewidths=2, s=100\n",
    "    )\n",
    "    ax.text(\n",
    "        x_vals[idx], y_vals[idx], z_vals[idx] + 0.05,\n",
    "        str(int(df_unids_log.loc[idx, 'number'])),\n",
    "        color='black', fontsize=9\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('E_peak')\n",
    "ax.set_ylabel('beta')\n",
    "ax.set_zlabel('sigma_det')  # or 'beta_Rel'\n",
    "ax.set_title(\"2F UNID Sources – Anomalies in Original Feature Space\")\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "ax.tick_params(colors='#333333')\n",
    "ax.xaxis.label.set_color('#333333')\n",
    "ax.yaxis.label.set_color('#333333')\n",
    "ax.zaxis.label.set_color('#333333')\n",
    "ax.title.set_color('#111111')\n",
    "ax.grid(color='#aaaaaa', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.savefig(\"../../outputs/figures/2F_UNIDs_OneClassSVM_og.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparar con UNIDs ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de unIDs (features)\n",
    "unids_3F = np.genfromtxt('../../data/raw/unids_3F_beta_err_names.txt', dtype='str') \n",
    "unids_3F_data = np.asarray(unids_3F[1:, :], dtype=float)\n",
    "unids_log = np.log10(unids_3F_data[:, [0,1,2,3]])\n",
    "\n",
    "N_unids = unids_log.shape[0]\n",
    "N_splits = 5\n",
    "N_Repeats = 1\n",
    "N_sample = N_splits * N_Repeats\n",
    "\n",
    "# Cargar las predicciones ANN 4F\n",
    "ann_unids_path = \"../../ANN_original/unids_DM_std_proba_check_repeated_kfold_rskf_4F_21.txt\"\n",
    "unids_DM_raw = np.genfromtxt(ann_unids_path, dtype='str')[1:]  # quitar cabecera\n",
    "unids_DM_data = np.asarray(unids_DM_raw, dtype=float)\n",
    "\n",
    "# Reconstruir matriz (N_unids, N_sample)\n",
    "unids_number = unids_DM_data[:N_unids, 0]  # índice de los unIDs\n",
    "prob_matrix = np.reshape(unids_DM_data[:, 1], (N_unids, N_sample))\n",
    "\n",
    "# Calcular media y desviación estándar por unID\n",
    "unids_mean = prob_matrix.mean(axis=1)\n",
    "unids_std = prob_matrix.std(axis=1, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con resultados ANN\n",
    "df_ann = pd.DataFrame({\n",
    "    'unid_idx': unids_number.astype(int),\n",
    "    'ann_mean_prob': unids_mean,\n",
    "    'ann_std_prob': unids_std\n",
    "})\n",
    "\n",
    "df_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar coincidencia de identificadores\n",
    "df_unids_log['unid_idx'] = df_unids_log['number'].astype(int)\n",
    "\n",
    "# Unir con resultados de OCSVM\n",
    "df_combined = pd.merge(df_unids_log, df_ann, on=\"unid_idx\", how=\"inner\")\n",
    "\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. Análisis de solapamiento\n",
    "print(\"Total unIDs ann:\", len(df_ann))\n",
    "print(\"Total unIDs ocsvm:\", len(df_unids_log))\n",
    "print(\"High ANN prob (≥0.9):\", (df_ann['ann_mean_prob'] >= 0.9).sum())\n",
    "print(\"High ANN prob (≥0.7):\", (df_ann['ann_mean_prob'] >= 0.7).sum())\n",
    "print(\"High ANN prob (≥0.5):\", (df_ann['ann_mean_prob'] >= 0.5).sum())\n",
    "print(\"Predicted anomalies (OCSVM):\", (df_unids_log['prediction'] == -1).sum())\n",
    "print(\"Anomaly rank ≥95%:\", (df_unids_log['Anomaly_Rank(%)'] >= 95).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. Extraer unIDs destacados por ambos modelos\n",
    "p_cut = 0.4\n",
    "anom_cut = 40\n",
    "\n",
    "top_candidates = df_combined[\n",
    "    (df_combined[\"ann_mean_prob\"] >= p_cut) &\n",
    "    (df_combined[\"Anomaly_Rank(%)\"] >= anom_cut)\n",
    "].sort_values(by=\"Anomaly_Rank(%)\", ascending=False).head(10)\n",
    "\n",
    "print(\"Top UNIDs by ANN and OCSVM agreement:\")\n",
    "display(top_candidates[[\"unid_idx\", \"E_peak\", \"beta\", \"ann_mean_prob\", \"Anomaly_Rank(%)\", \"Anomaly_Score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_combined[\"ann_mean_prob\"], df_combined[\"Anomaly_Rank(%)\"], alpha=0.8)\n",
    "plt.axvline(p_cut, color='red', linestyle='--', label=f'ANN prob ≥ {p_cut}')\n",
    "plt.axhline(anom_cut, color='green', linestyle='--', label=f'Anomaly Rank ≥ {anom_cut}')\n",
    "plt.xlabel(\"Probabilidad media ANN 4F (DM-like)\")\n",
    "plt.ylabel(\"Rango de anomalía OCSVM 4F (%)\")\n",
    "plt.title(\"Comparación entre ANN 4F y OCSVM 4F en unIDs\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"comparacion_ann4f_ocsvm4f.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv DarkMatter_TFG)",
   "language": "python",
   "name": "venv-tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
