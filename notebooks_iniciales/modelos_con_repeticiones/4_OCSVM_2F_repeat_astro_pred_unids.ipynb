{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ OCSVM - 2F - ValidaciÃ³n por RepeticiÃ³n\n",
    "# Basado en nÂº de outliers\n",
    "\n",
    "**Proyecto**: DetecciÃ³n de posibles fuentes de materia oscura usando ML en datos Fermi-LAT  \n",
    "**Autor**: Marta Canino Romero  \n",
    "**Fecha**: mayo 2025\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ DescripciÃ³n:\n",
    "\n",
    "Este notebook implementa un modelo **One-Class SVM** entrenado con datos de fuentes astrofÃ­sicas conocidas (ASTRO) usando las siguientes caracterÃ­sticas:\n",
    "\n",
    "- E_peak\n",
    "- beta\n",
    "\n",
    "**A diferencia de otros notebooks, aquÃ­ el proceso se repite mÃºltiples veces (con diferentes semillas aleatorias en los splits) para comprobar la robustez y variabilidad del modelo.** Cada iteraciÃ³n realiza:\n",
    "\n",
    "1. DivisiÃ³n de datos (train/val/test)\n",
    "2. Escalado\n",
    "3. BÃºsqueda de hiperparÃ¡metros (grid search sobre `nu` y `gamma`)\n",
    "4. EvaluaciÃ³n sobre validaciÃ³n y prueba\n",
    "5. Registro de mÃ©tricas clave (f1-score, nÃºmero de outliers, matriz de confusiÃ³n)\n",
    "\n",
    "Los resultados de cada iteraciÃ³n se almacenan y analizan para estimar la estabilidad del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Objetivos especÃ­ficos:\n",
    "\n",
    "- Evaluar la variabilidad del modelo OCSVM tras `N` repeticiones\n",
    "- Analizar la dispersiÃ³n del mejor f1-score entre repeticiones\n",
    "- Comparar nÃºmero de anomalÃ­as detectadas en cada iteraciÃ³n\n",
    "- Obtener mÃ©tricas medias y desviaciones estÃ¡ndar\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—‚ï¸ Entrada de datos:\n",
    "\n",
    "- `../../data/processed/XY_bal_log_Rel/astro/XY_bal_log_Rel_astro.txt`\n",
    "\n",
    "## ðŸ’¾ Salida esperada:\n",
    "\n",
    "- Tabla resumen de mÃ©tricas por iteraciÃ³n\n",
    "- Mejor combinaciÃ³n de hiperparÃ¡metros promedio\n",
    "- ExportaciÃ³n de anomalÃ­as mÃ¡s recurrentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# data_path = \"../../data/processed/XY_bal_log_Rel/astro/XY_bal_log_Rel_astro.txt\"\n",
    "data_path = \"../../data/processed/XY_bal_log_Rel/astro/astro_df.txt\"\n",
    "df_astro = pd.read_csv(data_path, sep='\\s+')\n",
    "\n",
    "df_astro = df_astro.rename(columns={\"0,1=astro,DM\": \"class\"})\n",
    "print(df_astro.columns)\n",
    "\n",
    "df_astro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SelecciÃ³n de caracterÃ­sticas ---\n",
    "features = ['Log(E_peak)', 'Log(beta)']\n",
    "target = 'astro_DM'\n",
    "\n",
    "print(f\"Features seleccionadas: {features}\")\n",
    "print(f\"Columna objetivo: {target}\")\n",
    "\n",
    "# --- Comprobamos valores nulos ---\n",
    "print(\"\\n Valores faltantes por columna:\")\n",
    "print(df_astro[features + [target]].isnull().sum())\n",
    "\n",
    "print(\"\\n Muestra del dataset:\")\n",
    "display(df_astro[features + [target]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos la distribuciÃ³n de los datos astro antes de escalar\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_astro,\n",
    "    x=\"Log(E_peak)\",\n",
    "    y=\"Log(beta)\",\n",
    "    color=\"cornflowerblue\",  # \"skyblue\" \"cornflowerblue\"\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40\n",
    ")\n",
    "\n",
    "plt.title(\"ASTRO Data: Log(E_peak) vs Log(Beta)\")\n",
    "plt.xlabel(\"Log(E_peak)\")\n",
    "plt.ylabel(\"Log(Beta)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ’¥ NÃºmero de repeticiones\n",
    "n_iterations = 5\n",
    "\n",
    "# Guardar resultados de todas las iteraciones\n",
    "all_iterations_results = []\n",
    "best_models = []  # opcional: guardar modelo por iteraciÃ³n\n",
    "\n",
    "print(\"ðŸ” Buscando hiperparÃ¡metros que minimicen outliers en ASTRO (validaciÃ³n)...\")\n",
    "for i in range(n_iterations):\n",
    "    print(f\"\\nðŸ”„ IteraciÃ³n {i+1}/{n_iterations}\")\n",
    "\n",
    "    # =============== 1ï¸âƒ£ Split dinÃ¡mico por iteraciÃ³n ===============\n",
    "    X = df_astro[features].values\n",
    "    y = df_astro[target].values\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, stratify=y, random_state=42 + i\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=99 + i\n",
    "    )\n",
    "\n",
    "    # =============== 2ï¸âƒ£ Escalado ===============\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # =============== 3ï¸âƒ£ Grid search en esta iteraciÃ³n ===============\n",
    "    nu_values = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05]\n",
    "    # gamma_values = ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10]\n",
    "    gamma_values = [0.1] \n",
    "\n",
    "    best_score = 0.0\n",
    "    iteration_results = []\n",
    "    best_outliers = np.inf\n",
    "    best_model_iter = None\n",
    "    best_params_iter = {}\n",
    "\n",
    "    for nu in nu_values:\n",
    "        for gamma in gamma_values:\n",
    "            model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma)\n",
    "            model.fit(X_train_scaled)\n",
    "\n",
    "            preds = model.predict(X_val_scaled)\n",
    "            pred_labels = np.where(preds == 1, 0, 1)  # 1â†’normal, -1â†’outlier\n",
    "            n_outliers = np.sum(preds == -1)\n",
    "            true_labels = y_val.astype(int)\n",
    "\n",
    "            # f1 = f1_score(true_labels, pred_labels, pos_label=0)\n",
    "\n",
    "            iteration_results.append({'nu': nu, 'gamma': gamma, 'val_outliers': n_outliers})\n",
    "\n",
    "            \"\"\"\n",
    "            if f1 > best_score:\n",
    "                best_score = f1\n",
    "                best_model_iter = model\n",
    "                best_params_iter = {'nu': nu, 'gamma': gamma}\n",
    "            \"\"\"\n",
    "            if n_outliers < best_outliers:\n",
    "                best_outliers = n_outliers\n",
    "                best_model_iter = model\n",
    "                best_params_iter = {'nu': nu, 'gamma': gamma}\n",
    "\n",
    "    print(f\"âœ… Mejor iteraciÃ³n {i+1}: nu={best_params_iter['nu']}, gamma={best_params_iter['gamma']}, Outliers (val set): {best_outliers} de {len(X_val_scaled)} muestras\")\n",
    "\n",
    "    # Guardar resultados de esta iteraciÃ³n\n",
    "    all_iterations_results.extend(\n",
    "        [dict(iter=i+1, **res) for res in iteration_results]\n",
    "    )\n",
    "\n",
    "    best_models.append(best_model_iter)  # opcional\n",
    "\n",
    "# onvertimos a DataFrame global\n",
    "df_all_results = pd.DataFrame(all_iterations_results)\n",
    "\n",
    "# Mostrar el top global\n",
    "display(df_all_results.sort_values(by='val_outliers', ascending=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# Paso 1: identificar hiperparÃ¡metros globales\n",
    "best_global = df_all_results.sort_values(by='val_outliers', ascending=True).iloc[0]\n",
    "print(\"Mejor combinaciÃ³n global:\")\n",
    "print(best_global)\n",
    "\n",
    "# Paso 2: reentrenar con X_train + X_val\n",
    "X_final_train = np.vstack([X_train, X_val])\n",
    "y_final_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "scaler_final = StandardScaler()\n",
    "X_final_train_scaled = scaler_final.fit_transform(X_final_train)\n",
    "X_test_scaled = scaler_final.transform(X_test)\n",
    "\n",
    "# Paso 3: entrenar modelo final\n",
    "final_model = OneClassSVM(kernel='rbf', nu=best_global['nu'], gamma=best_global['gamma'])\n",
    "final_model.fit(X_final_train_scaled)\n",
    "\n",
    "# Paso 4: evaluar en test\n",
    "test_preds = final_model.predict(X_test_scaled)\n",
    "test_labels = np.where(test_preds == 1, 0, 1)\n",
    "\n",
    "# Paso 5: mÃ©tricas\n",
    "print(\"\\nEvaluaciÃ³n en el conjunto de test:\")\n",
    "print(f\"Outliers (test set): {np.sum(test_preds == -1)} de {len(X_test_scaled)} muestras\")\n",
    "# print(f\"F1 Score: {f1_score(y_test, test_labels, pos_label=0):.4f}\")\n",
    "print(\"\\nMatriz de confusiÃ³n:\")\n",
    "cm = confusion_matrix(y_test, test_labels)\n",
    "print(cm)\n",
    "print(\"\\nReporte de clasificaciÃ³n:\")\n",
    "print(classification_report(y_test, test_labels, target_names=unique_labels(y_test, test_labels).astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meshgrid (with correct feature order)\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X_test_scaled[:, 0].min() - 0.5, X_test_scaled[:, 0].max() + 0.5, 300),  # E_peak\n",
    "    np.linspace(X_test_scaled[:, 1].min() - 0.5, X_test_scaled[:, 1].max() + 0.5, 300)   # beta\n",
    ")\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z = final_model.decision_function(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu_r, alpha=0.8)\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='crimson', linestyles='--')\n",
    "\n",
    "# Plot training and test data\n",
    "plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c='darkblue', edgecolors='k', s=40, label='Test')\n",
    "plt.scatter(X_final_train_scaled[:, 0], X_final_train_scaled[:, 1], c='skyblue', edgecolors='k', s=40, label='Train')\n",
    "\n",
    "plt.xlabel(\"E_peak (scaled)\")\n",
    "plt.ylabel(\"beta (scaled)\")\n",
    "plt.title(\"Best One-Class SVM Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on UNIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unids_path = \"../../data/raw/unids_3F_beta_err_names.txt\"\n",
    "unids_path = \"../../data/processed/unids_log/unids_log.txt\"\n",
    "df_unids = pd.read_csv(unids_path, sep='\\s+')\n",
    "df_unids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer y escalar\n",
    "X_unids_log = df_unids[[\"Log(E_peak)\", \"Log(beta)\"]].values\n",
    "X_unids_scaled = scaler.transform(X_unids_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create subplots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6), sharey=True)\n",
    "\n",
    "# --- Plot 1: Raw UNIDS data ---\n",
    "sns.scatterplot(\n",
    "    data=df_unids,\n",
    "    x=\"Log(E_peak)\",\n",
    "    y=\"Log(beta)\",\n",
    "    color=\"gold\",\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"UNIDS Data: E_peak vs Beta\")\n",
    "axes[0].set_xlabel(\"Log(E_peak)\")\n",
    "axes[0].set_ylabel(\"Log(beta)\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- Plot 3: Scaled UNIDS ---\n",
    "sns.scatterplot(\n",
    "    x=X_unids_scaled[:, 0],\n",
    "    y=X_unids_scaled[:, 1],\n",
    "    color=\"gold\",\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Scaled UNIDS: E_peak vs Beta\")\n",
    "axes[1].set_xlabel(\"Log(E_peak) (scaled)\")\n",
    "axes[1].set_ylabel(\"Log(beta) (scaled)\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter de unids escalados vs datos de entrenamiento escalados\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_final_train_scaled[:, 0],\n",
    "    y=X_final_train_scaled[:, 1],\n",
    "    color=\"skyblue\",  # \"skyblue\" \"cornflowerblue\"\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    label='Astro'\n",
    ")\n",
    "sns.scatterplot(\n",
    "    x=X_unids_scaled[:, 0],\n",
    "    y=X_unids_scaled[:, 1],\n",
    "    color=\"gold\",  # \"skyblue\" \"cornflowerblue\"\n",
    "    edgecolor='k',\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    label='Unids'\n",
    ")\n",
    "plt.title(\"Scaled UNIDS Data vs Train Data: E_peak vs Beta\")\n",
    "plt.xlabel(\"Log(E_peak) (scaled)\")\n",
    "plt.ylabel(\"Log(beta) (scaled)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir con el modelo final\n",
    "unids_preds = final_model.predict(X_unids_scaled)\n",
    "unids_labels = np.where(unids_preds == 1, 0, 1)  # 1â†’normal, -1â†’outlier\n",
    "\n",
    "n_outliers = np.sum(unids_preds == -1)\n",
    "n_normals = np.sum(unids_preds == 1)\n",
    "\n",
    "print(f\"Predicted ASTRO-like: {n_normals}\")\n",
    "print(f\"Predicted not ASTRO-like (anomalies): {n_outliers}\")\n",
    "\n",
    "# AÃ±adir etiquetas al DataFrame\n",
    "df_unids['pred_labels'] = unids_labels\n",
    "\n",
    "# Filtrar los datos no clasificados\n",
    "df_unids['pred_labels'] = df_unids['pred_labels'].astype(int)\n",
    "df_unids['pred_labels'] = df_unids['pred_labels'].replace({0: 'Normal', 1: 'AnomalÃ­a'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers = X_unids_scaled[unids_preds == 1]\n",
    "outliers = X_unids_scaled[unids_preds == -1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(inliers[:, 0], inliers[:, 1], c='gold', edgecolors='k', label='Inlier (likely astro)', alpha=0.6)\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], c='red', edgecolors='k', label='Outlier (potentially new)', alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Log(E_peak) (scaled)\")\n",
    "plt.ylabel(\"Log(beta) (scaled)\")\n",
    "\n",
    "plt.title(\"UNIDs: Inlier vs Outlier (One-Class SVM)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meshgrid (with correct feature order: [beta, E_peak])\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5, 300),  # E_peak\n",
    "    np.linspace(X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5, 300)   # beta\n",
    ")\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]  # correcto orden\n",
    "\n",
    "Z = final_model.decision_function(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu_r, alpha=0.8)\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='crimson', linestyles='--')\n",
    "\n",
    "preds_unids = final_model.predict(X_unids_scaled)\n",
    "inliers = X_unids_scaled[preds_unids == 1]\n",
    "outliers = X_unids_scaled[preds_unids == -1]\n",
    "\n",
    "plt.scatter(inliers[:, 0], inliers[:, 1], c='gold', edgecolors='k', s=40, label='UNIDs Inlier')\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], c='red', edgecolors='k', s=40, label='UNIDs Outlier')\n",
    "\n",
    "# Highlight most anomalous UNIDs using their lowest decision score:\n",
    "decision_scores = final_model.decision_function(X_unids_scaled)\n",
    "# Optional: mark top 5 most anomalous\n",
    "top_anomalies = X_unids_scaled[np.argsort(decision_scores)[:5]]\n",
    "plt.scatter(top_anomalies[:, 1], top_anomalies[:, 0], c='yellow', edgecolors='black', s=80, label='Top Outliers', marker='*')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Log(E_peak) (scaled)\")\n",
    "plt.ylabel(\"Log(beta) (scaled)\")\n",
    "plt.title(\"Predictions on UNIDs with One-Class SVM Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Scoring - UNIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Evaluar cada muestra no identificada con el modelo entrenado\n",
    "# decision_function devuelve un valor continuo: cuanto mÃ¡s alto, mÃ¡s normal (positivo); cuanto mÃ¡s bajo, mÃ¡s anÃ³malo (negativo)\n",
    "decision_scores = final_model.decision_function(X_unids_scaled)  # X_unids_scaled = muestras no etiquetadas, ya escaladas\n",
    "\n",
    "# Paso 2: Predecir si cada punto es inlier (1) o outlier (-1)\n",
    "unids_preds = final_model.predict(X_unids_scaled)  # 1 = normal, -1 = anomalÃ­a\n",
    "\n",
    "# Paso 3: Agregar los resultados al DataFrame original\n",
    "df_unids[\"svm_score\"] = decision_scores       # Puntaje bruto del modelo (positivo = normal)\n",
    "df_unids[\"prediction\"] = unids_preds          # ClasificaciÃ³n binaria: inlier o outlier\n",
    "\n",
    "# Paso 4: Invertimos el score para que valores mÃ¡s altos signifiquen mÃ¡s anomalÃ­a\n",
    "# Esto es Ãºtil para poder escalar la puntuaciÃ³n y ordenar mÃ¡s intuitivamente\n",
    "anom_scores = -decision_scores  # Ahora, valores grandes = mÃ¡s anÃ³malos\n",
    "\n",
    "# Paso 5: Escalamos los scores de anomalÃ­a al rango [0, 100] para facilitar su interpretaciÃ³n\n",
    "anom_percent = MinMaxScaler(feature_range=(0, 100)).fit_transform(anom_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Guardamos el puntaje invertido y su porcentaje normalizado en el DataFrame\n",
    "df_unids[\"Anomaly_Score\"] = anom_scores\n",
    "df_unids[\"Anomaly_Rank(%)\"] = anom_percent  # 100 = mÃ¡s anÃ³malo, 0 = mÃ¡s normal\n",
    "\n",
    "# Paso 6: Filtramos solo los puntos predichos como anÃ³malos y los ordenamos por su score mÃ¡s alto\n",
    "top_anomalies = df_unids[df_unids[\"prediction\"] == -1] \\\n",
    "                    .sort_values(by=\"Anomaly_Rank(%)\", ascending=False) \\\n",
    "                    .head(10)\n",
    "\n",
    "# Guardamos los Ã­ndices (puede ser Ãºtil si queremos recuperar sus posiciones originales)\n",
    "most_anomalous_idx = top_anomalies.index\n",
    "\n",
    "# Paso 7: Guardamos los 10 mÃ¡s anÃ³malos en un archivo\n",
    "# top_anomalies.to_csv(\"../../data/processed/unids_most_anomalous.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Paso 8: Mostramos en pantalla un resumen de las anomalÃ­as detectadas\n",
    "print(\"Top Most Anomalous UNID Sources (4F One-Class SVM):\")\n",
    "display(top_anomalies[['Log(E_peak)', 'Log(beta)', 'number', 'svm_score', 'Anomaly_Score', 'Anomaly_Rank(%)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort anomaly scores and grab top N labels\n",
    "N = 10\n",
    "sorted_idx = np.argsort(-anom_percent)  # high anomaly % = more anomalous\n",
    "top_N_idx = sorted_idx[:N]\n",
    "\n",
    "top_labels = df_unids.iloc[top_N_idx]['number'].astype(str).values\n",
    "top_scores = anom_percent[top_N_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(anom_percent, bins=50, color='blue', edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Anomaly Percentage Distribution (UNID Sources)\")\n",
    "plt.xlabel(\"Anomaly % (higher = more anomalous)\")\n",
    "plt.ylabel(\"Number of Sources\")\n",
    "\n",
    "for i in range(N):\n",
    "    x = top_scores[i]\n",
    "    label = top_labels[i]\n",
    "    plt.axvline(x, color='crimson', linestyle='--', alpha=0.8)\n",
    "    plt.text(x + 0.5, 3 + (i % 2) * 2, f\"ID {label}\", rotation=90, color='crimson', ha='left', fontsize=9)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Inliers and outliers from scaled data\n",
    "inliers = X_unids_scaled[unids_preds == 1]\n",
    "outliers = X_unids_scaled[unids_preds == -1]\n",
    "\n",
    "# Plot inliers (gold)\n",
    "plt.scatter(inliers[:, 0], inliers[:, 1], c='gold', edgecolors='k', label='Inlier (astro-like)', alpha=0.6)\n",
    "\n",
    "# Plot outliers (red)\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], c='red', edgecolors='k', label='Outlier (anomalous)', alpha=0.6)\n",
    "\n",
    "# Annotate top 10 anomalies by ID\n",
    "for idx in most_anomalous_idx:\n",
    "    x = X_unids_scaled[idx, 0]  # E_peak (scaled)\n",
    "    y = X_unids_scaled[idx, 1]  # beta (scaled)\n",
    "    label = int(df_unids.loc[idx, 'number'])\n",
    "    plt.text(x + 0.1, y, str(label), color='black', fontsize=9)\n",
    "\n",
    "# Axis labels and styling\n",
    "plt.xlabel(\"E_peak (scaled)\")\n",
    "plt.ylabel(\"beta (scaled)\")\n",
    "plt.title(\"UNID Sources (2F) â€“ Inliers vs Anomalies with ID Labels\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../../outputs/figures/scaled/2F_UNIDs_OneClassSVM_2D_scaled.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Original/log-transformed values for 3 relevant features\n",
    "x_vals = df_unids['Log(E_peak)'].values\n",
    "y_vals = df_unids['Log(beta)'].values\n",
    "z_vals = df_unids['Log(sigma_det)'].values  # or 'beta_Rel' for 3F.2\n",
    "\n",
    "# Use model predictions for coloring\n",
    "inlier_idx = df_unids['prediction'] == 1\n",
    "outlier_idx = df_unids['prediction'] == -1\n",
    "\n",
    "# Inliers\n",
    "ax.scatter(\n",
    "    x_vals[inlier_idx], y_vals[inlier_idx], z_vals[inlier_idx],\n",
    "    c='#1f77b4', edgecolor='k', s=30, label='Inlier (astro-like)', alpha=0.5\n",
    ")\n",
    "\n",
    "# Outliers\n",
    "ax.scatter(\n",
    "    x_vals[outlier_idx], y_vals[outlier_idx], z_vals[outlier_idx],\n",
    "    c='#d62728', marker='^', edgecolor='k', s=50, label='Outlier (anomaly)', alpha=0.9\n",
    ")\n",
    "\n",
    "# Annotate top anomalies\n",
    "top_anomalies = df_unids[df_unids['prediction'] == -1].sort_values('Anomaly_Rank(%)', ascending=False).head(10)\n",
    "for idx in top_anomalies.index:\n",
    "    ax.scatter(\n",
    "        x_vals[idx], y_vals[idx], z_vals[idx],\n",
    "        facecolors='none', edgecolors='black', linewidths=2, s=100\n",
    "    )\n",
    "    ax.text(\n",
    "        x_vals[idx], y_vals[idx], z_vals[idx] + 0.05,\n",
    "        str(int(df_unids.loc[idx, 'number'])),\n",
    "        color='black', fontsize=9\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('E_peak')\n",
    "ax.set_ylabel('beta')\n",
    "ax.set_zlabel('sigma_det')  # or 'beta_Rel'\n",
    "ax.set_title(\"2F UNID Sources â€“ Anomalies in Original Feature Space\")\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "ax.tick_params(colors='#333333')\n",
    "ax.xaxis.label.set_color('#333333')\n",
    "ax.yaxis.label.set_color('#333333')\n",
    "ax.zaxis.label.set_color('#333333')\n",
    "ax.title.set_color('#111111')\n",
    "ax.grid(color='#aaaaaa', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.savefig(\"../../outputs/figures/2F_UNIDs_OneClassSVM_og.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparar con UNIDs ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process supervised model predictions\n",
    "print(\"Loading supervised model predictions...\")\n",
    "\n",
    "# Load predictions data\n",
    "unids_DM_std_proba_repeated_kfold = np.genfromtxt('../../ANN_original/unids_DM_std_proba_check_repeated_kfold_2F_21.txt', dtype='str') \n",
    "unids_DM_std_proba_data_repeated_kfold = np.asarray(unids_DM_std_proba_repeated_kfold[1::], dtype=float)\n",
    "\n",
    "print(f\"Raw predictions shape: {unids_DM_std_proba_data_repeated_kfold.shape}\")\n",
    "\n",
    "# Get dimensions\n",
    "# Load unID source data\n",
    "unids_3F = np.genfromtxt('../../ANN_original/unids_3F_beta_err_names.txt',dtype='str') \n",
    "unids_3F_data = np.asarray(unids_3F[1::,:],dtype=float)\n",
    "unids_log=np.log10(unids_3F_data[:,[0,1]])\n",
    "N_unids = unids_log.shape[0]\n",
    "# N_sample = unids_DM_std_proba_data_repeated_kfold.shape[1] - 1  # CV folds\n",
    "N_sample = 10\n",
    "\n",
    "print(f\"Number of sources: {N_unids}\")\n",
    "print(f\"Number of CV folds: {N_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract indices and reshape probabilities\n",
    "unids_number = unids_DM_std_proba_data_repeated_kfold[0:N_unids, 0]\n",
    "prob_values = unids_DM_std_proba_data_repeated_kfold[:, 1].reshape(N_unids, N_sample)\n",
    "\n",
    "# Create probability matrix\n",
    "unids_DM_std_proba_N_sample_repeated_kfold = np.zeros((N_unids, N_sample + 1))\n",
    "unids_DM_std_proba_N_sample_repeated_kfold[:, 0] = unids_number[:].astype(int)\n",
    "unids_DM_std_proba_N_sample_repeated_kfold[:, 1:(N_sample + 1)] = prob_values\n",
    "\n",
    "print(f\"Probability matrix shape: {unids_DM_std_proba_N_sample_repeated_kfold.shape}\")\n",
    "print(\"Sample of probability matrix:\")\n",
    "print(unids_DM_std_proba_N_sample_repeated_kfold[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probability data (excluding index column)\n",
    "prob_data = unids_DM_std_proba_N_sample_repeated_kfold[:, 1:(N_sample + 1)]\n",
    "\n",
    "# Calculate statistics across CV folds\n",
    "unids_mean = np.mean(prob_data, axis=1)\n",
    "unids_std = np.std(prob_data, axis=1)\n",
    "unids_median = np.median(prob_data, axis=1)\n",
    "unids_min = np.min(prob_data, axis=1)\n",
    "unids_max = np.max(prob_data, axis=1)\n",
    "unids_q25 = np.percentile(prob_data, 25, axis=1)\n",
    "unids_q75 = np.percentile(prob_data, 75, axis=1)\n",
    "\n",
    "print(\"Supervised model statistics calculated:\")\n",
    "print(f\"  Mean probability range: {unids_mean.min():.4f} - {unids_mean.max():.4f}\")\n",
    "print(f\"  Standard deviation range: {unids_std.min():.4f} - {unids_std.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create supervised model DataFrame\n",
    "supervised_df = pd.DataFrame({\n",
    "    'source_id': unids_number.astype(int),\n",
    "    'supervised_mean_prob': unids_mean,\n",
    "    'supervised_std_prob': unids_std,\n",
    "    'supervised_median_prob': unids_median,\n",
    "    'supervised_min_prob': unids_min,\n",
    "    'supervised_max_prob': unids_max,\n",
    "    'supervised_q25': unids_q25,\n",
    "    'supervised_q75': unids_q75,\n",
    "    'supervised_cv_range': unids_max - unids_min,\n",
    "    'E_peak': unids_3F_data[:, 0],\n",
    "    'Beta': unids_3F_data[:, 1],\n",
    "    'log_E_peak': unids_log[:, 0],\n",
    "    'log_Beta': unids_log[:, 1]\n",
    "})\n",
    "\n",
    "print(f\"Supervised DataFrame created with {len(supervised_df)} sources\")\n",
    "print(\"\\nSupervised model top 10 candidates:\")\n",
    "display(supervised_df.nlargest(10, 'supervised_mean_prob')[['source_id', 'supervised_mean_prob', 'supervised_std_prob', 'E_peak', 'Beta']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with OCSVM results\n",
    "print(\"Merging supervised and OCSVM results...\")\n",
    "\n",
    "# Check available columns in OCSVM DataFrame\n",
    "print(\"OCSVM DataFrame columns:\", df_unids.columns.tolist())\n",
    "\n",
    "# Merge datasets - adjust merge column as needed\n",
    "if 'number' in df_unids.columns:\n",
    "    combined_df = pd.merge(supervised_df, df_unids, left_on='source_id', right_on='number', how='inner')\n",
    "elif 'source_id' in df_unids.columns:\n",
    "    combined_df = pd.merge(supervised_df, df_unids, on='source_id', how='inner')\n",
    "else:\n",
    "    # If no common key, merge by index (assuming same order)\n",
    "    print(\"Warning: No common identifier found, merging by index\")\n",
    "    combined_df = pd.concat([supervised_df.reset_index(drop=True), \n",
    "                           df_unids.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "print(f\"Combined dataset columns: {combined_df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model correlations\n",
    "print(\"Calculating model correlations...\")\n",
    "\n",
    "# Check if required columns exist\n",
    "required_cols = ['supervised_mean_prob', 'Anomaly_Rank(%)']\n",
    "missing_cols = [col for col in required_cols if col not in combined_df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Error: Missing columns {missing_cols}\")\n",
    "    print(\"Available columns:\", combined_df.columns.tolist())\n",
    "else:\n",
    "    # Calculate correlations\n",
    "    corr_pearson, p_pearson = pearsonr(combined_df['supervised_mean_prob'], \n",
    "                                      combined_df['Anomaly_Rank(%)'])\n",
    "    corr_spearman, p_spearman = spearmanr(combined_df['supervised_mean_prob'], \n",
    "                                         combined_df['Anomaly_Rank(%)'])\n",
    "    \n",
    "    print(f\"\\\\nMODEL CORRELATION RESULTS:\")\n",
    "    print(f\"  Pearson correlation: {corr_pearson:.4f} (p-value: {p_pearson:.4e})\")\n",
    "    print(f\"  Spearman correlation: {corr_spearman:.4f} (p-value: {p_spearman:.4e})\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if abs(corr_spearman) > 0.7:\n",
    "        print(\"  â†’ Strong correlation between models\")\n",
    "    elif abs(corr_spearman) > 0.4:\n",
    "        print(\"  â†’ Moderate correlation between models\")\n",
    "    else:\n",
    "        print(\"  â†’ Weak correlation between models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top candidates overlap\n",
    "top_n = 20\n",
    "print(f\"\\\\nAnalyzing top {top_n} candidates from each model...\")\n",
    "\n",
    "# Get top candidates from each model\n",
    "top_supervised = combined_df.nlargest(top_n, 'supervised_mean_prob')\n",
    "top_anomalous = combined_df.nlargest(top_n, 'Anomaly_Rank(%)')\n",
    "\n",
    "# Find overlap\n",
    "supervised_ids = set(top_supervised.index)\n",
    "anomaly_ids = set(top_anomalous.index)\n",
    "overlap_ids = supervised_ids.intersection(anomaly_ids)\n",
    "\n",
    "# Calculate overlap metrics\n",
    "total_overlap = len(overlap_ids)\n",
    "overlap_percentage = total_overlap / top_n * 100\n",
    "jaccard_index = total_overlap / len(supervised_ids.union(anomaly_ids))\n",
    "\n",
    "print(f\"\\\\nTOP {top_n} CANDIDATES OVERLAP:\")\n",
    "print(f\"  Sources in both top {top_n}: {total_overlap}\")\n",
    "print(f\"  Overlap percentage: {overlap_percentage:.1f}%\")\n",
    "print(f\"  Jaccard similarity index: {jaccard_index:.4f}\")\n",
    "\n",
    "# Show overlapping candidates\n",
    "if total_overlap > 0:\n",
    "    overlap_sources = combined_df.loc[list(overlap_ids)]\n",
    "    print(f\"\\\\nCONSENSUS CANDIDATES (in both top {top_n}):\")\n",
    "    display_cols = ['source_id', 'supervised_mean_prob', 'Anomaly_Rank(%)', 'F_peak', 'Beta']\n",
    "    available_cols = [col for col in display_cols if col in overlap_sources.columns]\n",
    "    display(overlap_sources[available_cols].round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create consensus ranking\n",
    "print(\"Creating consensus ranking...\")\n",
    "\n",
    "# Normalize scores to [0, 1] range\n",
    "supervised_norm = MinMaxScaler().fit_transform(\n",
    "    combined_df['supervised_mean_prob'].values.reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "ocsvm_norm = combined_df['Anomaly_Rank(%)'] / 100\n",
    "\n",
    "# Create weighted consensus score (adjust weights as needed)\n",
    "weights = {'supervised': 0.6, 'ocsvm': 0.4}\n",
    "consensus_score = (weights['supervised'] * supervised_norm + \n",
    "                  weights['ocsvm'] * ocsvm_norm)\n",
    "\n",
    "# Add consensus columns to DataFrame\n",
    "combined_df['supervised_norm'] = supervised_norm\n",
    "combined_df['ocsvm_norm'] = ocsvm_norm\n",
    "combined_df['consensus_score'] = consensus_score\n",
    "combined_df['consensus_rank'] = combined_df['consensus_score'].rank(ascending=False, method='dense')\n",
    "\n",
    "print(f\"Consensus ranking created with weights: {weights}\")\n",
    "print(f\"Consensus score range: {consensus_score.min():.4f} - {consensus_score.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top consensus candidates\n",
    "print(\"\\\\nTOP 15 CONSENSUS DARK MATTER CANDIDATES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by consensus score and display\n",
    "consensus_top = combined_df.sort_values('consensus_score', ascending=False)\n",
    "\n",
    "display_cols = ['source_id', 'consensus_rank', 'consensus_score', \n",
    "               'supervised_mean_prob', 'supervised_std_prob', \n",
    "               'Anomaly_Rank(%)', 'E_peak', 'Beta']\n",
    "available_display_cols = [col for col in display_cols if col in consensus_top.columns]\n",
    "\n",
    "display(consensus_top[available_display_cols].head(15).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "print(\"Creating comparison visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Scatter plot: Supervised prob vs Anomaly rank\n",
    "axes[0, 0].scatter(combined_df['supervised_mean_prob'], \n",
    "                  combined_df['Anomaly_Rank(%)'], \n",
    "                  alpha=0.6, s=40, c='blue')\n",
    "axes[0, 0].set_xlabel('Supervised Model Probability')\n",
    "axes[0, 0].set_ylabel('OCSVM Anomaly Rank (%)')\n",
    "axes[0, 0].set_title('Supervised vs OCSVM Scores')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation text\n",
    "axes[0, 0].text(0.05, 0.95, \n",
    "               f\"Pearson: {corr_pearson:.3f}\\\\nSpearman: {corr_spearman:.3f}\",\n",
    "               transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 2. Distribution comparison\n",
    "axes[0, 1].hist(combined_df['supervised_mean_prob'], bins=30, alpha=0.7, \n",
    "               label='Supervised Prob', density=True, color='blue')\n",
    "axes[0, 1].hist(combined_df['Anomaly_Rank(%)']/100, bins=30, alpha=0.7, \n",
    "               label='OCSVM Anomaly Rank', density=True, color='red')\n",
    "axes[0, 1].set_xlabel('Score')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Score Distributions')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Consensus score distribution\n",
    "axes[0, 2].hist(combined_df['consensus_score'], bins=30, alpha=0.7, \n",
    "               color='green', edgecolor='black')\n",
    "axes[0, 2].set_xlabel('Consensus Score')\n",
    "axes[0, 2].set_ylabel('Count')\n",
    "axes[0, 2].set_title('Consensus Score Distribution')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature space with supervised model coloring\n",
    "scatter1 = axes[1, 0].scatter(combined_df['E_peak'], combined_df['Beta'], \n",
    "                            c=combined_df['supervised_mean_prob'], \n",
    "                            cmap='viridis', alpha=0.7, s=40)\n",
    "axes[1, 0].set_xlabel('E_peak')\n",
    "axes[1, 0].set_ylabel('Beta')\n",
    "axes[1, 0].set_title('Feature Space (Supervised Prob)')\n",
    "axes[1, 0].set_xscale('log')\n",
    "axes[1, 0].set_yscale('log')\n",
    "plt.colorbar(scatter1, ax=axes[1, 0])\n",
    "\n",
    "# 5. Feature space with OCSVM colorin\n",
    "scatter2 = axes[1, 1].scatter(combined_df['E_peak'], combined_df['Beta'], \n",
    "                            c=combined_df['Anomaly_Rank(%)'], \n",
    "                            cmap='plasma', alpha=0.7, s=40)\n",
    "axes[1, 1].set_xlabel('E_peak')\n",
    "axes[1, 1].set_ylabel('Beta')\n",
    "axes[1, 1].set_title('Feature Space (OCSVM Anomaly)')\n",
    "axes[1, 1].set_xscale('log')\n",
    "axes[1, 1].set_yscale('log')\n",
    "plt.colorbar(scatter2, ax=axes[1, 1])\n",
    "\n",
    "# 6. Feature space with consensus coloring\n",
    "scatter3 = axes[1, 2].scatter(combined_df['E_peak'], combined_df['Beta'], \n",
    "                            c=combined_df['consensus_score'], \n",
    "                            cmap='coolwarm', alpha=0.7, s=40)\n",
    "axes[1, 2].set_xlabel('E_peak')\n",
    "axes[1, 2].set_ylabel('Beta')\n",
    "axes[1, 2].set_title('Feature Space (Consensus Score)')\n",
    "axes[1, 2].set_xscale('log')\n",
    "axes[1, 2].set_yscale('log')\n",
    "plt.colorbar(scatter3, ax=axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model agreement analysis\n",
    "print(\"\\\\nAnalyzing model agreement across score ranges...\")\n",
    "\n",
    "# Create score bins for agreement analysis\n",
    "supervised_bins = pd.cut(combined_df['supervised_mean_prob'], \n",
    "                        bins=5, labels=['Low', 'Med-Low', 'Med', 'Med-High', 'High'])\n",
    "anomaly_bins = pd.cut(combined_df['Anomaly_Rank(%)'], \n",
    "                     bins=5, labels=['Low', 'Med-Low', 'Med', 'Med-High', 'High'])\n",
    "\n",
    "# Create agreement matrix\n",
    "agreement_matrix = pd.crosstab(supervised_bins, anomaly_bins, margins=True)\n",
    "print(\"\\\\nModel Agreement Matrix:\")\n",
    "print(\"(Rows: Supervised Model, Columns: OCSVM)\")\n",
    "display(agreement_matrix)\n",
    "\n",
    "# Plot agreement heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(agreement_matrix.iloc[:-1, :-1], annot=True, fmt='d', \n",
    "            cmap='Blues', cbar_kws={'label': 'Number of Sources'})\n",
    "plt.xlabel('OCSVM Anomaly Level')\n",
    "plt.ylabel('Supervised Model Probability Level')\n",
    "plt.title('Model Agreement Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and final report\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"DARK MATTER CANDIDATE MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\\\nDATASET OVERVIEW:\")\n",
    "print(f\"  Total sources analyzed: {len(combined_df)}\")\n",
    "print(f\"  Supervised model probability range: {combined_df['supervised_mean_prob'].min():.4f} - {combined_df['supervised_mean_prob'].max():.4f}\")\n",
    "print(f\"  OCSVM anomaly rank range: {combined_df['Anomaly_Rank(%)'].min():.1f}% - {combined_df['Anomaly_Rank(%)'].max():.1f}%\")\n",
    "print(f\"  Consensus score range: {combined_df['consensus_score'].min():.4f} - {combined_df['consensus_score'].max():.4f}\")\n",
    "\n",
    "print(f\"\\\\nMODEL PERFORMANCE:\")\n",
    "print(f\"  Cross-validation folds used: {N_sample}\")\n",
    "print(f\"  Mean supervised model uncertainty: {combined_df['supervised_std_prob'].mean():.4f}\")\n",
    "print(f\"  Sources with high confidence (>90%): {sum(combined_df['supervised_mean_prob'] > 0.9)}\")\n",
    "print(f\"  Sources with low uncertainty (<10%): {sum(combined_df['supervised_std_prob'] < 0.1)}\")\n",
    "\n",
    "print(f\"\\\\nMODEL AGREEMENT:\")\n",
    "print(f\"  Pearson correlation: {corr_pearson:.4f}\")\n",
    "print(f\"  Spearman correlation: {corr_spearman:.4f}\")\n",
    "print(f\"  Top {top_n} overlap: {total_overlap} sources ({overlap_percentage:.1f}%)\")\n",
    "print(f\"  Jaccard similarity: {jaccard_index:.4f}\")\n",
    "\n",
    "# Identify most promising candidates\n",
    "high_consensus = combined_df[combined_df['consensus_score'] > 0.8]\n",
    "print(f\"\\\\nHIGH-CONFIDENCE CONSENSUS CANDIDATES:\")\n",
    "print(f\"  Sources with consensus score >0.8: {len(high_consensus)}\")\n",
    "\n",
    "if len(high_consensus) > 0:\n",
    "    print(\"\\\\nTop 5 highest consensus candidates:\")\n",
    "    top_consensus_cols = ['source_id', 'consensus_score', 'supervised_mean_prob', 'Anomaly_Rank(%)']\n",
    "    available_consensus_cols = [col for col in top_consensus_cols if col in high_consensus.columns]\n",
    "    display(high_consensus[available_consensus_cols].head().round(4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv DarkMatter_TFG)",
   "language": "python",
   "name": "venv-tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
