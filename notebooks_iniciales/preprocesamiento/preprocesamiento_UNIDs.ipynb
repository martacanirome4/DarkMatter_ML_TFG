{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos – Fuentes UNID\n",
    "\n",
    "Este notebook tiene como objetivo transformar y preparar los datos de las fuentes no identificadas (UNIDs), contenidas en el archivo `unids_3F_beta_err_names.txt`, para su uso en modelos de detección de anomalías como One-Class SVM.\n",
    "\n",
    "A diferencia de las fuentes ASTRO, las variables de los UNIDs no están logarítmicamente transformadas. Dado que el modelo se entrena con datos ASTRO ya transformados en `log10`, es necesario aplicar la misma transformación a los UNIDs para garantizar la coherencia de escala.\n",
    "\n",
    "Las variables transformadas serán:\n",
    "- `E_peak`\n",
    "- `beta`\n",
    "- `sigma_det`\n",
    "- `beta_Rel`\n",
    "\n",
    "Los datos procesados se guardarán tanto en formato `.csv` como `.txt` para facilitar su reutilización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación Logarítmica y Verificación de Compatibilidad - UNIDs Fermi-LAT\n",
    "# TFG: Utilización de técnicas de ML a datos del satélite Fermi-Lat para detección de posibles fuentes de materia oscura\n",
    "# Objetivo: Transformar datos UNIDs a escala logarítmica para compatibilidad con modelo OneClassSVM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unids_features = ['E_peak', 'beta', 'sigma_det', 'beta_Rel', 'number']\n",
    "unids_original = pd.read_csv('../../data/raw/unids_3F_beta_err_names.txt', \n",
    "                            sep=\"\\s+\", names=unids_features, engine='python', skiprows=1)\n",
    "\n",
    "print(f\"UNIDs cargados: {unids_original.shape}\")\n",
    "\n",
    "print(f\"Dimensiones del dataset UNIDs: {unids_original.shape}\")\n",
    "print(f\"Número total de fuentes no identificadas: {len(unids_original)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver muestra\n",
    "unids_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unids_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de entrenamiento ASTRO (ya en escala logarítmica)\n",
    "print(\"Cargando datos de entrenamiento ASTRO...\")\n",
    "training_features = ['E_peak', 'beta', 'sigma', 'beta_Rel', 'astro_DM']\n",
    "training_full = pd.read_csv('../../data/raw/XY_bal_log_Rel.txt', \n",
    "                           sep=\"\\s+\", names=training_features, engine='python', skiprows=1)\n",
    "\n",
    "# Filtrar solo datos ASTRO (clase 0.0) - datos usados para entrenar OneClassSVM\n",
    "astro_df = training_full[training_full['astro_DM'] == 0.0].copy()\n",
    "\n",
    "print(f\"- Dataset completo cargado: {training_full.shape}\")\n",
    "print(f\"- Datos ASTRO extraídos: {astro_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astro_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUNIDs (escala lineal):\")\n",
    "unids_original[['E_peak', 'beta', 'sigma_det', 'beta_Rel']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDatos ASTRO - Entrenamiento OneClassSVM (escala logarítmica):\")\n",
    "astro_df[['E_peak', 'beta', 'sigma', 'beta_Rel']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['E_peak', 'beta', 'sigma_det', 'beta_Rel']\n",
    "\n",
    "print(\"Verificando valores ≤ 0 que causarían problemas con log₁₀:\")\n",
    "\n",
    "problematic_values = {}\n",
    "for col in feature_cols:\n",
    "    zero_count = (unids_original[col] == 0).sum()\n",
    "    negative_count = (unids_original[col] < 0).sum()\n",
    "    min_positive = unids_original[unids_original[col] > 0][col].min() if (unids_original[col] > 0).any() else None\n",
    "    \n",
    "    problematic_values[col] = {\n",
    "        'zeros': zero_count,\n",
    "        'negatives': negative_count,\n",
    "        'min_positive': min_positive,\n",
    "        'total_problematic': zero_count + negative_count\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  • Valores = 0: {zero_count}\")\n",
    "    print(f\"  • Valores < 0: {negative_count}\")\n",
    "    print(f\"  • Valores problemáticos totales: {zero_count + negative_count}\")\n",
    "    print(f\"  • Valor positivo mínimo: {min_positive}\")\n",
    "    \n",
    "    if zero_count + negative_count > 0:\n",
    "        print(f\" ATENCIÓN: {zero_count + negative_count} valores problemáticos\")\n",
    "        # Mostrar algunos ejemplos\n",
    "        problematic_indices = unids_original[unids_original[col] <= 0].index[:5]\n",
    "        if len(problematic_indices) > 0:\n",
    "            print(f\"  Primeros índices problemáticos: {problematic_indices.tolist()}\")\n",
    "    else:\n",
    "        print(f\"Sin valores problemáticos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir estrategia basada en valores problemáticos\n",
    "total_problematic = sum([v['total_problematic'] for v in problematic_values.values()])\n",
    "\n",
    "if total_problematic > 0:\n",
    "    print(f\"Se encontraron {total_problematic} valores problemáticos totales\")\n",
    "    print(\"\\nEstrategia adoptada:\")\n",
    "    print(\"1. Aplicar transformación log10 a valores positivos\")\n",
    "    print(\"2. Para valores ≤ 0: aplicar log10 (valor + epsilon) donde epsilon = 1e-10\")\n",
    "    print(\"3. Verificar que no se introduzcan artifacts\")\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    print(f\"\\nEpsilon utilizado: {epsilon}\")\n",
    "else:\n",
    "    print(\"No hay valores problemáticos\")\n",
    "    print(\"Estrategia: Aplicar log10 directamente\")\n",
    "    epsilon = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de las columnas de UNIDS a log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para transformación\n",
    "unids_transformed = unids_original.copy()\n",
    "\n",
    "print(\"Aplicando transformación log10...\")\n",
    "\n",
    "# Aplicar transformación a cada variable\n",
    "transformation_log = {}\n",
    "\n",
    "for col in feature_cols:\n",
    "    original_col = col\n",
    "    \n",
    "    if epsilon > 0:\n",
    "        # Aplicar epsilon para valores problemáticos\n",
    "        transformed_values = np.log10(unids_transformed[col] + epsilon)\n",
    "        transformation_log[col] = f\"log10({col} + {epsilon})\"\n",
    "    else:\n",
    "        # Aplicar log₁₀ directamente\n",
    "        transformed_values = np.log10(unids_transformed[col])\n",
    "        transformation_log[col] = f\"log10({col})\"\n",
    "    \n",
    "    unids_transformed[col] = transformed_values\n",
    "    print(f\"- {col}: {transformation_log[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nESTADÍSTICAS DESPUÉS DE TRANSFORMACIÓN:\")\n",
    "print(\"\\nUNIDs (escala logarítmica):\")\n",
    "print(unids_transformed[feature_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparando rangos entre UNIDs transformados y datos ASTRO (usados para entrenar OneClassSVM):\")\n",
    "\n",
    "compatibility_check = {}\n",
    "for i, col in enumerate(['E_peak', 'beta', 'sigma_det', 'beta_Rel']):\n",
    "    astro_col = 'sigma' if col == 'sigma_det' else col\n",
    "    \n",
    "    # Rangos\n",
    "    unid_min, unid_max = unids_transformed[col].min(), unids_transformed[col].max()\n",
    "    astro_min, astro_max = astro_df[astro_col].min(), astro_df[astro_col].max()\n",
    "    \n",
    "    # Verificar superposición\n",
    "    overlap_min = max(unid_min, astro_min)\n",
    "    overlap_max = min(unid_max, astro_max)\n",
    "    has_overlap = overlap_min <= overlap_max\n",
    "    \n",
    "    # Verificar si UNIDs están dentro del rango ASTRO\n",
    "    within_range = (unid_min >= astro_min) and (unid_max <= astro_max)\n",
    "    \n",
    "    # Calcular porcentaje de superposición\n",
    "    if has_overlap:\n",
    "        overlap_range = overlap_max - overlap_min\n",
    "        astro_range = astro_max - astro_min\n",
    "        overlap_percentage = (overlap_range / astro_range) * 100 if astro_range > 0 else 0\n",
    "    else:\n",
    "        overlap_percentage = 0\n",
    "    \n",
    "    compatibility_check[col] = {\n",
    "        'unid_range': (unid_min, unid_max),\n",
    "        'astro_range': (astro_min, astro_max),\n",
    "        'overlap': has_overlap,\n",
    "        'within_astro_range': within_range,\n",
    "        'overlap_range': (overlap_min, overlap_max) if has_overlap else None,\n",
    "        'overlap_percentage': overlap_percentage\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{col} ↔ ASTRO {astro_col}:\")\n",
    "    print(f\"  UNIDs transformados: [{unid_min:.3f}, {unid_max:.3f}]\")\n",
    "    print(f\"  Datos ASTRO:         [{astro_min:.3f}, {astro_max:.3f}]\")\n",
    "    print(f\"  Superposición: {'Sí' if has_overlap else 'No'}\")\n",
    "    print(f\"  UNIDs dentro rango ASTRO: {'Sí' if within_range else 'No'}\")\n",
    "    \n",
    "    if has_overlap:\n",
    "        print(f\"  Rango superposición: [{overlap_min:.3f}, {overlap_max:.3f}]\")\n",
    "        print(f\"  % de superposición: {overlap_percentage:.1f}%\")\n",
    "    \n",
    "    # Advertencias específicas\n",
    "    if not has_overlap:\n",
    "        print(f\" SIN SUPERPOSICIÓN: Modelo puede no funcionar bien\")\n",
    "    elif overlap_percentage < 50:\n",
    "        print(f\"SUPERPOSICIÓN BAJA: Verificar performance del modelo\")\n",
    "    elif within_range:\n",
    "        print(f\"COMPATIBILIDAD EXCELENTE: UNIDs dentro del espacio ASTRO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualización antes vs después\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    train_col = 'sigma' if col == 'sigma_det' else col\n",
    "    \n",
    "    # Distribución original UNIDs\n",
    "    axes[i, 0].hist(unids_original[col], bins=50, alpha=0.7, color='lightcoral', \n",
    "                   edgecolor='black', label='UNIDs Original')\n",
    "    axes[i, 0].set_title(f'{col} - Original (Escala Lineal)')\n",
    "    axes[i, 0].set_xlabel(col)\n",
    "    axes[i, 0].set_ylabel('Frecuencia')\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribución transformada UNIDs\n",
    "    axes[i, 1].hist(unids_transformed[col], bins=50, alpha=0.7, color='lightgreen', \n",
    "                   edgecolor='black', label='UNIDs Transformado')\n",
    "    axes[i, 1].set_title(f'{col} - Transformado (Log10)')\n",
    "    axes[i, 1].set_xlabel(f'Log10({col})')\n",
    "    axes[i, 1].set_ylabel('Frecuencia')\n",
    "    axes[i, 1].legend()\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Comparación superpuesta - UNIDs vs ASTRO solamente\n",
    "    axes[i, 2].hist(unids_transformed[col], bins=40, alpha=0.6, color='lightgreen', \n",
    "                   density=True, label='UNIDs Transform.', edgecolor='black')\n",
    "    axes[i, 2].hist(astro_df[astro_col], bins=40, alpha=0.6, color='skyblue', \n",
    "                   density=True, label='ASTRO (Entrenamiento)', edgecolor='black')\n",
    "    axes[i, 2].set_title(f'{col} - UNIDs vs ASTRO (OneClassSVM)')\n",
    "    axes[i, 2].set_xlabel(f'Log10({col})')\n",
    "    axes[i, 2].set_ylabel('Densidad')\n",
    "    axes[i, 2].legend()\n",
    "    axes[i, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Transformación: Original → Log10 → Comparación vs ASTRO (OneClassSVM)', \n",
    "             fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualización de distribuciones\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    astro_col = 'sigma' if col == 'sigma_det' else col\n",
    "    \n",
    "    # ANTES: UNIDs original vs ASTRO (escalas diferentes)\n",
    "    axes[i, 0].hist(unids_original[col], bins=50, alpha=0.7, color='lightcoral', \n",
    "                   label=f'UNIDs Original', density=True)\n",
    "    axes[i, 0].set_title(f'{col} - ANTES: UNIDs Original (Escala Lineal)')\n",
    "    axes[i, 0].set_xlabel(col)\n",
    "    axes[i, 0].set_ylabel('Densidad')\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Estadísticas en el gráfico\n",
    "    mean_orig = unids_original[col].mean()\n",
    "    std_orig = unids_original[col].std()\n",
    "    axes[i, 0].text(0.7, 0.8, f'μ = {mean_orig:.2f}\\nσ = {std_orig:.2f}', \n",
    "                   transform=axes[i, 0].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # DESPUÉS: UNIDs transformado\n",
    "    axes[i, 1].hist(unids_transformed[col], bins=50, alpha=0.7, color='lightgreen', \n",
    "                   label=f'UNIDs Transformado', density=True)\n",
    "    axes[i, 1].set_title(f'{col} - DESPUÉS: UNIDs Log10')\n",
    "    axes[i, 1].set_xlabel(f'Log10({col})')\n",
    "    axes[i, 1].set_ylabel('Densidad')\n",
    "    axes[i, 1].legend()\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Estadísticas transformadas\n",
    "    mean_trans = unids_transformed[col].mean()\n",
    "    std_trans = unids_transformed[col].std()\n",
    "    axes[i, 1].text(0.7, 0.8, f'μ = {mean_trans:.2f}\\nσ = {std_trans:.2f}', \n",
    "                   transform=axes[i, 1].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # COMPARACIÓN: UNIDs transformado vs ASTRO (misma escala)\n",
    "    axes[i, 2].hist(unids_transformed[col], bins=40, alpha=0.6, color='lightgreen', \n",
    "                   density=True, label='UNIDs Log10')\n",
    "    axes[i, 2].hist(astro_df[astro_col], bins=40, alpha=0.6, color='skyblue', \n",
    "                   density=True, label='ASTRO')\n",
    "    axes[i, 2].set_title(f'{col} - COMPARACIÓN: Misma Escala Log10')\n",
    "    axes[i, 2].set_xlabel(f'Log10({col})')\n",
    "    axes[i, 2].set_ylabel('Densidad')\n",
    "    axes[i, 2].legend()\n",
    "    axes[i, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mostrar superposición\n",
    "    unid_min, unid_max = unids_transformed[col].min(), unids_transformed[col].max()\n",
    "    astro_min, astro_max = astro_df[astro_col].min(), astro_df[astro_col].max()\n",
    "    overlap_min = max(unid_min, astro_min)\n",
    "    overlap_max = min(unid_max, astro_max)\n",
    "    \n",
    "    if overlap_min <= overlap_max:\n",
    "        axes[i, 2].axvspan(overlap_min, overlap_max, alpha=0.2, color='purple', label='Superposición')\n",
    "        axes[i, 2].legend()\n",
    "\n",
    "plt.suptitle('Verificación Visual: Transformación Logarítmica UNIDs', fontsize=18, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pares de variables más importantes\n",
    "important_pairs = [\n",
    "    ('E_peak', 'beta'),\n",
    "    ('E_peak', 'sigma_det'),\n",
    "    ('beta', 'sigma_det'),\n",
    "    ('beta', 'beta_Rel')\n",
    "]\n",
    "\n",
    "# ANTES DE LA TRANSFORMACIÓN\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (x_var, y_var) in enumerate(important_pairs):\n",
    "    astro_x = 'sigma' if x_var == 'sigma_det' else x_var\n",
    "    astro_y = 'sigma' if y_var == 'sigma_det' else y_var\n",
    "    \n",
    "    # UNIDs original (escala lineal)\n",
    "    axes[i].scatter(unids_original[x_var], unids_original[y_var], \n",
    "                   alpha=0.6, c='red', s=20, label='UNIDs Original', marker='o')\n",
    "    \n",
    "    # ASTRO (ya en log) - escalar para visualización\n",
    "    # Nota: No se puede comparar directamente por diferente escala\n",
    "    axes[i].set_title(f'ANTES: {x_var} vs {y_var}\\n(UNIDs lineal - No comparable con ASTRO)')\n",
    "    axes[i].set_xlabel(x_var)\n",
    "    axes[i].set_ylabel(y_var)\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Scatter Plots ANTES de Transformación (Escalas Incompatibles)', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESPUÉS DE LA TRANSFORMACIÓN\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (x_var, y_var) in enumerate(important_pairs):\n",
    "    astro_x = 'sigma' if x_var == 'sigma_det' else x_var\n",
    "    astro_y = 'sigma' if y_var == 'sigma_det' else y_var\n",
    "    \n",
    "    # UNIDs transformado\n",
    "    axes[i].scatter(unids_transformed[x_var], unids_transformed[y_var], \n",
    "                   alpha=0.6, c='green', s=25, label='UNIDs Log₁₀', marker='o')\n",
    "    \n",
    "    # ASTRO (referencia)\n",
    "    axes[i].scatter(astro_df[astro_x], astro_df[astro_y], \n",
    "                   alpha=0.4, c='blue', s=15, label='ASTRO', marker='^')\n",
    "    \n",
    "    axes[i].set_title(f'DESPUÉS: Log10({x_var}) vs Log10({y_var})\\n(Escalas Compatibles)')\n",
    "    axes[i].set_xlabel(f'Log10({x_var})')\n",
    "    axes[i].set_ylabel(f'Log10({y_var})')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Scatter Plots DESPUÉS de Transformación (Escalas Compatibles)', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un resumen estadístico visual\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Resumen de medias antes/después\n",
    "variables = ['E_peak', 'beta', 'sigma_det', 'beta_Rel']\n",
    "unids_orig_means = [unids_original[col].mean() for col in variables]\n",
    "unids_trans_means = [unids_transformed[col].mean() for col in variables]\n",
    "astro_means = [astro_df['sigma' if col == 'sigma_det' else col].mean() for col in variables]\n",
    "\n",
    "x_pos = np.arange(len(variables))\n",
    "\n",
    "ax1.bar(x_pos - 0.25, unids_orig_means, 0.25, label='UNIDs Original', color='lightcoral', alpha=0.7)\n",
    "ax1.bar(x_pos, unids_trans_means, 0.25, label='UNIDs Log10', color='lightgreen', alpha=0.7)\n",
    "ax1.bar(x_pos + 0.25, astro_means, 0.25, label='ASTRO', color='skyblue', alpha=0.7)\n",
    "ax1.set_title('Comparación de Medias')\n",
    "ax1.set_ylabel('Valor Medio')\n",
    "ax1.set_xlabel('Variables')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(variables, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Resumen de desviaciones estándar\n",
    "unids_orig_stds = [unids_original[col].std() for col in variables]\n",
    "unids_trans_stds = [unids_transformed[col].std() for col in variables]\n",
    "astro_stds = [astro_df['sigma' if col == 'sigma_det' else col].std() for col in variables]\n",
    "\n",
    "ax2.bar(x_pos - 0.25, unids_orig_stds, 0.25, label='UNIDs Original', color='lightcoral', alpha=0.7)\n",
    "ax2.bar(x_pos, unids_trans_stds, 0.25, label='UNIDs Log10', color='lightgreen', alpha=0.7)\n",
    "ax2.bar(x_pos + 0.25, astro_stds, 0.25, label='ASTRO', color='skyblue', alpha=0.7)\n",
    "ax2.set_title('Comparación de Desviaciones Estándar')\n",
    "ax2.set_ylabel('Desviación Estándar')\n",
    "ax2.set_xlabel('Variables')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(variables, rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Histograma conjunto de una variable representativa (E_peak)\n",
    "ax3.hist(unids_original['E_peak'], bins=50, alpha=0.5, color='red', density=True, label='UNIDs Original')\n",
    "ax3.set_title('E_peak: Distribución Original UNIDs')\n",
    "ax3.set_xlabel('E_peak (escala lineal)')\n",
    "ax3.set_ylabel('Densidad')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Histograma después de transformación\n",
    "ax4.hist(unids_transformed['E_peak'], bins=40, alpha=0.6, color='green', density=True, label='UNIDs Log10')\n",
    "ax4.hist(astro_df['E_peak'], bins=40, alpha=0.6, color='blue', density=True, label='ASTRO')\n",
    "ax4.set_title('E_peak: Después de Transformación')\n",
    "ax4.set_xlabel('Log10(E_peak)')\n",
    "ax4.set_ylabel('Densidad')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Resumen Visual: Verificación de Transformación Exitosa', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar columnas para consistencia\n",
    "final_column_mapping = {\n",
    "    'E_peak': 'Log(E_peak)',\n",
    "    'beta': 'Log(beta)',\n",
    "    'sigma_det': 'Log(sigma)',\n",
    "    'beta_Rel': 'Log(beta_Rel)',\n",
    "    'number': 'number'\n",
    "}\n",
    "\n",
    "unids_final = unids_transformed.rename(columns=final_column_mapping)\n",
    "\n",
    "print(\"Columnas renombradas para consistencia:\")\n",
    "for old_name, new_name in final_column_mapping.items():\n",
    "    print(f\"  {old_name} → {new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDataset final UNIDs transformado:\")\n",
    "print(f\"Dimensiones: {unids_final.shape}\")\n",
    "print(f\"Columnas: {list(unids_final.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrimeras 5 filas del dataset transformado:\")\n",
    "unids_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nÚltimas 5 filas del dataset transformado:\")\n",
    "unids_final.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar y guardar archivos con UNIDs transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset completo transformado (con IDs)\n",
    "unids_final.to_csv('../../data/processed/unids_log/unids_transformed_complete.csv', index=False)\n",
    "unids_final.to_csv('../../data/processed/unids_log/unids_transformed_complete.txt', sep='\\t', index=False)\n",
    "print(\"- unids_transformed_complete.csv / .txt\")\n",
    "\n",
    "# Mapeo de IDs para trazabilidad (POR SI ACASO FALLASE ALGO UN DÍA)\n",
    "id_mapping = unids_final[['number']].copy()\n",
    "id_mapping['original_index'] = range(len(id_mapping))\n",
    "id_mapping.to_csv('../../data/processed/unids_log/unids_transformed_id_mapping.csv', index=False)\n",
    "print(\"- unids_transformed_id_mapping.csv\")\n",
    "\n",
    "# Log de transformación aplicada\n",
    "transformation_log_df = pd.DataFrame([\n",
    "    {'variable': var, 'transformation': transform, 'epsilon_used': epsilon}\n",
    "    for var, transform in transformation_log.items()\n",
    "])\n",
    "transformation_log_df.to_csv('../../data/processed/unids_log/unids_transformation_log.csv', index=False)\n",
    "print(\"- unids_transformation_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Con esta transformación, las fuentes UNID están ahora en la misma escala logarítmica que las fuentes ASTRO. Este paso es esencial para evitar inconsistencias al aplicar modelos de detección de anomalías como One-Class SVM.\n",
    "\n",
    "Los archivos resultantes (`unids_log.csv` y `unids_log.txt`) están listos para ser utilizados en los notebooks de análisis y predicción posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación visual antes y después de la transformación logarítmica\n",
    "\n",
    "Para comprobar la consistencia entre los datos UNID y ASTRO, se comparan ambas fuentes primero en su escala original y luego tras aplicar la transformación logarítmica base 10 (`log10`) a los UNIDs.\n",
    "\n",
    "- En escala original, los valores de `E_peak` y `beta` de los UNIDs se encuentran en un rango muy diferente al de los ASTRO.\n",
    "- Una vez transformados a escala `log10`, los valores de UNIDs se alinean mejor con los de ASTRO, lo que valida la transformación y confirma que ahora los conjuntos son comparables.\n",
    "\n",
    "Esta transformación es esencial para asegurar la coherencia cuando se utilice el modelo One-Class SVM entrenado exclusivamente con datos ASTRO."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv DarkMatter_TFG)",
   "language": "python",
   "name": "venv-tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
