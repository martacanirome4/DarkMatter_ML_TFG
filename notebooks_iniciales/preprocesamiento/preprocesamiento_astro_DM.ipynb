{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos – ASTRO vs DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de Datos ASTRO y DM para Modelado One-Class SVM\n",
    "\n",
    "El archivo original `XY_bal_log_Rel.txt` contiene un conjunto de datos etiquetado, en el cual las instancias se clasifican como:\n",
    "\n",
    "- **0 (ASTRO)**: fuentes astrofísicas reales, identificadas por telescopios.\n",
    "- **1 (DM)**: fuentes simuladas que imitan la señal esperada de materia oscura (Dark Matter).\n",
    "\n",
    "El dataset también contiene las siguientes variables (features):\n",
    "\n",
    "1. `Log(E_peak)`: Logaritmo del pico de energía del espectro gamma.\n",
    "2. `Log(beta)`: Logaritmo de la curvatura espectral.\n",
    "3. `Log(sigma)`: Logaritmo de la significancia de detección.\n",
    "4. `Log(beta_Rel)`: Logaritmo del error relativo de `beta`.\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivo del preprocesamiento\n",
    "\n",
    "El propósito principal de este notebook es preparar los datos para entrenar un modelo **One-Class SVM**, el cual está diseñado para aprender únicamente a partir de ejemplos de una sola clase, considerada \"normal\". En este caso:\n",
    "\n",
    "> Se considera que las fuentes ASTRO representan el comportamiento normal observable en el espacio gamma.\n",
    "\n",
    "Por tanto, se decide **separar el dataset original en dos subconjuntos distintos**:\n",
    "\n",
    "- Un archivo que contiene exclusivamente fuentes ASTRO (`astro_DM == 0`), que será usado para el entrenamiento del modelo.\n",
    "- Otro archivo que contiene las simulaciones de materia oscura (`astro_DM == 1`), que será reservado para futuras fases de evaluación del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Procedimiento aplicado\n",
    "\n",
    "1. Se cargó el dataset original con nombres de columnas definidos manualmente.\n",
    "2. Se verificó la distribución de clases y se confirmó que los datos estaban correctamente etiquetados.\n",
    "3. Se dividieron los datos en dos nuevos DataFrames: `astro_df` y `dm_df`.\n",
    "4. Ambos subconjuntos se guardaron en rutas separadas para su posterior reutilización:\n",
    "\n",
    "- `data/processed/XY_bal_log_Rel/astro/XY_bal_log_Rel_astro.txt`\n",
    "- `data/processed/XY_bal_log_Rel/DM/XY_bal_log_Rel_DM.txt`\n",
    "\n",
    "---\n",
    "\n",
    "### Justificación metodológica\n",
    "\n",
    "Separar las clases desde el preprocesamiento tiene varias ventajas:\n",
    "\n",
    "- Permite un entrenamiento limpio del One-Class SVM con datos puramente normales.\n",
    "- Evita introducir sesgos derivados de simulaciones (DM) en la fase de entrenamiento.\n",
    "- Establece un flujo de datos reproducible y controlado, ideal para procesos de validación y experimentación.\n",
    "\n",
    "Este enfoque está alineado con el objetivo general del trabajo: identificar potenciales señales de materia oscura a través de la detección de anomalías en fuentes no identificadas (unIDs), partiendo de un modelo entrenado únicamente con datos astrofísicos reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir nombres de las columnas\n",
    "features = ['E_peak', 'beta', 'sigma', 'beta_Rel', '0,1=astro,DM']\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv('../../data/raw/XY_bal_log_Rel.txt', sep=\"\\s+\", names=features, engine='python', skiprows=1)\n",
    "\n",
    "# Renombrar la columna de etiquetas para mayor claridad\n",
    "df = df.rename(columns={'0,1=astro,DM': 'astro_DM'})\n",
    "\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"Número total de observaciones: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de clases\n",
    "print(df['astro_DM'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas por clase\n",
    "print(f\"\\n\\nEstadísticas por clase:\")\n",
    "\n",
    "print(f\"\\nCLASE ASTRO (0.0):\")\n",
    "astro_stats = df[df['astro_DM'] == 0.0].describe()\n",
    "print(astro_stats)\n",
    "\n",
    "print(f\"\\nCLASE DM (1.0):\")\n",
    "dm_stats = df[df['astro_DM'] == 1.0].describe()\n",
    "print(dm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de características (excluyendo la etiqueta)\n",
    "feature_cols = ['E_peak', 'beta', 'sigma', 'beta_Rel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas superpuestos por clase\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    # Datos por clase\n",
    "    astro_data = df[df['astro_DM'] == 0.0][col]\n",
    "    dm_data = df[df['astro_DM'] == 1.0][col]\n",
    "    \n",
    "    # Histogramas superpuestos\n",
    "    axes[i].hist(astro_data, bins=40, alpha=0.6, color='skyblue', label='ASTRO', density=True)\n",
    "    axes[i].hist(dm_data, bins=40, alpha=0.6, color='lightcoral', label='DM', density=True)\n",
    "    \n",
    "    axes[i].set_title(f'Distribución de {col} por Clase')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Densidad')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear scatter plots para pares de variables más interesantes\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Pares de variables importantes\n",
    "pairs = [\n",
    "    ('E_peak', 'beta'),\n",
    "    ('E_peak', 'sigma'),\n",
    "    ('beta', 'sigma'),\n",
    "    ('beta', 'beta_Rel'),\n",
    "    ('sigma', 'beta_Rel'),\n",
    "    ('E_peak', 'beta_Rel')\n",
    "]\n",
    "\n",
    "for i, (x_var, y_var) in enumerate(pairs):\n",
    "    # Separar por clase\n",
    "    astro_data = df[df['astro_DM'] == 0.0]\n",
    "    dm_data = df[df['astro_DM'] == 1.0]\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[i].scatter(astro_data[x_var], astro_data[y_var], alpha=0.6, \n",
    "                   c='skyblue', label='ASTRO', s=20)\n",
    "    axes[i].scatter(dm_data[x_var], dm_data[y_var], alpha=0.6, \n",
    "                   c='lightcoral', label='DM', s=20)\n",
    "    \n",
    "    axes[i].set_xlabel(x_var)\n",
    "    axes[i].set_ylabel(y_var)\n",
    "    axes[i].set_title(f'{x_var} vs {y_var}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar datos por clase\n",
    "astro_df = df[df['astro_DM'] == 0.0].copy()\n",
    "dm_df = df[df['astro_DM'] == 1.0].copy()\n",
    "\n",
    "print(f\"Datos ASTRO: {len(astro_df)} observaciones\")\n",
    "print(f\"Datos DM: {len(dm_df)} observaciones\")\n",
    "\n",
    "# Mostrar estadísticas finales de cada conjunto\n",
    "print(\"\\nEstadísticas finales - Conjunto ASTRO:\")\n",
    "print(astro_df[feature_cols].describe())\n",
    "\n",
    "print(\"\\nEstadísticas finales - Conjunto DM:\")\n",
    "print(dm_df[feature_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nombres de columnas con prefijo Log() para indicar transformación logarítmica\n",
    "log_feature_names = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)', 'astro_DM']\n",
    "\n",
    "# Preparar datasets con nombres de columnas actualizados\n",
    "astro_df_save = astro_df.copy()\n",
    "dm_df_save = dm_df.copy()\n",
    "df_complete_save = df.copy()\n",
    "\n",
    "# Renombrar columnas para indicar transformación logarítmica\n",
    "column_mapping = dict(zip(['E_peak', 'beta', 'sigma', 'beta_Rel', 'astro_DM'], log_feature_names))\n",
    "astro_df_save = astro_df_save.rename(columns=column_mapping)\n",
    "dm_df_save = dm_df_save.rename(columns=column_mapping)\n",
    "df_complete_save = df_complete_save.rename(columns=column_mapping)\n",
    "\n",
    "print(\"Nombres de columnas actualizados:\")\n",
    "for old_name, new_name in column_mapping.items():\n",
    "    print(f\"  {old_name} → {new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nGuardando datos ASTRO:\")\n",
    "\n",
    "# Con etiqueta - CSV\n",
    "astro_df_save.to_csv('../../data/processed/XY_bal_log_Rel/astro/astro_data_with_labels.csv', index=False)\n",
    "print(\"- astro_data_with_labels.csv\")\n",
    "\n",
    "# Con etiqueta - TXT\n",
    "astro_df_save.to_csv('../../data/processed/XY_bal_log_Rel/astro/astro_data_with_labels.txt', sep='\\t', index=False)\n",
    "print(\"- astro_data_with_labels.txt\")\n",
    "\n",
    "# Sin etiqueta (solo features) - CSV\n",
    "astro_features_only = astro_df_save[log_feature_names[:-1]]  # Excluir astro_DM\n",
    "astro_features_only.to_csv('../../data/processed/XY_bal_log_Rel/astro/astro_data.csv', index=False)\n",
    "print(\"- astro_data.csv (solo features)\")\n",
    "\n",
    "# Sin etiqueta (solo features) - TXT\n",
    "astro_features_only.to_csv('../../data/processed/XY_bal_log_Rel/astro/astro_data.txt', sep='\\t', index=False)\n",
    "print(\"- astro_data.txt (solo features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nGuardando datos DM:\")\n",
    "\n",
    "# Con etiqueta - CSV\n",
    "dm_df_save.to_csv('../../data/processed/XY_bal_log_Rel/DM/dm_data_with_labels.csv', index=False)\n",
    "print(\"- dm_data_with_labels.csv\")\n",
    "\n",
    "# Con etiqueta - TXT\n",
    "dm_df_save.to_csv('../../data/processed/XY_bal_log_Rel/DM/dm_data_with_labels.txt', sep='\\t', index=False)\n",
    "print(\"- dm_data_with_labels.txt\")\n",
    "\n",
    "# Sin etiqueta (solo features) - CSV\n",
    "dm_features_only = dm_df_save[log_feature_names[:-1]]  # Excluir astro_DM\n",
    "dm_features_only.to_csv('../../data/processed/XY_bal_log_Rel/DM/dm_data.csv', index=False)\n",
    "print(\"- dm_data.csv (solo features)\")\n",
    "\n",
    "# Sin etiqueta (solo features) - TXT\n",
    "dm_features_only.to_csv('../../data/processed/XY_bal_log_Rel/DM/dm_data.txt', sep='\\t', index=False)\n",
    "print(\"- dm_data.txt (solo features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nGuardando dataset completo:\")\n",
    "\n",
    "# Dataset completo - CSV\n",
    "df_complete_save.to_csv('../../data/processed/XY_bal_log_Rel/fermi_lat_complete.csv', index=False)\n",
    "print(\"- fermi_lat_complete.csv\")\n",
    "\n",
    "# Dataset completo - TXT\n",
    "df_complete_save.to_csv('../../data/processed/XY_bal_log_Rel/fermi_lat_complete.txt', sep='\\t', index=False)\n",
    "print(\"- fermi_lat_complete.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverificación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Cargar datos ASTRO desde TXT\n",
    "    print(\"Cargando astro_data_with_labels.txt...\")\n",
    "    astro_reloaded = pd.read_csv('../../data/processed/XY_bal_log_Rel/astro/astro_data_with_labels.txt', sep='\\t')\n",
    "    print(f\"- Archivo cargado exitosamente: {astro_reloaded.shape}\")\n",
    "    \n",
    "    # Verificar estructura\n",
    "    print(f\"\\nESTRUCTURA DEL DATASET ASTRO:\")\n",
    "    print(f\"  • Dimensiones: {astro_reloaded.shape}\")\n",
    "    print(f\"  • Columnas: {list(astro_reloaded.columns)}\")\n",
    "    print(f\"  • Tipos de datos:\")\n",
    "    for col, dtype in astro_reloaded.dtypes.items():\n",
    "        print(f\"    - {col}: {dtype}\")\n",
    "    \n",
    "    # Verificar contenido\n",
    "    print(f\"\\nCONTENIDO DEL DATASET ASTRO:\")\n",
    "    print(\"Primeras 5 filas:\")\n",
    "    print(astro_reloaded.head())\n",
    "    \n",
    "    print(\"\\nÚltimas 3 filas:\")\n",
    "    print(astro_reloaded.tail(3))\n",
    "    \n",
    "    # Verificar etiquetas\n",
    "    print(f\"\\nVERIFICACIÓN DE ETIQUETAS:\")\n",
    "    print(f\"  • Valores únicos en astro_DM: {astro_reloaded['astro_DM'].unique()}\")\n",
    "    print(f\"  • Conteo de clases: {astro_reloaded['astro_DM'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Verificar integridad\n",
    "    original_astro_count = len(astro_df_save)\n",
    "    reloaded_astro_count = len(astro_reloaded)\n",
    "    \n",
    "    print(f\"\\nVERIFICACIÓN DE INTEGRIDAD ASTRO:\")\n",
    "    print(f\"  • Observaciones originales: {original_astro_count:,}\")\n",
    "    print(f\"  • Observaciones recargadas: {reloaded_astro_count:,}\")\n",
    "    \n",
    "    if original_astro_count == reloaded_astro_count:\n",
    "        print(\"Sin pérdida de datos\")\n",
    "    else:\n",
    "        print(\"Posible pérdida de datos\")\n",
    "    \n",
    "    # Verificar nombres de columnas\n",
    "    expected_columns = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)', 'astro_DM']\n",
    "    \n",
    "    print(f\"\\nVERIFICACIÓN DE NOMBRES DE COLUMNAS:\")\n",
    "    print(f\"  • Esperadas: {expected_columns}\")\n",
    "    print(f\"  • Encontradas: {list(astro_reloaded.columns)}\")\n",
    "    \n",
    "    if list(astro_reloaded.columns) == expected_columns:\n",
    "        print(\"Nombres de columnas correctos\")\n",
    "    else:\n",
    "        print(\"Nombres de columnas no coinciden\")\n",
    "    \n",
    "    # Estadísticas básicas\n",
    "    feature_columns = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)']\n",
    "    print(f\"\\nESTADÍSTICAS DESCRIPTIVAS ASTRO:\")\n",
    "    print(astro_reloaded[feature_columns].describe())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VERIFICACIÓN ASTRO COMPLETADA EXITOSAMENTE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: No se encontró astro_data_with_labels.txt\")\n",
    "    print(f\"Detalle: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error inesperado al verificar datos ASTRO: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Cargar datos DM desde TXT  \n",
    "    print(\"Cargando dm_data_with_labels.txt...\")\n",
    "    dm_reloaded = pd.read_csv('../../data/processed/XY_bal_log_Rel/DM/dm_data_with_labels.txt', sep='\\t')\n",
    "    print(f\"- Archivo cargado exitosamente: {dm_reloaded.shape}\")\n",
    "    \n",
    "    # Verificar estructura\n",
    "    print(f\"\\nESTRUCTURA DEL DATASET DM:\")\n",
    "    print(f\"  • Dimensiones: {dm_reloaded.shape}\")\n",
    "    print(f\"  • Columnas: {list(dm_reloaded.columns)}\")\n",
    "    print(f\"  • Tipos de datos:\")\n",
    "    for col, dtype in dm_reloaded.dtypes.items():\n",
    "        print(f\"    - {col}: {dtype}\")\n",
    "    \n",
    "    # Verificar contenido\n",
    "    print(f\"\\nCONTENIDO DEL DATASET DM:\")\n",
    "    print(\"Primeras 5 filas:\")\n",
    "    print(dm_reloaded.head())\n",
    "    \n",
    "    print(\"\\nÚltimas 3 filas:\")\n",
    "    print(dm_reloaded.tail(3))\n",
    "    \n",
    "    # Verificar etiquetas\n",
    "    print(f\"\\nVERIFICACIÓN DE ETIQUETAS:\")\n",
    "    print(f\"  • Valores únicos en astro_DM: {dm_reloaded['astro_DM'].unique()}\")\n",
    "    print(f\"  • Conteo de clases: {dm_reloaded['astro_DM'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Verificar integridad\n",
    "    original_dm_count = len(dm_df_save)\n",
    "    reloaded_dm_count = len(dm_reloaded)\n",
    "    \n",
    "    print(f\"\\nVERIFICACIÓN DE INTEGRIDAD DM:\")\n",
    "    print(f\"  • Observaciones originales: {original_dm_count:,}\")\n",
    "    print(f\"  • Observaciones recargadas: {reloaded_dm_count:,}\")\n",
    "    \n",
    "    if original_dm_count == reloaded_dm_count:\n",
    "        print(\"Sin pérdida de datos\")\n",
    "    else:\n",
    "        print(\"Posible pérdida de datos\")\n",
    "    \n",
    "    # Verificar nombres de columnas\n",
    "    expected_columns = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)', 'astro_DM']\n",
    "    \n",
    "    print(f\"\\nVERIFICACIÓN DE NOMBRES DE COLUMNAS:\")\n",
    "    print(f\"  • Esperadas: {expected_columns}\")\n",
    "    print(f\"  • Encontradas: {list(dm_reloaded.columns)}\")\n",
    "    \n",
    "    if list(dm_reloaded.columns) == expected_columns:\n",
    "        print(\"Nombres de columnas correctos\")\n",
    "    else:\n",
    "        print(\"Nombres de columnas no coinciden\")\n",
    "    \n",
    "    # Estadísticas básicas\n",
    "    feature_columns = ['Log(E_peak)', 'Log(beta)', 'Log(sigma)', 'Log(beta_Rel)']\n",
    "    print(f\"\\nESTADÍSTICAS DESCRIPTIVAS DM:\")\n",
    "    print(dm_reloaded[feature_columns].describe())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VERIFICACIÓN DM COMPLETADA EXITOSAMENTE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: No se encontró dm_data_with_labels.txt\")\n",
    "    print(f\"Detalle: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error inesperado al verificar datos DM: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv DarkMatter_TFG)",
   "language": "python",
   "name": "venv-tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
