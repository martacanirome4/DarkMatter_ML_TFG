{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üìì Modelo de OneClassSVM entrenado con datos  Unid (no identificados), y predicci√≥n sobre datos DM (drk matter, materia oscura simulada)\n",
    "\n",
    "**Proyecto**: Detecci√≥n de posibles fuentes de materia oscura usando ML en datos Fermi-LAT  \n",
    "**Autor**: Marta Canino Romero  \n",
    "**Fecha**: febrero-mayo 20225\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Descripci√≥n:\n",
    "\n",
    "Este notebook aplica un modelo **One-Class SVM** entrenado con datos de fuentes astrof√≠sicas desconocidas (UNIDs) usando las siguientes caracter√≠sticas:\n",
    "\n",
    "- E_peak\n",
    "- beta\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Objetivos espec√≠ficos:\n",
    "\n",
    "- Entrenar modelo OCSVM con [n√∫mero de features] \n",
    "- Optimizar hiperpar√°metros (grid search sobre `nu` y `gamma`)\n",
    "- Evaluar sobre datos de validaci√≥n y prueba\n",
    "- Aplicar modelo final sobre datos UNID para predicci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Entrada de datos:\n",
    "\n",
    "- `../../data/raw/unids_3F_beta_err_names.txt`\n",
    "\n",
    "## üíæ Salida esperada:\n",
    "\n",
    "- Mejor combinaci√≥n de hiperpar√°metros\n",
    "- M√©tricas de evaluaci√≥n (f1-score, confusion matrix)\n",
    "- Exportaci√≥n de los UNIDs m√°s an√≥malos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unids_path = \"../../data/raw/unids_3F_beta_err_names.txt\"\n",
    "df_unids = pd.read_csv(unids_path, sep='\\s+')\n",
    "df_unids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log10 to data\n",
    "cols_to_log = [\"E_peak\", \"beta\", \"sigma_det\", \"beta_Rel\"] # Aplicamos la transformaci√≥n a todas las columnas aunque solo usemos 2\n",
    "df_unids_log = df_unids.copy()\n",
    "df_unids_log[cols_to_log] = df_unids_log[cols_to_log].apply(lambda x: np.log10(x.clip(lower=1e-10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Original\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(data=df_unids, x=\"E_peak\", y=\"beta\", color=\"skyblue\")\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"E_peak\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Log10\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(data=df_unids_log, x=\"E_peak\", y=\"beta\", color=\"orange\")\n",
    "plt.title(\"Log10 Transformed\")\n",
    "plt.xlabel(\"log10(E_peak)\")\n",
    "plt.ylabel(\"log10(beta)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and IDs\n",
    "X = df_unids_log[[\"beta\", \"E_peak\"]].values\n",
    "y = df_unids_log[\"number\"].values\n",
    "\n",
    "# === SPLIT: Train / Val / Test ===\n",
    "# First split into train and temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# Then split temp into val and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split column with default value\n",
    "df_unids_log['split'] = 'unused'\n",
    "\n",
    "# Set split labels by matching 'number'\n",
    "df_unids_log.loc[df_unids_log['number'].isin(y_train), 'split'] = 'train'\n",
    "df_unids_log.loc[df_unids_log['number'].isin(y_val), 'split'] = 'val'\n",
    "df_unids_log.loc[df_unids_log['number'].isin(y_test), 'split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_values = [0.01, 0.02, 0.05, 0.1]\n",
    "gamma_values = ['scale', 'auto'] + list(np.logspace(-3, 1, 5))  # 0.001 to 10\n",
    "\n",
    "results = []\n",
    "best_score = np.inf\n",
    "best_model = None\n",
    "best_params = {}\n",
    "\n",
    "for nu in nu_values:\n",
    "    for gamma in gamma_values:\n",
    "        model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma)\n",
    "        model.fit(X_train_scaled)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        preds = model.predict(X_val_scaled)\n",
    "        n_outliers = np.sum(preds == -1)  # -1 means \"anomaly\"\n",
    "        \n",
    "        results.append({'nu': nu, 'gamma': gamma, 'outliers': n_outliers})\n",
    "        \n",
    "        if n_outliers < best_score:\n",
    "            best_score = n_outliers\n",
    "            best_model = model\n",
    "            best_params = {'nu': nu, 'gamma': gamma}\n",
    "\n",
    "# Show best hyperparameters\n",
    "print(f\"‚úÖ Best Parameters: nu = {best_params['nu']}, gamma = {best_params['gamma']}\")\n",
    "print(f\"üö® Outliers on Validation Set: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results['gamma_str'] = df_results['gamma'].astype(str)\n",
    "heatmap_data = df_results.pivot(index='nu', columns='gamma_str', values='outliers')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='coolwarm')\n",
    "plt.title(\"Outliers on Validation Set\")\n",
    "plt.xlabel(\"Gamma\")\n",
    "plt.ylabel(\"Nu\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_preds = best_model.predict(X_test_scaled)\n",
    "n_test_outliers = np.sum(test_preds == -1)\n",
    "\n",
    "print(f\"üîç Outliers on Final Test Set: {n_test_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = best_model.decision_function(X_test_scaled)\n",
    "outliers_by_score = np.sum(scores_test < 0)\n",
    "\n",
    "print(f\"Outliers by actual decision function: {outliers_by_score}\")\n",
    "print(f\"Test min score: {scores_test.min()}, Max score: {scores_test.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = best_model.predict(X_val_scaled)\n",
    "print(f\"Outliers on Validation Set (double check): {np.sum(val_preds == -1)}\")\n",
    "\n",
    "test_preds = best_model.predict(X_test_scaled)\n",
    "print(f\"Outliers on Test Set (final eval): {np.sum(test_preds == -1)}\")\n",
    "\n",
    "train_preds = best_model.predict(X_train_scaled)\n",
    "print(f\"Outliers on Training Set: {np.sum(train_preds == -1)}\")\n",
    "\n",
    "print(f\"üìä Outliers:\")\n",
    "print(f\"- Training set: {np.sum(train_preds == -1)}\")\n",
    "print(f\"- Validation set: {np.sum(val_preds == -1)}\")\n",
    "print(f\"- Test set: {np.sum(test_preds == -1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create meshgrid ===\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5, 300),  # E_peak\n",
    "    np.linspace(X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5, 300)   # beta\n",
    ")\n",
    "\n",
    "grid = np.c_[yy.ravel(), xx.ravel()]\n",
    "Z = best_model.decision_function(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# === Get latest predictions and outlier indices ===\n",
    "test_preds = best_model.predict(X_test_scaled)\n",
    "scores_test = best_model.decision_function(X_test_scaled)\n",
    "outlier_indices = np.where(test_preds == -1)[0]\n",
    "\n",
    "# === Plot ===\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# Decision function contour\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu_r, alpha=0.8)\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='crimson', linestyles='--')\n",
    "\n",
    "# Training data\n",
    "plt.scatter(X_train_scaled[:, 1], X_train_scaled[:, 0],\n",
    "            c='skyblue', edgecolors='k', s=40, label='Train')\n",
    "\n",
    "# Test data\n",
    "plt.scatter(X_test_scaled[:, 1], X_test_scaled[:, 0],\n",
    "            c='orchid', edgecolors='k', s=40, label='Test')\n",
    "\n",
    "# Val data\n",
    "plt.scatter(X_val_scaled[:, 1], X_test_scaled[:, 0],\n",
    "            c='magenta', edgecolors='k', s=40, label='Val')\n",
    "\n",
    "# Outliers\n",
    "plt.scatter(X_test_scaled[outlier_indices, 1],\n",
    "            X_test_scaled[outlier_indices, 0],\n",
    "            facecolors='none', edgecolors='red', linewidths=2, s=120, label='Outliers')\n",
    "\n",
    "# Optional: annotate outlier IDs\n",
    "for i in outlier_indices:\n",
    "    plt.text(\n",
    "        X_test_scaled[i, 1] + 0.1,\n",
    "        X_test_scaled[i, 0],\n",
    "        f\"ID {y_test[i]}\",\n",
    "        color='red',\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# Axis labels and formatting\n",
    "plt.xlabel(\"log(E_peak) [scaled]\")\n",
    "plt.ylabel(\"beta [scaled]\")\n",
    "plt.title(\"Best One-Class SVM Decision Boundary with Outliers Highlighted\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_test_scaled[:, 1], X_test_scaled[:, 0], c='orchid', label=\"Test\", edgecolors='k')\n",
    "\n",
    "# Highlight the 2 outliers in red\n",
    "plt.scatter(\n",
    "    X_test_scaled[outlier_indices, 1], X_test_scaled[outlier_indices, 0],\n",
    "    facecolors='none', edgecolors='red', linewidths=2, s=100, label=\"Outliers\"\n",
    ")\n",
    "\n",
    "for i in outlier_indices:\n",
    "    plt.text(\n",
    "        X_test_scaled[i, 1] + 0.1,\n",
    "        X_test_scaled[i, 0],\n",
    "        f\"ID {y_test[i]}\",\n",
    "        color='red', fontsize=9\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"log(E_peak) [scaled]\")\n",
    "plt.ylabel(\"beta [scaled]\")\n",
    "plt.title(\"Detected Outliers in Test Set\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Predict on DM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_data_path = \"../../data/processed/XY_bal_log_Rel/DM/XY_bal_log_Rel_DM.txt\"\n",
    "df_dm = pd.read_csv(dm_data_path, sep='\\s+')\n",
    "df_dm = df_dm.rename(columns={\"0,1=astro,DM\": \"class\"})  # 1.0 = DM, 0.0 = ASTRO\n",
    "\n",
    "df_dm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer y escalar para predicci√≥n\n",
    "X_dm = df_dm[[\"beta\", \"E_peak\"]].values\n",
    "X_dm_scaled = scaler.transform(X_dm)\n",
    "\n",
    "df_dm_scaled = pd.DataFrame(X_dm_scaled, columns=[\"beta_scaled\", \"E_peak_scaled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Original\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(data=df_dm, x=\"E_peak\", y=\"beta\", color=\"magenta\", edgecolor='k')\n",
    "plt.title(\"Original Data DM\")\n",
    "plt.xlabel(\"E_peak\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Scaled\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(data=df_dm_scaled, x=\"E_peak_scaled\", y=\"beta_scaled\", color=\"red\", edgecolor='k')\n",
    "plt.title(\"After Scaling DM\")\n",
    "plt.xlabel(\"E_peak (scaled)\")\n",
    "plt.ylabel(\"beta (scaled)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot ASTRO training and test data\n",
    "plt.scatter(X_train_scaled[:, 1], X_train_scaled[:, 0], \n",
    "            c='skyblue', edgecolors='k', label='ASTRO Train', alpha=0.6, s=40)\n",
    "\n",
    "# Plot DM data\n",
    "plt.scatter(X_dm_scaled[:, 1], X_dm_scaled[:, 0], \n",
    "            c='red', edgecolors='k', label='DM Sources', alpha=0.7, s=40)\n",
    "\n",
    "plt.xlabel(\"E_peak\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.title(\"Input Data Distribution: ASTRO (Train/Test) vs Dark Matter\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on DM data\n",
    "dm_preds = best_model.predict(X_dm_scaled)  # +1 = normal (UNID-like), -1 = anomaly\n",
    "dm_scores = best_model.decision_function(X_dm_scaled)  # higher = more \"normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions and scores to DataFrame\n",
    "df_dm[\"ocsvm_prediction\"] = dm_preds\n",
    "df_dm[\"ocsvm_score\"] = dm_scores\n",
    "\n",
    "df_dm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check summmary stats\n",
    "n_anomalies = np.sum(dm_preds == -1)\n",
    "n_normals = np.sum(dm_preds == 1)\n",
    "\n",
    "print(f\"ü™ê Predicted UNID-like (Normal): {n_normals}\")\n",
    "print(f\"üßä Predicted Anomalies (Not UNID-like, maybe ASTRO): {n_anomalies}\")\n",
    "\n",
    "# Normalize anomaly scores\n",
    "anom_percent = MinMaxScaler((0, 100)).fit_transform(-dm_scores.reshape(-1, 1)).flatten()\n",
    "df_dm[\"anom_percent\"] = anom_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top N most anomalous\n",
    "N = 10\n",
    "most_anomalous_idx = np.argsort(-anom_percent)[:N]\n",
    "top_ids = df_dm.iloc[most_anomalous_idx].index.values\n",
    "top_scores = anom_percent[most_anomalous_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Full scatter, colored by anomaly %\n",
    "scatter = plt.scatter(\n",
    "    df_dm[\"E_peak\"],\n",
    "    df_dm[\"beta\"],\n",
    "    c=df_dm[\"anom_percent\"],\n",
    "    cmap=\"inferno\",\n",
    "    edgecolors=\"k\",\n",
    "    s=60,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label(\"Anomaly % (higher = more anomalous)\", fontsize=11)\n",
    "\n",
    "# Highlight & label top anomalies\n",
    "for i, idx in enumerate(most_anomalous_idx):\n",
    "    row = df_dm.iloc[idx]\n",
    "    plt.scatter(row[\"E_peak\"], row[\"beta\"],\n",
    "                facecolors='none', edgecolors='red', s=140, linewidths=2)\n",
    "    plt.text(row[\"E_peak\"] + 0.1, row[\"beta\"], f\"ID {row.name}\", fontsize=9, color='red')\n",
    "\n",
    "# Final plot style\n",
    "plt.xlabel(\"E_peak\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.title(\"Simulated Dark Matter Sources Colored by Anomaly %\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create Meshgrid for Decision Function (same scale as DM + UNID)\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(min(X_train_scaled[:, 1].min(), X_dm_scaled[:, 1].min()) - 0.5,\n",
    "                max(X_train_scaled[:, 1].max(), X_dm_scaled[:, 1].max()) + 0.5, 400),\n",
    "    np.linspace(min(X_train_scaled[:, 0].min(), X_dm_scaled[:, 0].min()) - 0.5,\n",
    "                max(X_train_scaled[:, 0].max(), X_dm_scaled[:, 0].max()) + 0.5, 400)\n",
    ")\n",
    "\n",
    "grid = np.c_[yy.ravel(), xx.ravel()]\n",
    "Z = best_model.decision_function(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# === Plot\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# Background: Decision function contour\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu_r, alpha=0.8)\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='crimson', linestyles='--', label='Decision Boundary')\n",
    "\n",
    "# UNID training data (normal class)\n",
    "plt.scatter(\n",
    "    X_train_scaled[:, 1],  # E_peak\n",
    "    X_train_scaled[:, 0],  # beta\n",
    "    c='skyblue',\n",
    "    edgecolors='k',\n",
    "    s=50,\n",
    "    label='UNID (Train)',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Simulated Dark Matter points\n",
    "scatter = plt.scatter(\n",
    "    X_dm_scaled[:, 1],\n",
    "    X_dm_scaled[:, 0],\n",
    "    c=anom_percent,\n",
    "    cmap='inferno',\n",
    "    edgecolors='k',\n",
    "    s=60,\n",
    "    alpha=0.9,\n",
    "    label='Dark Matter'\n",
    ")\n",
    "\n",
    "# Highlight top N anomalies in DM\n",
    "for i in most_anomalous_idx:\n",
    "    x = X_dm_scaled[i, 1]\n",
    "    y = X_dm_scaled[i, 0]\n",
    "    plt.scatter(x, y, facecolors='none', edgecolors='red', s=140, linewidths=2)\n",
    "    plt.text(x + 0.1, y, f\"ID {df_dm.index[i]}\", color='red', fontsize=9)\n",
    "\n",
    "# Colorbar for anomaly %\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label(\"Anomaly % (higher = more anomalous)\", fontsize=11)\n",
    "\n",
    "# Axes and formatting\n",
    "plt.xlabel(\"log(E_peak) [scaled]\")\n",
    "plt.ylabel(\"beta [scaled]\")\n",
    "plt.title(\"DM Predictions Overlaid with UNID Training and One-Class SVM Boundary\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by highest anomaly percentage (most anomalous at top)\n",
    "top_dm_anomalies = df_dm.sort_values(\"anom_percent\", ascending=False).head(10)\n",
    "\n",
    "# Show key info\n",
    "print(\"Top 10 Most Anomalous Simulated Dark Matter Sources:\")\n",
    "display(top_dm_anomalies[[\"beta\", \"E_peak\", \"ocsvm_score\", \"anom_percent\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv DarkMatter_TFG)",
   "language": "python",
   "name": "venv-tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
